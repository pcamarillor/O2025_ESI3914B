{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 05**: Data pipeline with Neo4j\n",
    "\n",
    "**Date**: October 2nd 2025\n",
    "\n",
    "**Student Name**: Jaime Enrique Galindo Villegas\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893c817",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "\n",
    "Select a dataset in any public repository (e.g. Kaggle or Network Repository) containing relationships. In this section of the Notebook, you need to describe nodes and edges of the graph (a column representing the sourceand another column representing the destination of the relationships).\n",
    "\n",
    "Mi dataset es \"Spotify Dataset 2023\", obtenido desde kaggle en la siguiente liga: [Spotify Dataset 2023](https://www.kaggle.com/datasets/tonygordonjr/spotify-dataset-2023?select=spotify_data_12_20_2023.csv)\n",
    "\n",
    "Este dataset recopila información relevante de canciones, artistas y albumes de Spotify, permitiendo hacer relaciones entre estas entidades sobre artistas que escriben una cancion y canciones dentro de un album.\n",
    "\n",
    "En este dataset tenemos varios archivos, los principales son: <br>\n",
    "Para información relevante de los nodos artista, canción y album de los archivos:\n",
    "- spotify_artist_data_2023\n",
    "- spotify_tracks_data_2023\n",
    "- spotify-albums_data_2023\n",
    "\n",
    "Y podemos sacar relaciones entre artistas-albums y albums-canciones de los archivos:\n",
    "- spotify_data_12_20_2023 \n",
    "<br>\n",
    "(También es posible sacar toda la información relevante de los nodos desde este último archivo.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e84c3",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed17a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4d31339c-3ba9-4fcb-8f2f-5ce2438e8cb8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#caniuse-core;1.3.0 in central\n",
      "\tfound org.neo4j#caniuse-api;1.3.0 in central\n",
      "\tfound org.jetbrains.kotlin#kotlin-stdlib;2.1.20 in central\n",
      "\tfound org.jetbrains#annotations;13.0 in central\n",
      "\tfound org.neo4j#caniuse-neo4j-detection;1.3.0 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver-slim;4.4.21 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound io.netty#netty-handler;4.1.127.Final in central\n",
      "\tfound io.netty#netty-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.127.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.127.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.73.Final in central\n",
      "\tfound io.projectreactor#reactor-core;3.6.11 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.11.0 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-spi;1.0.0-rc2 in central\n",
      "\tfound org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-provided;1.0.0-rc2 in central\n",
      ":: resolution report :: resolve 1013ms :: artifacts dl 30ms\n",
      "\t:: modules in use:\n",
      "\tio.netty#netty-buffer;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.73.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.127.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.6.11 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from central in [default]\n",
      "\torg.jetbrains#annotations;13.0 from central in [default]\n",
      "\torg.jetbrains.kotlin#kotlin-stdlib;2.1.20 from central in [default]\n",
      "\torg.neo4j#caniuse-api;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-core;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-neo4j-detection;1.3.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.11.0 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-provided;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-spi;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver-slim;4.4.21 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   0   ||   24  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4d31339c-3ba9-4fcb-8f2f-5ce2438e8cb8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 24 already retrieved (0kB/31ms)\n",
      "25/10/05 03:51:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Examples on SparkSQL\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.13:5.3.10_for_spark_3\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93cd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build schemas\n",
    "# Import your module\n",
    "from jaime_galindo.spark_utils import SparkUtils\n",
    "\n",
    "# Artist schema:\n",
    "artists_schema = SparkUtils.generate_schema([\n",
    "('artist_id', 'string'),\n",
    "('name', 'string'),\n",
    "('artist_popularity', 'int'),\n",
    "('followers', 'int')\n",
    "])\n",
    "\n",
    "\n",
    "# Tracks schema:\n",
    "tracks_schema = SparkUtils.generate_schema([\n",
    "('track_id', 'string'),\n",
    "('track_name', 'string'),\n",
    "('track_href', 'string'),\n",
    "('track_popularity', 'int'),\n",
    "('explicit', 'boolean'),\n",
    "('duration_ms', 'int')\t\n",
    "])\n",
    "\n",
    "# Album schema:\n",
    "albums_schema = SparkUtils.generate_schema([\n",
    "('album_id', 'string'),\n",
    "('album_name', 'string'),\n",
    "('release_date', 'timestamp'),\n",
    "('album_popularity', 'int'),\n",
    "('album_type', 'string'),\n",
    "('label', 'string'),\n",
    "('total_tracks', 'int')\n",
    "])\n",
    "\n",
    "# albums -> canciones\n",
    "albums_tracks_schema = SparkUtils.generate_schema([\n",
    "('album_id', 'string'),\n",
    "('track_id', 'string'),\n",
    "('track_number', 'int')\n",
    "])\n",
    "\n",
    "# artists -> canciones\n",
    "artists_tracks_schema = SparkUtils.generate_schema([\n",
    "('artist_id', 'string'),\n",
    "('track_id', 'string')\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e82d5",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f1a42",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b853266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add the code for your transformations to create nodes and edges DataFrames HERE\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "# Usaré el archivo de datos completo, lo leo y creo un dataset completo\n",
    "full_df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .csv(\"/opt/spark/work-dir/data/spotify_data/spotify_data_12_20_2023.csv\")\n",
    "\n",
    "\n",
    "# artista --------------------------------------------------------------------------------------------\n",
    "# obtener las columnas que usaré del dataset\n",
    "column_names = [field.name for field in artists_schema.fields]\n",
    "df_temporal = full_df.select(column_names)\n",
    "\n",
    "# Aplicar esquema a este nuevo dataset\n",
    "for field in artists_schema.fields:\n",
    "    df_temporal = df_temporal.withColumn(\n",
    "        field.name,\n",
    "        trim(col(field.name)).cast(field.dataType)\n",
    "    )\n",
    "\n",
    "# Normalizar df\n",
    "df_norm = df_temporal.withColumn(\n",
    "    \"artist_id\",\n",
    "    lower(col(\"artist_id\"))\n",
    ")\n",
    "# Obtener df final para artistas, sin duplicados\n",
    "artists_df = df_norm.dropDuplicates([\"artist_id\"])\n",
    "\n",
    "# cancion ---------------------------------------------------------------------------------------------\n",
    "# obtener las columnas que usaré del dataset\n",
    "column_names = [field.name for field in tracks_schema.fields]\n",
    "df_temporal = full_df.select(column_names)\n",
    "\n",
    "# Aplicar esquema a este nuevo dataset\n",
    "for field in tracks_schema.fields:\n",
    "    df_temporal = df_temporal.withColumn(\n",
    "        field.name,\n",
    "        trim(col(field.name)).cast(field.dataType)\n",
    "    )\n",
    "\n",
    "# Normalizar df\n",
    "df_norm = df_temporal.withColumn(\n",
    "    \"track_id\",\n",
    "    lower(col(\"track_id\"))\n",
    ")\n",
    "\n",
    "# Obtener df final para canciones, sin duplicados\n",
    "tracks_df = df_norm.dropDuplicates([\"track_id\"])\n",
    "\n",
    "\n",
    "# albums ---------------------------------------------------------------------------------------------\n",
    "# obtener las columnas que usaré del dataset\n",
    "column_names = [field.name for field in albums_schema.fields]\n",
    "df_temporal = full_df.select(column_names)\n",
    "\n",
    "# Aplicar esquema a este nuevo dataset\n",
    "for field in albums_schema.fields:\n",
    "    df_temporal = df_temporal.withColumn(\n",
    "        field.name,\n",
    "        trim(col(field.name)).cast(field.dataType)\n",
    "    )\n",
    "\n",
    "# Normalizar df\n",
    "df_norm = df_temporal.withColumn(\n",
    "    \"album_id\",\n",
    "    lower(col(\"album_id\"))\n",
    ")\n",
    "\n",
    "# Obtener df final para albums, sin duplicados\n",
    "albums_df = df_norm.dropDuplicates([\"album_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31076081",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1621affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# albums -> canciones --------------------------------------------------------------------------------------------\n",
    "# obtener las columnas que usaré del dataset\n",
    "column_names = [field.name for field in albums_tracks_schema.fields]\n",
    "df_temporal = full_df.select(column_names)\n",
    "\n",
    "# Aplicar esquema a este nuevo dataset\n",
    "for field in albums_tracks_schema.fields:\n",
    "    df_temporal = df_temporal.withColumn(\n",
    "        field.name,\n",
    "        trim(col(field.name)).cast(field.dataType)\n",
    "    )\n",
    "\n",
    "# Normalizar df\n",
    "df_norm = df_temporal \\\n",
    "    .withColumn(\"album_id\", lower(col(\"album_id\"))) \\\n",
    "    .withColumn(\"track_id\", lower(col(\"track_id\"))) \\\n",
    "\n",
    "# Obtener df final, sin canciones duplicadas, no puede haber una cancion en más de un album\n",
    "albums_tracks_df = df_norm.dropDuplicates([\"track_id\"])\n",
    "\n",
    "# Renombrar columnas para Neo4j\n",
    "albums_tracks_df = albums_tracks_df \\\n",
    "    .withColumnRenamed(\"album_id\", \"src\") \\\n",
    "    .withColumnRenamed(\"track_id\", \"dst\") \n",
    "\n",
    "\n",
    "# artistas -> canciones --------------------------------------------------------------------------------------------\n",
    "# obtener las columnas que usaré del dataset\n",
    "column_names = [field.name for field in artists_tracks_schema.fields]\n",
    "df_temporal = full_df.select(column_names)\n",
    "\n",
    "# Aplicar esquema a este nuevo dataset\n",
    "for field in artists_tracks_schema.fields:\n",
    "    df_temporal = df_temporal.withColumn(\n",
    "        field.name,\n",
    "        trim(col(field.name)).cast(field.dataType)\n",
    "    )\n",
    "\n",
    "# Normalizar df\n",
    "df_norm = df_temporal \\\n",
    "    .withColumn(\"artist_id\", lower(col(\"artist_id\"))) \\\n",
    "    .withColumn(\"track_id\", lower(col(\"track_id\"))) \\\n",
    "\n",
    "# Obtener df final, sin canciones duplicadas, por complejidad, solo puede haber una canción con un unico artista.\n",
    "# En el dataset original vienen arrays con todos los creadores de la canción pero aparecen por nombre y no por ID, de modo que hay conflictos \n",
    "# debido a que algunos artistas comparten nombre y es imposible saber a cual artista se refiere.\n",
    "artists_tracks_df = df_norm.dropDuplicates([\"track_id\"])\n",
    "\n",
    "# Renombrar columnas para Neo4j\n",
    "artists_tracks_df = artists_tracks_df \\\n",
    "    .withColumnRenamed(\"artist_id\", \"src\") \\\n",
    "    .withColumnRenamed(\"track_id\", \"dst\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a68775",
   "metadata": {},
   "source": [
    "# Writing Data in Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d475585",
   "metadata": {},
   "source": [
    "### Escribir nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01d7a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31699 artists wrote in Neo4j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add the code to write a graph from PySpark's DataFrames to Neo4j\n",
    "\n",
    "neo4j_url = \"bolt://neo4j-iteso:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_passwd = \"neo4j@1234\"\n",
    "\n",
    "artists_df.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"Overwrite\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "  .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "  .option(\"labels\", \":Artist\") \\\n",
    "  .option(\"node.keys\", \"artist_id\") \\\n",
    "  .option(\"batch.size\", \"10000\") \\\n",
    "  .save()\n",
    "\n",
    "print(f\"{artists_df.count()} artists wrote in Neo4j\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d385f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67991 albums wrote in Neo4j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "albums_df.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"Overwrite\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "  .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "  .option(\"labels\", \":Album\") \\\n",
    "  .option(\"node.keys\", \"album_id\") \\\n",
    "  .option(\"batch.size\", \"10000\") \\\n",
    "  .save()\n",
    "\n",
    "print(f\"{albums_df.count()} albums wrote in Neo4j\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fcd28c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375141 tracks wrote in Neo4j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "tracks_df.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"Overwrite\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "  .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "  .option(\"labels\", \":Track\") \\\n",
    "  .option(\"node.keys\", \"track_id\") \\\n",
    "  .option(\"batch.size\", \"10000\") \\\n",
    "  .save()\n",
    "\n",
    "print(f\"{tracks_df.count()} tracks wrote in Neo4j\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20e533",
   "metadata": {},
   "source": [
    "### Escribir relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375141 contains edges wrote in Neo4j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "albums_tracks_df.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"Overwrite\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "  .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "  .option(\"relationship\", \"CONTAINS\") \\\n",
    "  .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "  .option(\"relationship.source.labels\", \":Album\") \\\n",
    "  .option(\"relationship.source.save.mode\", \"match\") \\\n",
    "  .option(\"relationship.source.node.keys\", \"src:album_id\") \\\n",
    "  .option(\"relationship.target.labels\", \":Track\") \\\n",
    "  .option(\"relationship.target.save.mode\", \"match\") \\\n",
    "  .option(\"relationship.target.node.keys\", \"dst:track_id\") \\\n",
    "  .option(\"batch.size\", \"10000\") \\\n",
    "  .save()\n",
    "\n",
    "print(f\"{albums_tracks_df.count()} contains edges wrote in Neo4j\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33e87b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375141 performed edges wrote in Neo4j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "artists_tracks_df.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"Overwrite\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "  .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "  .option(\"relationship\", \"PERFORMED\") \\\n",
    "  .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "  .option(\"relationship.source.labels\", \":Artist\") \\\n",
    "  .option(\"relationship.source.save.mode\", \"match\") \\\n",
    "  .option(\"relationship.source.node.keys\", \"src:artist_id\") \\\n",
    "  .option(\"relationship.target.labels\", \":Track\") \\\n",
    "  .option(\"relationship.target.save.mode\", \"match\") \\\n",
    "  .option(\"relationship.target.node.keys\", \"dst:track_id\") \\\n",
    "  .option(\"batch.size\", \"10000\") \\\n",
    "  .save()\n",
    "\n",
    "print(f\"{artists_tracks_df.count()} performed edges wrote in Neo4j\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30287ea",
   "metadata": {},
   "source": [
    "# Read and Query Graphs with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ca7b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   a|                   r|                   t|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{63660, [Album], ...|{285699, CONTAINS...|{385391, [Track],...|\n",
      "|{63660, [Album], ...|{170385, CONTAINS...|{302032, [Track],...|\n",
      "|{63661, [Album], ...|{302964, CONTAINS...|{402656, [Track],...|\n",
      "|{63661, [Album], ...|{165109, CONTAINS...|{296756, [Track],...|\n",
      "|{63661, [Album], ...|{69249, CONTAINS,...|{200896, [Track],...|\n",
      "|{63662, [Album], ...|{280831, CONTAINS...|{380523, [Track],...|\n",
      "|{63662, [Album], ...|{275689, CONTAINS...|{375381, [Track],...|\n",
      "|{63662, [Album], ...|{184606, CONTAINS...|{316253, [Track],...|\n",
      "|{63662, [Album], ...|{125314, CONTAINS...|{256961, [Track],...|\n",
      "|{63662, [Album], ...|{37581, CONTAINS,...|{169228, [Track],...|\n",
      "|{63662, [Album], ...|{17946, CONTAINS,...|{149593, [Track],...|\n",
      "|{63663, [Album], ...|{321399, CONTAINS...|{421091, [Track],...|\n",
      "|{63663, [Album], ...|{231158, CONTAINS...|{13444, [Track], ...|\n",
      "|{63663, [Album], ...|{226054, CONTAINS...|{8340, [Track], h...|\n",
      "|{63663, [Album], ...|{103734, CONTAINS...|{235381, [Track],...|\n",
      "|{63663, [Album], ...|{100544, CONTAINS...|{232191, [Track],...|\n",
      "|{63663, [Album], ...|{9207, CONTAINS, ...|{140854, [Track],...|\n",
      "|{63664, [Album], ...|{205994, CONTAINS...|{337641, [Track],...|\n",
      "|{63664, [Album], ...|{135258, CONTAINS...|{266905, [Track],...|\n",
      "|{63664, [Album], ...|{80228, CONTAINS,...|{211875, [Track],...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the code to read a data frame from Neo4J and run a simple query to verify \n",
    "# Consulta hecha así para obtener varias canciones por cada album, si solo obtengo la relación, vienen cada album con una cancion ya que no recorre toda la información\n",
    "contains_df = spark.read \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"query\",\n",
    "            \"\"\"\n",
    "\t\t\t\t// Obtener 5 albumes\n",
    "\t\t\t\tMATCH (a:Album)\n",
    "\t\t\t\tWITH a LIMIT 5\n",
    "\t\t\t\t// hacer la relacion con sus canciones\n",
    "\t\t\t\tMATCH (a)-[r:CONTAINS]->(t:Track)\n",
    "\t\t\t\t// mostrar todo\n",
    "\t\t\t\tRETURN a, r, t\n",
    "            \"\"\") \\\n",
    "    .load()\n",
    "\n",
    "contains_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87bd3dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   p|\n",
      "+--------------------+\n",
      "|\"path[(56761)-[37...|\n",
      "|\"path[(33475)-[37...|\n",
      "|\"path[(58495)-[37...|\n",
      "|\"path[(48743)-[37...|\n",
      "|\"path[(39790)-[37...|\n",
      "|\"path[(33631)-[37...|\n",
      "|\"path[(51046)-[37...|\n",
      "|\"path[(41715)-[37...|\n",
      "|\"path[(36970)-[37...|\n",
      "|\"path[(37029)-[37...|\n",
      "|\"path[(51752)-[37...|\n",
      "|\"path[(43356)-[37...|\n",
      "|\"path[(58639)-[37...|\n",
      "|\"path[(35172)-[37...|\n",
      "|\"path[(55031)-[37...|\n",
      "|\"path[(33434)-[37...|\n",
      "|\"path[(50713)-[37...|\n",
      "|\"path[(60004)-[37...|\n",
      "|\"path[(33231)-[37...|\n",
      "|\"path[(50809)-[37...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "contains_df = spark.read \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"query\",\n",
    "            \"\"\"\n",
    "\t\t\t\t\tMATCH p=()-[r:PERFORMED]->() \n",
    "               RETURN p \n",
    "            \"\"\") \\\n",
    "    .load()\n",
    "\n",
    "contains_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9078a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
