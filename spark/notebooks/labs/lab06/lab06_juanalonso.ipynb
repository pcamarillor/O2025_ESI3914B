{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 05**: Data pipeline with Neo4j\n",
    "\n",
    "**Date**: October 2nd 2025\n",
    "\n",
    "**Student Name**: Juan Alonso\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893c817",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "\n",
    "The dataset describes all stations in the UK with routes trains take between each station.\n",
    "\n",
    "Its important to note that some trains skip stations. For example, if a train visited stations A, B and C, and another train visited A and C, the path in a Graph Network should be A, B and C.\n",
    "\n",
    "Frequency of routes through stations can be calculated, however, some routes have been obtained multiple times due detecting routes from different stations. Therefore, duplicate routes need to be taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e84c3",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed17a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Examples on SparkSQL - UK Train Stations\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.13:5.3.10_for_spark_3\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93cd621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows read: 7636\n",
      "Distinct sources: 2556\n",
      "Distinct targets: 2556\n",
      "Distinct stations (union): 2556\n"
     ]
    }
   ],
   "source": [
    "from juanalonso.spark_utils import SparkUtils\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "columns_info = [\n",
    "    (\"source\", \"string\"),\n",
    "    (\"target\", \"string\"),\n",
    "    (\"distance\", \"double\")\n",
    "]\n",
    "\n",
    "edges_schema = SparkUtils.generate_schema(columns_info)\n",
    "\n",
    "base_path = \"/opt/spark/work-dir/data/train_stations/\"\n",
    "\n",
    "edges_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(edges_schema) \\\n",
    "    .csv(base_path)\n",
    "\n",
    "total_rows = edges_df.count()\n",
    "distinct_sources = edges_df.select(\"source\").distinct().count()\n",
    "distinct_targets = edges_df.select(\"target\").distinct().count()\n",
    "distinct_stations = edges_df.select(\"source\").union(edges_df.select(\"target\")).distinct().count()\n",
    "\n",
    "print(\"Total rows read:\", total_rows)\n",
    "print(\"Distinct sources:\", distinct_sources)\n",
    "print(\"Distinct targets:\", distinct_targets)\n",
    "print(\"Distinct stations (union):\", distinct_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e82d5",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000b6d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routes preview:\n",
      "+------+-----------+--------+\n",
      "|source|destination|distance|\n",
      "+------+-----------+--------+\n",
      "|AAP   |BOP        |1.0     |\n",
      "|AAP   |HRN        |1.43    |\n",
      "|AAP   |NSG        |2.37    |\n",
      "|AAP   |PAL        |2.38    |\n",
      "|AAT   |ACN        |10.03   |\n",
      "+------+-----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "stations_df = edges_df.select(col(\"source\").alias(\"name\")).union(\n",
    "    edges_df.select(col(\"target\").alias(\"name\"))\n",
    ").distinct()\n",
    "\n",
    "routes_df = edges_df.select(\n",
    "    col(\"source\"),\n",
    "    col(\"target\").alias(\"destination\"),\n",
    "    col(\"distance\")\n",
    ")\n",
    "\n",
    "print(\"Routes preview:\")\n",
    "routes_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a68775",
   "metadata": {},
   "source": [
    "# Writing Data in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01d7a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556 station nodes wrote in Neo4j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7636 CONNECTS relationships wrote in Neo4j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "neo4j_url = \"bolt://neo4j-iteso:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_passwd = \"neo4j@1234\"\n",
    "\n",
    "stations_df.write \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Overwrite\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"labels\", \":Station\") \\\n",
    "    .option(\"node.keys\", \"name\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"{stations_df.count()} station nodes wrote in Neo4j\")\n",
    "\n",
    "routes_df.write \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Overwrite\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"relationship\", \"CONNECTS\") \\\n",
    "    .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "    .option(\"relationship.source.labels\", \":Station\") \\\n",
    "    .option(\"relationship.source.save.mode\", \"match\") \\\n",
    "    .option(\"relationship.source.node.keys\", \"source:name\") \\\n",
    "    .option(\"relationship.target.labels\", \":Station\") \\\n",
    "    .option(\"relationship.target.save.mode\", \"match\") \\\n",
    "    .option(\"relationship.target.node.keys\", \"destination:name\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"{routes_df.count()} CONNECTS relationships wrote in Neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30287ea",
   "metadata": {},
   "source": [
    "# Read and Query Graphs with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ca7b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+\n",
      "|source|destination|distance|\n",
      "+------+-----------+--------+\n",
      "|HFD   |AGV        |34.16   |\n",
      "|BSK   |ADV        |28.9    |\n",
      "|SAL   |ADV        |26.96   |\n",
      "|ACN   |ACH        |18.94   |\n",
      "|CWM   |AGV        |17.81   |\n",
      "|PPL   |AGV        |13.21   |\n",
      "|GYM   |ACL        |12.21   |\n",
      "|WCH   |ADV        |11.11   |\n",
      "|GRT   |ADV        |10.08   |\n",
      "|STC   |ACH        |8.74    |\n",
      "|BAG   |ACT        |4.74    |\n",
      "|LGD   |ACL        |3.9     |\n",
      "|MAO   |ACT        |3.39    |\n",
      "|SNG   |ACT        |3.37    |\n",
      "|CBS   |ADR        |2.85    |\n",
      "|WIV   |ALR        |2.8     |\n",
      "|CMH   |ABA        |2.5     |\n",
      "|STM   |AIG        |2.1     |\n",
      "|DRU   |ADR        |1.81    |\n",
      "|COA   |ADR        |1.38    |\n",
      "+------+-----------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Add the code to read a data frame from Neo4J and run a simple query to verify \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sample_query_df = spark.read \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"query\", \"\"\"\n",
    "        MATCH (a:Station)-[r:CONNECTS]->(b:Station)\n",
    "        RETURN a.name AS source, b.name AS destination, r.distance AS distance\n",
    "    \"\"\") \\\n",
    "    .load()\n",
    "\n",
    "sample_query_df = sample_query_df.withColumn(\"distance\", col(\"distance\").cast(\"double\"))\n",
    "\n",
    "sample_query_df.orderBy(col(\"distance\").desc()).limit(25).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9078a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
