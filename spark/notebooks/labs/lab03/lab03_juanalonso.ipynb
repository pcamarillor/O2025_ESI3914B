{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 03**: Data Cleaning and Transformation Pipeline\n",
    "\n",
    "**Date**: September 18th 2025\n",
    "\n",
    "**Student Name**: Juan Carlos Alonso\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5042aa29",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab 3: Juan ALonso\") \\\n",
    "    .master(\"spark://5fded284cb17:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824fc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from juanalonso.spark_utils import SparkUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary(df):\n",
    "    return df.drop(\"flight\", \"class\") if all(c in df.columns for c in [\"flight\", \"class\"]) else df\n",
    "\n",
    "def count_nulls(df):\n",
    "    return df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "def normalize_stops(df):\n",
    "    mapping = {\n",
    "        \"zero\": 0, \"non-stop\": 0,\n",
    "        \"one\": 1,\n",
    "        \"two_or_more\": 2, \"2_or_more\": 2, \"two or more\": 2,\n",
    "    }\n",
    "    mapping_expr = F.create_map([F.lit(x) for kv in mapping.items() for x in kv])\n",
    "    return df.withColumn(\"stops\", mapping_expr.getItem(F.lower(F.col(\"stops\"))).cast(\"int\"))\n",
    "\n",
    "def add_route(df):\n",
    "    return df.withColumn(\"route\", F.concat_ws(\" → \", F.col(\"source_city\"), F.col(\"destination_city\")))\n",
    "\n",
    "def encode_times(df):\n",
    "    order = [\"Early_Morning\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\", \"Late_Night\"]\n",
    "    mapping_expr = F.create_map([F.lit(x) for kv in enumerate(order) for x in (kv[1].lower(), kv[0])])\n",
    "\n",
    "    df = df.withColumn(\"departure_time_id\", mapping_expr.getItem(F.lower(F.col(\"departure_time\"))))\n",
    "    df = df.withColumn(\"arrival_time_id\", mapping_expr.getItem(F.lower(F.col(\"arrival_time\"))))\n",
    "    return df\n",
    "\n",
    "def add_is_expensive(df):\n",
    "    return df.withColumn(\"is_expensive\", (F.col(\"price\") > 6000))\n",
    "\n",
    "def avg_price_per_airline(df):\n",
    "    return df.groupBy(\"airline\").agg(F.avg(\"price\").alias(\"avg_price\"))\n",
    "\n",
    "def avg_duration_per_route(df):\n",
    "    return df.groupBy(\"route\").agg(F.avg(\"duration\").alias(\"avg_duration\"))\n",
    "\n",
    "def min_max_price_per_airline(df):\n",
    "    return df.groupBy(\"airline\").agg(\n",
    "        F.min(\"price\").alias(\"min_price\"),\n",
    "        F.max(\"price\").alias(\"max_price\")\n",
    "    )\n",
    "\n",
    "def count_by_departure(df):\n",
    "    return df.groupBy(\"departure_time\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46377a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "airlines_data = pd.read_csv('../../data/airlines_flights_data.csv')\n",
    "\n",
    "airlines_schema = SparkUtils.generate_schema([\n",
    "    (\"index\", \"int\"),\n",
    "    (\"airline\", \"string\"),\n",
    "    (\"flight\", \"string\"),\n",
    "    (\"source_city\", \"string\"),\n",
    "    (\"departure_time\", \"string\"),\n",
    "    (\"stops\", \"string\"),\n",
    "    (\"arrival_time\", \"string\"),\n",
    "    (\"destination_city\", \"string\"),\n",
    "    (\"class\", \"string\"),\n",
    "    (\"duration\", \"double\"),\n",
    "    (\"days_left\", \"int\"),\n",
    "    (\"price\", \"int\"),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(airlines_data, airlines_schema)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
