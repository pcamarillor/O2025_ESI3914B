{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 03**: Data Cleaning and Transformation Pipeline\n",
    "\n",
    "**Date**: September 18th 2025\n",
    "\n",
    "**Student Name**: Santiago Montes de Oca\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc205e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/21 21:56:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from santiagomdeo.spark_utils import proof\n",
    "from santiagomdeo.spark_utils import SparkUtils\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Examples on SparkSQL\") \\\n",
    "    .master(\"spark://e52511a8c7e2:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a749af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "string\n",
      "float\n",
      "int\n",
      "int\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('index', IntegerType(), True), StructField('airline', StringType(), True), StructField('flight', StringType(), True), StructField('source_city', StringType(), True), StructField('departure_time', StringType(), True), StructField('stops', StringType(), True), StructField('arrival_time', StringType(), True), StructField('destination_city', StringType(), True), StructField('class', StringType(), True), StructField('duration', FloatType(), True), StructField('days_left', IntegerType(), True), StructField('price', IntegerType(), True)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_schema_columns = [(\"index\", \"int\"), \n",
    "     (\"airline\", \"string\"), \n",
    "     (\"flight\", \"string\"),\n",
    "     (\"source_city\", \"string\"),\n",
    "     (\"departure_time\", \"string\"),\n",
    "     (\"stops\", \"string\"),\n",
    "     (\"arrival_time\", \"string\"),\n",
    "     (\"destination_city\", \"string\"),\n",
    "     (\"class\", \"string\"),\n",
    "     (\"duration\", \"float\"),\n",
    "     (\"days_left\", \"int\"),\n",
    "     (\"price\", \"int\")\n",
    "     ]\n",
    "airlines_schema = SparkUtils.generate_schema(airlines_schema_columns)\n",
    "airlines_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc6013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "|index| airline| flight|source_city|departure_time|stops| arrival_time|destination_city|  class|duration|days_left|price|\n",
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "|    0|SpiceJet|SG-8709|      Delhi|       Evening| zero|        Night|          Mumbai|Economy|    2.17|        1| 5953|\n",
      "|    1|SpiceJet|SG-8157|      Delhi| Early_Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5953|\n",
      "|    2| AirAsia| I5-764|      Delhi| Early_Morning| zero|Early_Morning|          Mumbai|Economy|    2.17|        1| 5956|\n",
      "|    3| Vistara| UK-995|      Delhi|       Morning| zero|    Afternoon|          Mumbai|Economy|    2.25|        1| 5955|\n",
      "|    4| Vistara| UK-963|      Delhi|       Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5955|\n",
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_airlines = spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .schema(airlines_schema) \\\n",
    "                .csv(\"/opt/spark/work-dir/data/airline/\")\n",
    "\n",
    "df_airlines.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f396b6",
   "metadata": {},
   "source": [
    "## Drop unnecessary columns. Count how many null values the dataset has before/after the cleaning process\n",
    "basically remove nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96897436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airline', 'flight', 'source_city', 'departure_time', 'stops', 'arrival_time', 'destination_city', 'class', 'duration', 'days_left']\n"
     ]
    }
   ],
   "source": [
    "print(df_airlines.columns[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff41dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "\n",
    "df2 = df_airlines.dropna(subset=df_airlines.columns[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e47f292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df_airlines.count())\n",
    "print(df2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc5729",
   "metadata": {},
   "source": [
    "## Normalize categorical values: map “zero” → 0, “one” → 1, etc. in stops.\n",
    "we will first see how many unique elements are in this set, and then map each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e552e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(stops='two_or_more'), Row(stops='one'), Row(stops='zero')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select('stops').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28d34a",
   "metadata": {},
   "source": [
    "this really confuses me, cause if i have 2 or more, and the teacher kinda shows that he wants this data to be saved with integers instead of strings, so imma do both just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f134974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the question with this, is that it probably creates teh schema for the new column as string, which would be anoying\n",
    "df3 = df2.withColumn(\"Normalize Stops\",\n",
    "    when(col(\"stops\") == 'one', 1)\n",
    "    .otherwise(when(col('stops') == 'zero', 0)\n",
    "               .otherwise(2)      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34916d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+---------------+\n",
      "|index| airline| flight|source_city|departure_time|stops| arrival_time|destination_city|  class|duration|days_left|price|Normalize Stops|\n",
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+---------------+\n",
      "|    0|SpiceJet|SG-8709|      Delhi|       Evening| zero|        Night|          Mumbai|Economy|    2.17|        1| 5953|              0|\n",
      "|    1|SpiceJet|SG-8157|      Delhi| Early_Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5953|              0|\n",
      "|    2| AirAsia| I5-764|      Delhi| Early_Morning| zero|Early_Morning|          Mumbai|Economy|    2.17|        1| 5956|              0|\n",
      "|    3| Vistara| UK-995|      Delhi|       Morning| zero|    Afternoon|          Mumbai|Economy|    2.25|        1| 5955|              0|\n",
      "|    4| Vistara| UK-963|      Delhi|       Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5955|              0|\n",
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df3.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeeb1bf",
   "metadata": {},
   "source": [
    "## Create a new column called route: “Delhi → Mumbai” from source_city and destination_city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be39b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+---------------+--------------+\n",
      "|index| airline| flight|source_city|departure_time|stops| arrival_time|destination_city|  class|duration|days_left|price|Normalize Stops|         route|\n",
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+---------------+--------------+\n",
      "|    0|SpiceJet|SG-8709|      Delhi|       Evening| zero|        Night|          Mumbai|Economy|    2.17|        1| 5953|              0|Delhi-->Mumbai|\n",
      "|    1|SpiceJet|SG-8157|      Delhi| Early_Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5953|              0|Delhi-->Mumbai|\n",
      "|    2| AirAsia| I5-764|      Delhi| Early_Morning| zero|Early_Morning|          Mumbai|Economy|    2.17|        1| 5956|              0|Delhi-->Mumbai|\n",
      "|    3| Vistara| UK-995|      Delhi|       Morning| zero|    Afternoon|          Mumbai|Economy|    2.25|        1| 5955|              0|Delhi-->Mumbai|\n",
      "|    4| Vistara| UK-963|      Delhi|       Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5955|              0|Delhi-->Mumbai|\n",
      "+-----+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+---------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "df4 = df3.withColumn(\"route\", concat(col(\"source_city\"), lit(\"-->\"), col(\"destination_city\")))\n",
    "df4.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af683c97",
   "metadata": {},
   "source": [
    "## Transform departure_time and arrival_time to numerical categories (Morning, Afternoon, etc.), then encode as numbers (0=Early_Morning, 1=Morning, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e72a7b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(arrival_time='Evening'),\n",
       " Row(arrival_time='Morning'),\n",
       " Row(arrival_time='Late_Night'),\n",
       " Row(arrival_time='Afternoon'),\n",
       " Row(arrival_time='Early_Morning'),\n",
       " Row(arrival_time='Night')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.select('arrival_time').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d848c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(departure_time='Evening'),\n",
       " Row(departure_time='Morning'),\n",
       " Row(departure_time='Late_Night'),\n",
       " Row(departure_time='Afternoon'),\n",
       " Row(departure_time='Early_Morning'),\n",
       " Row(departure_time='Night')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.select('departure_time').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1036dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evening= 0, Morning= 1, Late_Night= 2, Afternoon= 3, Early_Morning= 4, Night= 5,\n",
    "\n",
    "df5 = df4.withColumn(\n",
    "\"Num_Cat_Arrival_time\",\n",
    "when(col(\"arrival_time\") == \"Evening\", 0)\n",
    ".when(col(\"arrival_time\") == \"Morning\", 1)\n",
    ".when(col(\"arrival_time\") == \"Late_Night\", 2)\n",
    ".when(col(\"arrival_time\") == \"Afternoon\", 3)\n",
    ".when(col(\"arrival_time\") == \"Early_Morning\", 4)\n",
    ".when(col(\"arrival_time\") == \"Night\", 5)\n",
    ".otherwise(\"unknown\")\n",
    ") \n",
    "df6 = df5.withColumn(\n",
    "\"Num_Cat_Departure_time\",\n",
    "when(col(\"departure_time\") == \"Evening\", 0)\n",
    ".when(col(\"departure_time\") == \"Morning\", 1)\n",
    ".when(col(\"departure_time\") == \"Late_Night\", 2)\n",
    ".when(col(\"departure_time\") == \"Afternoon\", 3)\n",
    ".when(col(\"departure_time\") == \"Early_Morning\", 4)\n",
    ".when(col(\"departure_time\") == \"Night\", 5)\n",
    ".otherwise(\"unknown\")\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aad7ebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------+----------------------+\n",
      "| arrival_time|Num_Cat_Arrival_time|departure_time|Num_Cat_Departure_time|\n",
      "+-------------+--------------------+--------------+----------------------+\n",
      "|        Night|                   5|       Evening|                     0|\n",
      "|      Morning|                   1| Early_Morning|                     4|\n",
      "|Early_Morning|                   4| Early_Morning|                     4|\n",
      "|    Afternoon|                   3|       Morning|                     1|\n",
      "|      Morning|                   1|       Morning|                     1|\n",
      "|    Afternoon|                   3|       Morning|                     1|\n",
      "|      Morning|                   1|       Morning|                     1|\n",
      "+-------------+--------------------+--------------+----------------------+\n",
      "only showing top 7 rows\n"
     ]
    }
   ],
   "source": [
    "df6.select(\"arrival_time\",\"Num_Cat_Arrival_time\",\"departure_time\",\"Num_Cat_Departure_time\" ).show(n=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6c6d9",
   "metadata": {},
   "source": [
    "## Add a new column is_expensive: when(price > 6000, True).otherwise(False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbe8444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df6.withColumn(\"is_expensive\", \n",
    "        when(col(\"price\") > 6000, lit(True)).otherwise(lit(False))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe850b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|price|is_expensive|\n",
      "+-----+------------+\n",
      "| 5953|       false|\n",
      "| 5953|       false|\n",
      "| 5956|       false|\n",
      "| 5955|       false|\n",
      "| 5955|       false|\n",
      "| 5955|       false|\n",
      "| 6060|        true|\n",
      "| 6060|        true|\n",
      "| 5954|       false|\n",
      "+-----+------------+\n",
      "only showing top 9 rows\n"
     ]
    }
   ],
   "source": [
    "df7.select(\"price\", \"is_expensive\").show(n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b03a8cf",
   "metadata": {},
   "source": [
    "# In addition, the Notebook should also contain the results of the following aggregations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fee2e",
   "metadata": {},
   "source": [
    "### Get the average price per airline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4cfc548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(price)        |\n",
      "+------------------+\n",
      "|20889.660523133203|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df7.select(mean(\"price\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9cacf",
   "metadata": {},
   "source": [
    "### Average duration per route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aafb7704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|avg(duration)    |\n",
      "+-----------------+\n",
      "|12.22102081367383|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df7.select(mean(\"duration\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae94a7",
   "metadata": {},
   "source": [
    "### Minimum and maximum price per airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c46abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(price)|\n",
      "+----------+\n",
      "|1105      |\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(price)|\n",
      "+----------+\n",
      "|123071    |\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df7.select(min(\"price\")).show(truncate=False)\n",
    "df7.select(max(\"price\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155499f2",
   "metadata": {},
   "source": [
    "### Count flights by departure_time category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c716e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65102"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.select(\"departure_time\").where(df7.departure_time==lit(\"Evening\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71146"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.select(\"departure_time\").where(df7.departure_time==lit(\"Morning\")).count()\n",
    "# Late_Night\", 2)\n",
    "# .when(col(\"departure_time\") == \"Afternoon\", 3)\n",
    "# .when(col(\"departure_time\") == \"Early_Morning\", 4)\n",
    "# .when(col(\"departure_time\") == \"Night\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8aeafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47794"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.select(\"departure_time\").where(df7.departure_time==lit(\"Afternoon\")).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
