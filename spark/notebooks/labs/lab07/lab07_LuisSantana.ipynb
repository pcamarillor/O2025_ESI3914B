{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 07**: Structured Streaming with Files\n",
    "\n",
    "**Date**: October 7nd 2025\n",
    "\n",
    "**Student Name**: Luis Angel Santana Hernandez\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893c817",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e4e4a",
   "metadata": {},
   "source": [
    "Jupyter Notebook: Build a Jupyter Notebook (spark_cluster/notebooks/labs/lab07/lab07_<your_name>.ipynb) containing a data pipeline using structured streaming. The pipeline should monitor a directory for simulated server log files, analyze error patterns in real time, and filter alerts for critical issues (for example, repeated 500 errors). The sink should be the output console.\n",
    "\n",
    "Producer: Create a script that generates random log entries (using Bash or Python). This script should be included in your module under the lib directory.\n",
    "\n",
    "Submit to Canvas a pull request (PR) link including both the script that produces random log entries and the Jupyter Notebook with your data pipeline. The notebook should display at least three micro-batches of the streaming process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e84c3",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed17a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Examples on Structured Streaming (files)\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540d429",
   "metadata": {},
   "source": [
    "# Check on producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93cd621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/logs\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95f1a3",
   "metadata": {},
   "source": [
    "# Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d75119d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+------+--------------------+-------------+\n",
      "|          Timestamp|Status|                Data|         node|\n",
      "+-------------------+------+--------------------+-------------+\n",
      "|2025-10-09 14:09:44| ERROR|500 Internal Serv...|server-node-2|\n",
      "+-------------------+------+--------------------+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+------+---------------+-------------+\n",
      "|          Timestamp|Status|           Data|         node|\n",
      "+-------------------+------+---------------+-------------+\n",
      "|2025-10-09 14:09:51|  INFO|502 Bad Gateway|server-node-2|\n",
      "|2025-10-09 14:09:51|  WARN|  404 Not Found|server-node-3|\n",
      "+-------------------+------+---------------+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------------------+------+--------------------+-------------+\n",
      "|          Timestamp|Status|                Data|         node|\n",
      "+-------------------+------+--------------------+-------------+\n",
      "|2025-10-09 14:10:04|  WARN|500 Internal Serv...|server-node-2|\n",
      "|2025-10-09 14:10:04|  INFO|     502 Bad Gateway|server-node-2|\n",
      "+-------------------+------+--------------------+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------------------+------+---------------+-------------+\n",
      "|          Timestamp|Status|           Data|         node|\n",
      "+-------------------+------+---------------+-------------+\n",
      "|2025-10-09 14:10:10|  WARN|502 Bad Gateway|server-node-1|\n",
      "+-------------------+------+---------------+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------+------+-------------+-------------+\n",
      "|          Timestamp|Status|         Data|         node|\n",
      "+-------------------+------+-------------+-------------+\n",
      "|2025-10-09 14:10:14|  WARN|404 Not Found|server-node-3|\n",
      "|2025-10-09 14:10:14|  WARN|404 Not Found|server-node-1|\n",
      "+-------------------+------+-------------+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+---------+------+----+----+\n",
      "|Timestamp|Status|Data|node|\n",
      "+---------+------+----+----+\n",
      "+---------+------+----+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------------------+------+--------------------+-------------+\n",
      "|          Timestamp|Status|                Data|         node|\n",
      "+-------------------+------+--------------------+-------------+\n",
      "|2025-10-09 14:10:35| ERROR|500 Internal Serv...|server-node-1|\n",
      "|2025-10-09 14:10:35|  WARN|       404 Not Found|server-node-2|\n",
      "|2025-10-09 14:10:35|  WARN|500 Internal Serv...|server-node-3|\n",
      "+-------------------+------+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from luis_santana.spark_utils import SparkUtils\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "logs_schema = SparkUtils.generate_schema([(\"Timestamp\", \"timestamp\"), (\"Status\", \"string\"), (\"Data\",\"string\"), (\"node\",\"string\")])\n",
    "\n",
    "# get the raw logs\n",
    "logs_raw = spark.readStream \\\n",
    "    .format(\"text\") \\\n",
    "    .load(\"/opt/spark/work-dir/data/logs/\")\n",
    "\n",
    "\n",
    "# parse the logs \n",
    "logs = logs_raw.select(\n",
    "    split(col(\"value\"), \" \\\\| \").alias(\"split\")\n",
    ").select(\n",
    "    col(\"split\").getItem(0).alias(\"Timestamp\").cast(\"timestamp\"),\n",
    "    col(\"split\").getItem(1).alias(\"Status\"),\n",
    "    col(\"split\").getItem(2).alias(\"Data\"),\n",
    "    col(\"split\").getItem(3).alias(\"node\")\n",
    ").filter((col(\"Status\") == \"ERROR\") | (col(\"Data\").rlike(\"50[0-9]\") | col(\"Data\").rlike(\"404\") ))\n",
    "\n",
    "# filter only ERROR logs or 50*(505,506,507)(im using regex) status codes on the Data field\n",
    "# error_logs_df = logs.filter(\n",
    "#     (col(\"Status\") == \"ERROR\") | (col(\"Data\").rlike(\"50[0-9]\") | col(\"Data\").rlike(\"404\") )\n",
    "# )\n",
    "\n",
    "# put both streams together\n",
    "query = logs.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "query.awaitTermination(60) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb81afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
