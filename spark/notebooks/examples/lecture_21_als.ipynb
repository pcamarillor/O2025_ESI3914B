{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Autumn 2025** </center>\n",
    "---\n",
    "### <center> **Examples on Machine Learning: Alternating Least Squares (ALS)** </center>\n",
    "---\n",
    "**Profesor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/31 13:09:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML: ALS\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Songs recommednation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|song_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|      1|     4|\n",
      "|      1|      2|     5|\n",
      "|      1|      5|     5|\n",
      "|      2|      2|     3|\n",
      "|      2|      3|     4|\n",
      "|      2|      4|     3|\n",
      "|      3|      1|     2|\n",
      "|      3|      3|     5|\n",
      "|      3|      5|     1|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pcamarillor.spark_utils import SparkUtils\n",
    "\n",
    "# Sample user-song interaction data\n",
    "data = [(1, 1, 4),\n",
    "        (1, 2, 5),\n",
    "        (1, 5, 5),\n",
    "        (2, 2, 3),\n",
    "        (2, 3, 4),\n",
    "        (2, 4, 3),\n",
    "        (3, 1, 2),\n",
    "        (3, 3, 5),\n",
    "        (3, 5, 1)]\n",
    "  \n",
    "# Define schema for the DataFrame\n",
    "schema = SparkUtils.generate_schema([(\"user_id\", \"int\"), (\"song_id\", \"int\"), (\"rating\", \"int\")])\n",
    "\n",
    "# Create DataFrame for interactions\n",
    "interactions_df = spark.createDataFrame(data, schema)\n",
    "interactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items o canciones (n):5\n",
      "Number of users (m):3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of items o canciones (n):{interactions_df.groupBy('song_id').count().count()}\")\n",
    "print(f\"Number of users (m):{interactions_df.groupBy('user_id').count().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"user_id\", \n",
    "    itemCol=\"song_id\", \n",
    "    ratingCol=\"rating\", \n",
    "    maxIter=10, \n",
    "    regParam=0.1, \n",
    "    rank=5, # Controls the dimensionality of the latent vector space for \n",
    "            # users and items.\n",
    "    coldStartStrategy=\"drop\"  # Avoids NaN predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation system generated successfully\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(interactions_df)\n",
    "print(\"Recommendation system generated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:==============================>                       (57 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------+\n",
      "|user_id|recommendations                                |\n",
      "+-------+-----------------------------------------------+\n",
      "|1      |[{2, 4.947236}, {5, 4.8592587}, {1, 3.9400382}]|\n",
      "|2      |[{3, 3.9432032}, {2, 2.96633}, {4, 2.9080243}] |\n",
      "|3      |[{3, 4.831301}, {4, 3.1713889}, {2, 2.3089547}]|\n",
      "+-------+-----------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate recommendations for each user\n",
    "user_recommendations = model.recommendForAllUsers(numItems=3)\n",
    "\n",
    "# Show recommendations\n",
    "user_recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = [\n",
    "    (1, \"song a\"),\n",
    "    (2, \"song b\"),\n",
    "    (3, \"song c\"),\n",
    "    (4, \"song d\"),\n",
    "    (5, \"song e\")]\n",
    "\n",
    "songs_schema = SparkUtils.generate_schema([(\"song_id\", \"int\"), (\"title\", \"string\")])\n",
    "songs_df = spark.createDataFrame(songs, songs_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 153:==========>   (78 + 1) / 100][Stage 155:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n",
      "|user_id|title |rating   |\n",
      "+-------+------+---------+\n",
      "|1      |song b|4.947236 |\n",
      "|1      |song e|4.8592587|\n",
      "|1      |song a|3.9400382|\n",
      "|1      |song d|2.9627154|\n",
      "|1      |song c|2.913545 |\n",
      "|2      |song c|3.9432032|\n",
      "|2      |song b|2.96633  |\n",
      "|2      |song d|2.9080243|\n",
      "|2      |song a|2.4423301|\n",
      "|2      |song e|2.1428497|\n",
      "|3      |song c|4.831301 |\n",
      "|3      |song d|3.1713889|\n",
      "|3      |song b|2.3089547|\n",
      "|3      |song a|1.9651916|\n",
      "|3      |song e|1.0496502|\n",
      "+-------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Explode recommendations for easier reading\n",
    "recommendations = user_recommendations.select(\"user_id\", explode(\"recommendations\").alias(\"rec\"))\n",
    "recommendations = recommendations.join(songs_df, recommendations.rec.song_id == songs_df.song_id).select(\"user_id\", \"title\", \"rec.rating\")\n",
    "\n",
    "# Show user-song recommendations with titles\n",
    "recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+----------+\n",
      "|user_id|song_id|rating|prediction|\n",
      "+-------+-------+------+----------+\n",
      "|1      |1      |4     |3.9400382 |\n",
      "|1      |2      |5     |4.947236  |\n",
      "|1      |5      |5     |4.8592587 |\n",
      "|2      |2      |3     |2.96633   |\n",
      "|3      |1      |2     |1.9651916 |\n",
      "|3      |3      |5     |4.831301  |\n",
      "|3      |5      |1     |1.0496502 |\n",
      "|2      |3      |4     |3.9432032 |\n",
      "|2      |4      |3     |2.9080243 |\n",
      "+-------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(interactions_df)\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error (RMSE) = 0.08890862503498724\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Recommendation System\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Set up evaluator to compute RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", \n",
    "    labelCol=\"rating\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-mean-square error (RMSE) = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "+-------+--------+------+----------+\n",
      "|user_id|movie_id|rating| timestamp|\n",
      "+-------+--------+------+----------+\n",
      "|      0|       2|     3|1424380312|\n",
      "|      0|       3|     1|1424380312|\n",
      "|      0|       5|     2|1424380312|\n",
      "+-------+--------+------+----------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "movies_ratings_path = \"/opt/spark/work-dir/data/ml/als\"\n",
    "\n",
    "movies_ratings_schema = SparkUtils.generate_schema([(\"user_id\", \"int\"), (\"movie_id\", \"int\"), (\"rating\", \"int\"),(\"timestamp\", \"int\")])\n",
    "\n",
    "# Source https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/sample_movielens_ratings.txt\n",
    "movies_ratings_df = spark.read \\\n",
    "                    .option(\"header\", \"false\") \\\n",
    "                    .option(\"delimiter\", \"::\") \\\n",
    "                    .schema(movies_ratings_schema) \\\n",
    "                    .csv(movies_ratings_path)\n",
    "\n",
    "movies_ratings_df.printSchema()\n",
    "movies_ratings_df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items o movies (n):100\n",
      "Number of users (m):30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of items o movies (n):{movies_ratings_df.groupBy('movie_id').count().count()}\")\n",
    "print(f\"Number of users (m):{movies_ratings_df.groupBy('user_id').count().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Train the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure ALS model\n",
    "als = ALS(\n",
    "    userCol=\"user_id\", \n",
    "    itemCol=\"movie_id\",\n",
    "    ratingCol=\"rating\", \n",
    "    maxIter=25,\n",
    "    regParam=0.05,\n",
    "    rank=5, # Controls the dimensionality of the latent vector space for \n",
    "            # users and items.\n",
    "    coldStartStrategy=\"drop\"  # Avoids NaN predictions\n",
    ")\n",
    "# Train the model (THIS STEP MAY TAKE SOME TIME)\n",
    "als_model = als.fit(movies_ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model_path = \"/opt/spark/work-dir/data/mlmodels/als/als_movies\"\n",
    "als_model.write().overwrite().save(als_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 420:=============================================>        (85 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------+\n",
      "|user_id|recommendations                                                                      |\n",
      "+-------+-------------------------------------------------------------------------------------+\n",
      "|0      |[{92, 2.567516}, {2, 2.2980113}, {62, 2.2533116}, {93, 2.18572}, {25, 2.1733978}]    |\n",
      "|10     |[{92, 2.7359204}, {93, 2.6829367}, {2, 2.6401443}, {25, 2.5964687}, {49, 2.5660012}] |\n",
      "|20     |[{22, 3.577638}, {68, 3.1335194}, {51, 3.096676}, {94, 3.0710056}, {77, 3.0283942}]  |\n",
      "|1      |[{22, 2.9241138}, {68, 2.6358395}, {77, 2.5409534}, {90, 2.515977}, {62, 2.4985847}] |\n",
      "|11     |[{32, 5.098444}, {18, 4.7203503}, {30, 4.640781}, {27, 4.482382}, {79, 4.144085}]    |\n",
      "|21     |[{29, 4.340982}, {52, 4.2697444}, {76, 3.7713296}, {63, 3.5152457}, {53, 3.477858}]  |\n",
      "|22     |[{51, 4.4850426}, {75, 4.4336934}, {22, 4.1221323}, {74, 4.1156454}, {88, 4.0854187}]|\n",
      "|2      |[{93, 4.2608924}, {83, 4.136333}, {8, 4.035904}, {39, 3.7468219}, {2, 3.5693603}]    |\n",
      "|12     |[{46, 5.7865796}, {55, 4.808673}, {49, 4.556037}, {90, 4.2682123}, {48, 4.108876}]   |\n",
      "|23     |[{46, 5.515423}, {55, 4.7244763}, {32, 4.6282973}, {90, 4.6234307}, {49, 4.4407673}] |\n",
      "|3      |[{30, 4.047981}, {74, 3.8262262}, {51, 3.8083932}, {69, 3.807145}, {75, 3.7403758}]  |\n",
      "|13     |[{74, 2.695813}, {93, 2.6103673}, {29, 2.5446362}, {52, 2.4831789}, {53, 2.4453442}] |\n",
      "|24     |[{29, 4.4517136}, {52, 4.4268513}, {30, 3.8739305}, {53, 3.8392613}, {69, 3.7060266}]|\n",
      "|4      |[{29, 3.26125}, {52, 3.1586778}, {62, 2.79272}, {76, 2.7926083}, {93, 2.7918983}]    |\n",
      "|14     |[{29, 4.637094}, {52, 4.597331}, {76, 3.9001129}, {63, 3.8207188}, {85, 3.5231779}]  |\n",
      "|5      |[{46, 4.1468544}, {90, 3.595825}, {55, 3.318556}, {49, 3.2379372}, {94, 3.1786878}]  |\n",
      "|15     |[{46, 3.4577112}, {49, 2.7973864}, {55, 2.6599402}, {90, 2.6231585}, {64, 2.4417667}]|\n",
      "|25     |[{46, 3.1443932}, {49, 3.0247333}, {55, 2.5011685}, {93, 2.4533358}, {91, 2.2964349}]|\n",
      "|6      |[{29, 3.0965314}, {52, 3.0417485}, {25, 3.0243843}, {93, 2.7878106}, {2, 2.7752302}] |\n",
      "|16     |[{22, 3.8124924}, {77, 3.7014937}, {90, 3.656544}, {62, 3.5950313}, {85, 3.5944526}] |\n",
      "+-------+-------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "# Load saved model\n",
    "#als_model = ALSModel.load(als_model_path)\n",
    "\n",
    "# Generate the  top 5 recommendations for each user\n",
    "\n",
    "user_recommendations = als_model.recommendForAllUsers(numItems=5)\n",
    "# Show recommendations\n",
    "user_recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error (RMSE) = 0.5242583719702174\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Generate predictions for all users\n",
    "predictions = als_model.transform(movies_ratings_df)\n",
    "\n",
    "\n",
    "# Set up evaluator to compute RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", \n",
    "    labelCol=\"rating\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-mean-square error (RMSE) = {rmse}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
