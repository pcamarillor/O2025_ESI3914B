{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164ace65",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Machine Learning** </center>\n",
    "---\n",
    "\n",
    "**Date**: October, 2025\n",
    "\n",
    "**Student Name**: Juan Alonso\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599198d2",
   "metadata": {},
   "source": [
    "## Machine Learning algorithm to use\n",
    "\n",
    "En este proyecto se quiere hacer clustering de clientes de tarjeta de crédito.  \n",
    "La idea es agrupar a los clientes según su forma de usar la tarjeta: cuánto gastan, cuánto deben, cuánto adelantan en efectivo, cuánto pagan del mínimo, etc.\n",
    "\n",
    "Se eligió K-Means porque:\n",
    "\n",
    "- Es un algoritmo de aprendizaje no supervisado.\n",
    "- Permite crear segmentos de clientes que tienen comportamientos parecidos.\n",
    "- Es más fácil de entender, cada cluster tiene un centroide que resume el promedio del grupo.\n",
    "- Spark MLlib ya trae una implementación de K-Means que vimos en clase.\n",
    "\n",
    "Con este modelo se busca encontrar grupos como:\n",
    "- Clientes que gastan mucho y pagan a tiempo.\n",
    "- Clientes que usan mucho adelantos en efectivo.\n",
    "- Clientes con poco uso de la tarjeta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e92ec",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "Para este proyecto se usa el dataset \"Credit Card Dataset for Clustering\" de Kaggle  \n",
    "Link: https://www.kaggle.com/datasets/arjunbhasin2013/ccdata.\n",
    "\n",
    "Fuente: Kaggle – Credit Card Dataset for Clustering  \n",
    "Tamaño:\n",
    "- 8,950 clientes.\n",
    "- 18 columnas con información de uso de tarj eta.\n",
    "\n",
    "Algunas columnas importantes son:\n",
    "\n",
    "- `BALANCE`: saldo promedio de la tarjeta.\n",
    "- `PURCHASES`: total de compras hechas.\n",
    "- `ONEOFF_PURCHASES`: compras grandes de una sola exhibición.\n",
    "- `INSTALLMENTS_PURCHASES`: compras en pagos.\n",
    "- `CASH_ADVANCE`: adelantos de efectivo.\n",
    "- `CREDIT_LIMIT`: límite de crédito.\n",
    "- `PAYMENTS`: pagos realizados.\n",
    "- `MINIMUM_PAYMENTS`: pagos mínimos hechos.\n",
    "\n",
    "Como es un problema de clustering, lo que importa es cuántas dimensiones vamos a usar para agrupar. En este caso se seleccionan las 8 columnas identificadas como variables numéricas para usarse como features principales; así que el problema de clustering se hace en un espacio de 8 dimensiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55a4cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Esquema del Dataset ---\n",
      "root\n",
      " |-- CUST_ID: string (nullable = true)\n",
      " |-- BALANCE: double (nullable = true)\n",
      " |-- BALANCE_FREQUENCY: double (nullable = true)\n",
      " |-- PURCHASES: double (nullable = true)\n",
      " |-- ONEOFF_PURCHASES: double (nullable = true)\n",
      " |-- INSTALLMENTS_PURCHASES: double (nullable = true)\n",
      " |-- CASH_ADVANCE: double (nullable = true)\n",
      " |-- PURCHASES_FREQUENCY: double (nullable = true)\n",
      " |-- ONEOFF_PURCHASES_FREQUENCY: double (nullable = true)\n",
      " |-- PURCHASES_INSTALLMENTS_FREQUENCY: double (nullable = true)\n",
      " |-- CASH_ADVANCE_FREQUENCY: double (nullable = true)\n",
      " |-- CASH_ADVANCE_TRX: double (nullable = true)\n",
      " |-- PURCHASES_TRX: double (nullable = true)\n",
      " |-- CREDIT_LIMIT: double (nullable = true)\n",
      " |-- PAYMENTS: double (nullable = true)\n",
      " |-- MINIMUM_PAYMENTS: double (nullable = true)\n",
      " |-- PRC_FULL_PAYMENT: double (nullable = true)\n",
      " |-- TENURE: double (nullable = true)\n",
      "\n",
      "\n",
      "--- Tamaño del Dataset ---\n",
      "Total de filas: 8950\n",
      "Total de columnas: 18\n"
     ]
    }
   ],
   "source": [
    "# Schema definition\n",
    "schema_columns = [\n",
    "    (\"CUST_ID\", \"string\"),\n",
    "    (\"BALANCE\", \"double\"),\n",
    "    (\"BALANCE_FREQUENCY\", \"double\"),\n",
    "    (\"PURCHASES\", \"double\"),\n",
    "    (\"ONEOFF_PURCHASES\", \"double\"),\n",
    "    (\"INSTALLMENTS_PURCHASES\", \"double\"),\n",
    "    (\"CASH_ADVANCE\", \"double\"),\n",
    "    (\"PURCHASES_FREQUENCY\", \"double\"),\n",
    "    (\"ONEOFF_PURCHASES_FREQUENCY\", \"double\"),\n",
    "    (\"PURCHASES_INSTALLMENTS_FREQUENCY\", \"double\"),\n",
    "    (\"CASH_ADVANCE_FREQUENCY\", \"double\"),\n",
    "    (\"CASH_ADVANCE_TRX\", \"double\"),\n",
    "    (\"PURCHASES_TRX\", \"double\"),\n",
    "    (\"CREDIT_LIMIT\", \"double\"),\n",
    "    (\"PAYMENTS\", \"double\"),\n",
    "    (\"MINIMUM_PAYMENTS\", \"double\"),\n",
    "    (\"PRC_FULL_PAYMENT\", \"double\"),\n",
    "    (\"TENURE\", \"double\")\n",
    "]\n",
    "\n",
    "credit_schema = SparkUtils.generate_schema(schema_columns)\n",
    "\n",
    "DATA_PATH = \"/opt/spark/work-dir/data/ml/juanalonso/credit_card.csv\"\n",
    "\n",
    "df_credit = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(credit_schema) \\\n",
    "    .csv(DATA_PATH)\n",
    "\n",
    "print(\"--- Esquema del Dataset ---\")\n",
    "df_credit.printSchema()\n",
    "\n",
    "print(\"\\n--- Tamaño del Dataset ---\")\n",
    "print(f\"Total de filas: {df_credit.count()}\")\n",
    "print(f\"Total de columnas: {len(df_credit.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315af023",
   "metadata": {},
   "source": [
    "## ML Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5f536",
   "metadata": {},
   "source": [
    "Antes de entrenar el modelo, se hacen algunos pasos básicos:\n",
    "\n",
    "1. Se eliminan filas que tengan valores nulos en columnas importantes, para no afectar el entrenamiento.\n",
    "2. Se seleccionan solo las columnas numéricas que se van a usar como features.\n",
    "3. Se usa VectorAssembler para juntar las 8 columnas numéricas en una sola columna llamada features.\n",
    "\n",
    "Se eligió K-Means porque:\n",
    "- Es sencillo de entender.\n",
    "- Funciona bien cuando se quiere agrupar puntos en base a distancia.\n",
    "- Es parte de Spark MLlib, así que se integra fácil al pipeline.\n",
    "\n",
    "Hiperparámetros usados:\n",
    "\n",
    "- `k`: número de clusters.  \n",
    "  En lugar de poner un valor fijo, se probó un rango de k (de 2 a 10).  \n",
    "  Para cada k se entrena un modelo y se calcula el Silhouette Score.  \n",
    "  Se elige el k que tenga el Silhouette más alto.  \n",
    "  Se eligió esta forma porque así el modelo decide cuántos grupos tienen más sentido.\n",
    "\n",
    "- `seed`: valor de semilla aleatoria.  \n",
    "  Se usa `setSeed(42)` para que el resultado sea reproducible.  \n",
    "  Esto quiere decir que, si se vuelve a correr el notebook, el modelo da los mismos clusters.\n",
    "\n",
    "- `featuresCol`: nombre de la columna de entrada para el modelo.  \n",
    "  Se usa `\"features\"`, que es la salida del `VectorAssembler`.\n",
    "\n",
    "Con esto se arma el pipeline simple:\n",
    "datos limpios -> vector assembler -> múltiples modelos K-Means -> elegir mejor k -> entrenar modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab2012b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/25 03:31:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from juanalonso.spark_utils import SparkUtils\n",
    "\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_FinalProject_JuanAlonso\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"SparkSession iniciada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a186c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas despues de dropna: 8636\n",
      "\n",
      "--- Ejemplo de la columna features ---\n",
      "+--------------------------------------------------------------------+\n",
      "|features                                                            |\n",
      "+--------------------------------------------------------------------+\n",
      "|[40.900749,95.4,0.0,95.4,0.0,1000.0,201.802084,139.509787]          |\n",
      "|[3202.467416,0.0,0.0,0.0,6442.945483,7000.0,4103.032597,1072.340217]|\n",
      "|[2495.148862,773.17,773.17,0.0,0.0,7500.0,622.066742,627.284787]    |\n",
      "|[817.714335,16.0,16.0,0.0,0.0,1200.0,678.334763,244.791237]         |\n",
      "|[1809.828751,1333.28,0.0,1333.28,0.0,1800.0,1400.05777,2407.246035] |\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Data cleanup\n",
    "feature_cols = [\n",
    "    \"BALANCE\",\n",
    "    \"PURCHASES\",\n",
    "    \"ONEOFF_PURCHASES\",\n",
    "    \"INSTALLMENTS_PURCHASES\",\n",
    "    \"CASH_ADVANCE\",\n",
    "    \"CREDIT_LIMIT\",\n",
    "    \"PAYMENTS\",\n",
    "    \"MINIMUM_PAYMENTS\"\n",
    "]\n",
    "\n",
    "df_clean = df_credit.dropna(subset=feature_cols)\n",
    "\n",
    "print(f\"Filas despues de dropna: {df_clean.count()}\")\n",
    "\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "assembled_df = vector_assembler.transform(df_clean)\n",
    "\n",
    "print(\"\\n--- Ejemplo de la columna features ---\")\n",
    "assembled_df.select(\"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be56ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Buscando k optimo ---\n",
      "k = 2, Silhouette Score = 0.6441\n",
      "k = 3, Silhouette Score = 0.5983\n",
      "k = 4, Silhouette Score = 0.6074\n",
      "k = 5, Silhouette Score = 0.5198\n",
      "k = 6, Silhouette Score = 0.5285\n",
      "k = 7, Silhouette Score = 0.5341\n",
      "k = 8, Silhouette Score = 0.4446\n",
      "k = 9, Silhouette Score = 0.5410\n",
      "k = 10, Silhouette Score = 0.4970\n",
      "\n",
      "--- Resultados ---\n",
      "El mejor k es: 2 con Silhouette = 0.6441\n"
     ]
    }
   ],
   "source": [
    "# Get best Silhouette Score\n",
    "\n",
    "k_values = range(2, 11)\n",
    "silhouette_scores = {}\n",
    "\n",
    "evaluator = ClusteringEvaluator(\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"silhouette\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Buscando k optimo ---\")\n",
    "for k in k_values:\n",
    "    kmeans = KMeans().setK(k).setSeed(42).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(assembled_df)\n",
    "    \n",
    "    predictions = model.transform(assembled_df)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_scores[k] = silhouette\n",
    "    \n",
    "    print(f\"k = {k}, Silhouette Score = {silhouette:.4f}\")\n",
    "\n",
    "optimal_k = max(silhouette_scores, key=silhouette_scores.get)\n",
    "best_score = silhouette_scores[optimal_k]\n",
    "\n",
    "print(\"\\n--- Resultados ---\")\n",
    "print(f\"El mejor k es: {optimal_k} con Silhouette = {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92154386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo final con k=2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo final guardado en: /opt/spark/work-dir/data/mlmodels/final_project/kmeans_credit_card_clusters\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "\n",
    "MODEL_PATH = \"/opt/spark/work-dir/data/mlmodels/final_project/kmeans_credit_card_clusters\"\n",
    "\n",
    "print(f\"\\nEntrenando modelo final con k={optimal_k} ...\")\n",
    "kmeans_final = KMeans().setK(optimal_k).setSeed(42).setFeaturesCol(\"features\")\n",
    "final_model = kmeans_final.fit(assembled_df)\n",
    "\n",
    "final_model.write().overwrite().save(MODEL_PATH)\n",
    "print(f\"Modelo final guardado en: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d344d19",
   "metadata": {},
   "source": [
    "## ML Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138d88a",
   "metadata": {},
   "source": [
    "Como este es un problema de clustering, no hay etiquetas reales.  \n",
    "Por eso no se puede usar Accuracy, Precision o Recall.\n",
    "\n",
    "Para evaluar el modelo se usa el Silhouette Score, que mide dos cosas:\n",
    "\n",
    "- Qué tan cerca está cada punto de los puntos de su propio cluster, la cohesión.\n",
    "- Qué tan lejos está de los puntos de otros clusters, la separación.\n",
    "\n",
    "El valor va de -1 a 1:\n",
    "\n",
    "- Cerca de 1: clusters bien formados y separados.\n",
    "- Cerca de 0: clusters que se empalman.\n",
    "- cerca de -1: mala asignación de clusters.\n",
    "\n",
    "En este proyecto se usa el Silhouette de dos formas:\n",
    "\n",
    "1. Para elegir el mejor k en la fase de entrenamiento.\n",
    "2. Para evaluar el modelo final cargado desde disco.\n",
    "\n",
    "También se muestran:\n",
    "\n",
    "- Un ejemplo de clientes con su cluster asignado.\n",
    "- El conteo de clientes por cluster.\n",
    "- Los centroides de cada cluster, que ayudan a interpretar cada segmento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo K-Means cargado. k = 2\n",
      "\n",
      "--- Muestra de predicciones ---\n",
      "+-------+-----------+---------+------------+------------+-----------+----------------+----------+\n",
      "|CUST_ID|BALANCE    |PURCHASES|CASH_ADVANCE|CREDIT_LIMIT|PAYMENTS   |MINIMUM_PAYMENTS|prediction|\n",
      "+-------+-----------+---------+------------+------------+-----------+----------------+----------+\n",
      "|C10001 |40.900749  |95.4     |0.0         |1000.0      |201.802084 |139.509787      |0         |\n",
      "|C10002 |3202.467416|0.0      |6442.945483 |7000.0      |4103.032597|1072.340217     |1         |\n",
      "|C10003 |2495.148862|773.17   |0.0         |7500.0      |622.066742 |627.284787      |0         |\n",
      "|C10005 |817.714335 |16.0     |0.0         |1200.0      |678.334763 |244.791237      |0         |\n",
      "|C10006 |1809.828751|1333.28  |0.0         |1800.0      |1400.05777 |2407.246035     |0         |\n",
      "|C10007 |627.260806 |7091.01  |0.0         |13500.0     |6354.314328|198.065894      |1         |\n",
      "|C10008 |1823.652743|436.2    |0.0         |2300.0      |679.065082 |532.03399       |0         |\n",
      "|C10009 |1014.926473|861.49   |0.0         |7000.0      |688.278568 |311.963409      |0         |\n",
      "|C10010 |152.225975 |1281.6   |0.0         |11000.0     |1164.770591|100.302262      |1         |\n",
      "|C10011 |1293.124939|920.12   |0.0         |1200.0      |1083.301007|2172.697765     |0         |\n",
      "+-------+-----------+---------+------------+------------+-----------+----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "--- Evaluacion final ---\n",
      "Silhouette Score: 0.6441\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "loaded_model = KMeansModel.load(MODEL_PATH)\n",
    "print(f\"Modelo K-Means cargado. k = {loaded_model.getK()}\")\n",
    "\n",
    "# Get predictions\n",
    "predictions = loaded_model.transform(assembled_df)\n",
    "\n",
    "print(\"\\n--- Muestra de predicciones ---\")\n",
    "predictions.select(\n",
    "    \"CUST_ID\",\n",
    "    \"BALANCE\",\n",
    "    \"PURCHASES\",\n",
    "    \"CASH_ADVANCE\",\n",
    "    \"CREDIT_LIMIT\",\n",
    "    \"PAYMENTS\",\n",
    "    \"MINIMUM_PAYMENTS\",\n",
    "    \"prediction\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Evaluate Silhouette Score\n",
    "final_silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"\\n--- Evaluacion final ---\")\n",
    "print(f\"Silhouette Score: {final_silhouette:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12dc90cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conteo de clientes por cluster ---\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0| 6797|\n",
      "|         1| 1839|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "--- Centroides de cada cluster ---\n",
      "Clúster 0:\n",
      "  BALANCE promedio:                978.04\n",
      "  PURCHASES promedio:              646.17\n",
      "  ONEOFF_PURCHASES promedio:       338.44\n",
      "  INSTALLMENTS_PURCHASES promedio: 308.01\n",
      "  CASH_ADVANCE promedio:           550.28\n",
      "  CREDIT_LIMIT promedio:           3068.29\n",
      "  PAYMENTS promedio:               1075.95\n",
      "  MINIMUM_PAYMENTS promedio:       610.34\n",
      "\n",
      "Clúster 1:\n",
      "  BALANCE promedio:                3904.52\n",
      "  PURCHASES promedio:              2427.20\n",
      "  ONEOFF_PURCHASES promedio:       1589.74\n",
      "  INSTALLMENTS_PURCHASES promedio: 837.87\n",
      "  CASH_ADVANCE promedio:           2634.84\n",
      "  CREDIT_LIMIT promedio:           9895.38\n",
      "  PAYMENTS promedio:               4403.23\n",
      "  MINIMUM_PAYMENTS promedio:       1802.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Conteo de clientes por cluster ---\")\n",
    "predictions.groupBy(\"prediction\").count().orderBy(\"prediction\").show()\n",
    "\n",
    "print(\"\\n--- Centroides de cada cluster ---\")\n",
    "centers = loaded_model.clusterCenters()\n",
    "for idx, center in enumerate(centers):\n",
    "    print(f\"Clúster {idx}:\")\n",
    "    print(f\"  BALANCE promedio:                {center[0]:.2f}\")\n",
    "    print(f\"  PURCHASES promedio:              {center[1]:.2f}\")\n",
    "    print(f\"  ONEOFF_PURCHASES promedio:       {center[2]:.2f}\")\n",
    "    print(f\"  INSTALLMENTS_PURCHASES promedio: {center[3]:.2f}\")\n",
    "    print(f\"  CASH_ADVANCE promedio:           {center[4]:.2f}\")\n",
    "    print(f\"  CREDIT_LIMIT promedio:           {center[5]:.2f}\")\n",
    "    print(f\"  PAYMENTS promedio:               {center[6]:.2f}\")\n",
    "    print(f\"  MINIMUM_PAYMENTS promedio:       {center[7]:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09c09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
