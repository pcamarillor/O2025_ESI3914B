{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Machine Learning** </center>\n",
    "---\n",
    "\n",
    "**Date**: November, 2025\n",
    "\n",
    "**Student Name**: Luis Antonio Pelayo Sierra\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6174ab2",
   "metadata": {},
   "source": [
    "# Machine Learning algorithm to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e32b7",
   "metadata": {},
   "source": [
    "El problema que este algoritmo de machine learning intentara resolver es uno de clasificacion, en las dos entregas anteriores he experimentado nuevos datos que se pueden agregar al dataset original, uno de ellos siendo una clara clasificacion, que los divide en base a la popularidad del contenido. El objetivo es que en base a la mayoria de los datos, sin utilizar las metricas de likes y retweets, pueda predecir el tipo de impacto que un tweet tendra.\n",
    "\n",
    "Para ello, se implementara un modelo de clasificacion basado en Random Forest, por que considero que por la naturaleza de los datos, es muy probable que exista overfitting, por lo que este algoritmo puede ayudar a disminuir el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c4815",
   "metadata": {},
   "source": [
    "# Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91319ded",
   "metadata": {},
   "source": [
    "El dataset utilizado para esta actividad sera el Twitter-Dataset de Kaggle (https://www.kaggle.com/datasets/goyaladi/twitter-dataset/data), el cual consiste en informacion de 10,000 tweets del primer semestre de 2023, lo que incluye una gran variedad de usuarios, texto, cuentas de retweets y likes, ademas de los timestamps asociados a cada tweet. \n",
    "\n",
    "El dataset base contiene 6 columnas, ademas a estas, he decidido agregar nuevas columnas derivadas de la informacion base (similar a mis entregas pasadas, tanto las mismas como nuevas), con el fin de facilitar las predicciones, puesto que quitando los contadores de likes y retweets, habria muy pocas features relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a072308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Machine Learning\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98f3db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from luis_pelayo.spark_utils import SparkUtils\n",
    "\n",
    "tweets_schema = SparkUtils.generate_schema([\n",
    "    (\"Tweet_ID\", \"int\"),\n",
    "    (\"Username\", \"string\"),\n",
    "    (\"Text\", \"string\"),\n",
    "    (\"Retweets\", \"int\"),\n",
    "    (\"Likes\", \"int\"),\n",
    "    (\"Timestamp\", \"string\")\n",
    "])\n",
    "\n",
    "tweets_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(tweets_schema) \\\n",
    "    .csv(\"/opt/spark/work-dir/data/machine_learning/twitter_dataset.csv\")\n",
    "\n",
    "tweets_df = tweets_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3592f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, size, length, split, to_timestamp, hour, dayofweek, month, year, dayofmonth\n",
    "\n",
    "tweets_df = tweets_df.withColumn(\"Engagement\", col(\"Likes\") + col(\"Retweets\"))\n",
    "tweets_df = tweets_df.withColumn(\"Text_Length\", length(col(\"Text\")))\n",
    "tweets_df = tweets_df.withColumn(\"Word_Count\", size(split(col(\"Text\"), \" \")))\n",
    "\n",
    "tweets_df = tweets_df.withColumn(\n",
    "    \"Avg_Word_Length\",\n",
    "    when(col(\"Word_Count\") > 0, col(\"Text_Length\") / col(\"Word_Count\")).otherwise(0)\n",
    ")\n",
    "\n",
    "tweets_df = tweets_df.withColumn(\n",
    "    \"Hashtag_Count\",\n",
    "    size(split(col(\"Text\"), \"#\")) - 1\n",
    ")\n",
    "\n",
    "tweets_df = tweets_df.withColumn(\"Timestamp_parsed\", to_timestamp(col(\"Timestamp\")))\n",
    "tweets_df = tweets_df.withColumn(\"Hour\", hour(col(\"Timestamp_parsed\")))\n",
    "tweets_df = tweets_df.withColumn(\"Day\", dayofmonth(col(\"Timestamp_parsed\")))\n",
    "tweets_df = tweets_df.withColumn(\"Month\", month(col(\"Timestamp_parsed\")))\n",
    "tweets_df = tweets_df.withColumn(\"Year\", year(col(\"Timestamp_parsed\")))\n",
    "tweets_df = tweets_df.withColumn(\"DayOfWeek\", dayofweek(col(\"Timestamp_parsed\")))\n",
    "\n",
    "tweets_df = tweets_df.withColumn(\n",
    "    \"Is_Weekend\",\n",
    "    when((col(\"DayOfWeek\") == 1) | (col(\"DayOfWeek\") == 7), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "tweets_df = tweets_df.withColumn(\n",
    "    \"Time_Category\",\n",
    "    when((col(\"Hour\") >= 6) & (col(\"Hour\") < 12), 0) \n",
    "    .when((col(\"Hour\") >= 12) & (col(\"Hour\") < 18), 1)  \n",
    "    .when((col(\"Hour\") >= 18) & (col(\"Hour\") < 22), 2) \n",
    "    .otherwise(3)  \n",
    ")\n",
    "\n",
    "# Label (antes era popularidad)\n",
    "tweets_df = tweets_df.withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"Engagement\") >= 100, 3) # A mayor numero, mayor popularidad (nivel de popularidad)\n",
    "    .when(col(\"Engagement\") >= 50, 2)\n",
    "    .when(col(\"Engagement\") >= 10, 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# tweets_df.printSchema()\n",
    "# tweets_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e8662",
   "metadata": {},
   "source": [
    "# ML Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a9c6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance_counts = tweets_df.groupBy(\"label\").count().orderBy(\"label\")\n",
    "# balance_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ab2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [ \"Text_Length\", \"Word_Count\", \"Avg_Word_Length\", \"Hashtag_Count\", \"Hour\", \n",
    "                \"Day\", \"Month\", \"DayOfWeek\", \"Is_Weekend\", \"Time_Category\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(tweets_df)\n",
    "assembled_df = assembled_df.select(\"features\", \"label\")\n",
    "\n",
    "train_df, test_df = assembled_df.randomSplit([0.8, 0.2], seed=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c83e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 00:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1038.6 KiB\n",
      "25/11/25 00:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1764.0 KiB\n",
      "25/11/25 00:12:34 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/11/25 00:12:35 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/11/25 00:12:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB\n",
      "25/11/25 00:12:38 WARN DAGScheduler: Broadcasting large task binary with size 1287.0 KiB\n",
      "25/11/25 00:12:39 WARN DAGScheduler: Broadcasting large task binary with size 9.2 MiB\n",
      "25/11/25 00:12:40 WARN DAGScheduler: Broadcasting large task binary with size 1600.0 KiB\n",
      "25/11/25 00:12:41 WARN DAGScheduler: Broadcasting large task binary with size 12.3 MiB\n",
      "25/11/25 00:12:42 WARN DAGScheduler: Broadcasting large task binary with size 1838.4 KiB\n",
      "25/11/25 00:12:44 WARN DAGScheduler: Broadcasting large task binary with size 15.6 MiB\n",
      "25/11/25 00:12:45 WARN DAGScheduler: Broadcasting large task binary with size 2002.2 KiB\n",
      "25/11/25 00:12:44 WARN DAGScheduler: Broadcasting large task binary with size 19.0 MiB\n",
      "25/11/25 00:12:45 WARN DAGScheduler: Broadcasting large task binary with size 2033.3 KiB\n",
      "25/11/25 00:12:47 WARN DAGScheduler: Broadcasting large task binary with size 22.3 MiB\n",
      "25/11/25 00:12:48 WARN DAGScheduler: Broadcasting large task binary with size 1934.2 KiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=200,\n",
    "    maxDepth=20,\n",
    "    seed=57\n",
    ")\n",
    "\n",
    "rf_trained = rf_model.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b749e",
   "metadata": {},
   "source": [
    "# ML Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a3e2581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 00:12:51 WARN DAGScheduler: Broadcasting large task binary with size 10.3 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.4091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 00:12:52 WARN DAGScheduler: Broadcasting large task binary with size 10.3 MiB\n",
      "[Stage 39:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions = rf_trained.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                            predictionCol=\"prediction\")\n",
    "\n",
    "f1 = evaluator.evaluate(predictions, \n",
    "            {evaluator.metricName: \"f1\"})\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, \n",
    "            {evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab14052",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "Se que para muchos, estos valores se podrian considerar un fracaso, intente reajustar el entrenamiento, modificar las propiedades del modelo, pero basicamente los resultados eran los mismos, pero hay una justificacion. La popularidad de un tweet no se basa unicamente en los datos del mismo, un factor primordial son los usuarios, tanto los que publican los tweets, como con los que interactuan mediante likes o respuestas. El modelo podria ser mas acertado si tuviera acceso a datos como los promedios historicos de popularidad del usuario que publica, pero siempre existiran variantes que impidan determinar cual va a ser el proximo \"hit tweet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d803fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
