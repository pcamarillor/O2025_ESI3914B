{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Autumn 2025** </center>\n",
    "---\n",
    "### <center> **Poryect Machine Learning: Logistic Regression** </center>\n",
    "---\n",
    "**Profesor**: Pablo Camarillo Ramirez\n",
    "\n",
    "Diego Orozco Alvarado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning algorithm to use (10 points)\n",
    "\n",
    "El problema que busco resolver es clasificar si un usuario va a dar “like” a un video recomendado o no, utilizando información de comportamiento como tiempo de visualización, categoría del video, dispositivo, interacciones previas, entre otros atributos.\n",
    "Este problema es naturalmente un caso de clasificación binaria, ya que la variable objetivo (liked) solo toma dos valores: 0 (no le dio like) y 1 (sí le dio like).\n",
    "\n",
    "Elegí utilizar Logistic Regression por que:\n",
    "\n",
    "- Es un modelo ampliamente utilizado para clasificación binaria y proporciona una interpretación clara en términos de probabilidades.\n",
    "- Escala bien para datasets grandes gracias a su implementación distribuida en PySpark.\n",
    "- Permite inspeccionar coeficientes y contribuciones de cada feature, lo cual resulta útil para entender los factores que influyen en que un usuario dé like.\n",
    "\n",
    "Ademas dado que el objetivo de esa parte del proyecto es construir un modelo de los vistos en clase, me parecio adecuado usar este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description (20 points)\n",
    "\n",
    "El dataset utilizado proviene de Kaggle:\n",
    "“YouTube Recommendation Data for Cleaning and ML”\n",
    "(https://www.kaggle.com/datasets/iitanshravan/youtube-recommendation-data-for-cleaning-and-ml)\n",
    "\n",
    "Este dataset contiene información sobre la interacción de usuarios con videos recomendados, incluyendo métricas de visualización, dispositivos, categorías, horario del día, acciones del usuario (likes, comentarios, suscripciones), entre otras.\n",
    "\n",
    "El dataset original contiene aproximadamente:\n",
    "\n",
    "40,000+ filas (registros de interacciones usuario–video)\n",
    "\n",
    "columnas: user_id, video_id, video_duration, watch_time, liked, commented, subscribed_after, category, device, watch_time_of_day, recommended, clicked, timestamp, watch_percent\n",
    "\n",
    "#### is Balanced?\n",
    "\n",
    "Como el objetivo del proyecto es resolver un problema de clasificación, se analizó la distribución de la clase objetivo liked usando PySpark. (como se muestra mas adelante)\n",
    "Los resultados muestran que:\n",
    "\n",
    "La clase 0 (no like) es más frecuente.\n",
    "\n",
    "La clase 1 (like) aparece menos en proporción.\n",
    "\n",
    "Esto significa que el dataset no está perfectamente balanceado, aunque no presenta un desbalance extremo.\n",
    "En este escenario, en lugar de usar pesos (weightCol), se optó por usar un ajuste directo del umbral de clasificación (threshold) en el modelo, lo cual permite controlar la sensibilidad del modelo sin modificar las proporciones del dataset.\n",
    "\n",
    "Features Utilizados\n",
    "\n",
    "El dataset mezcla características numéricas y categóricas:\n",
    "- Numéricas: watch_time, video_duration, watch_percent, interacciones históricas, etc.\n",
    "- Categóricas: device, category, watch_time_of_day.\n",
    "- Objetivo: liked.\n",
    "\n",
    "Para poder usar este algoritmo todas las variables categóricas fueron transformadas usando StringIndexer y OneHotEncoder para crear un vector de características adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Training Process (30 points)\n",
    "\n",
    "Para entrenar el modelo se implementó un pipeline completo en PySpark.\n",
    "\n",
    "Data Transformation Steps\n",
    "\n",
    "1 Conversión de tipos\n",
    "- Se normalizaron los tipos: columnas numéricas a double, categóricas a string.\n",
    "\n",
    "2 Indexación de columnas categóricas\n",
    "- Con StringIndexer se generaron índices numéricos consistentes.\n",
    "\n",
    "3 One-Hot Encoding\n",
    "- Las categorías indexadas se transformaron en vectores dispersos mediante OneHotEncoder.\n",
    "\n",
    "4 VectorAssembler\n",
    "- Todas las características numéricas + OHE se integraron en un único vector features.\n",
    "\n",
    "Train/Test Split\n",
    "Se dividió el dataset en:\n",
    "- 80% entrenamiento\n",
    "- 20% prueba\n",
    "\n",
    "Logistic Regression config:\n",
    "- El modelo utilizado fue Logistic Regression, configurado de la siguiente manera:\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"liked\",\n",
    "    threshold=0.45\n",
    ")\n",
    "\n",
    "Hyperparameters\n",
    "\n",
    "Además del threshold, se usaron los parámetros estándar:\n",
    "\n",
    "- featuresCol=\"features\": vector ensamblado.\n",
    "- labelCol=\"liked\": variable objetivo.\n",
    "- maxIter=100 (por defecto): asegura convergencia del modelo.\n",
    "- Regularización por defecto (regParam estándar de Spark): controla el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Evaluation (20 points)\n",
    "\n",
    "Para evaluar el modelo se usaron las métricas estándar para clasificación binaria:\n",
    "\n",
    "Metrics used\n",
    "\n",
    "- Accuracy: proporción de predicciones correctas.\n",
    "- Precision: qué tan confiables son las predicciones positivas.\n",
    "- Recall: capacidad del modelo para detectar correctamente los casos positivos.\n",
    "- F1 Score: balance entre precision y recall.\n",
    "\n",
    "Estas métricas fueron calculadas usando BinaryClassificationEvaluator y funciones de PySpark.\n",
    "\n",
    "Prediction Process\n",
    "\n",
    "Una vez entrenado el modelo se aplicó el modelo al conjunto de prueba (test_df).\n",
    "\n",
    "El modelo generó columnas:\n",
    "\n",
    "- rawPrediction\n",
    "- probability (vector con probabilidades de cada clase)\n",
    "- prediction (clase binaria final)\n",
    "\n",
    "A partir de estas columnas se computaron las métricas mencionadas.\n",
    "\n",
    "El modelo obtuvo resultados coherentes y consistentes, mostrando que es capaz de capturar patrones en el comportamiento del usuario respecto a la probabilidad de dar like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/22 22:50:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML: Logistic Regression\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir/final_project/machine_learning\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83M\t../../data/data_proy/ml_regresion\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../../data/data_proy/ml_regresion\n",
    "base_path = \"/opt/spark/work-dir/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from diego_orozco.spark_utils import SparkUtils\n",
    "\n",
    "df = spark.read.option(\"header\", True).csv(base_path+\"/data_proy/ml_regresion\") # hay strings mezclados (por eso no uso mi schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- video_duration: string (nullable = true)\n",
      " |-- watch_time: string (nullable = true)\n",
      " |-- liked: string (nullable = true)\n",
      " |-- commented: string (nullable = true)\n",
      " |-- subscribed_after: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- watch_time_of_day: string (nullable = true)\n",
      " |-- recommended: string (nullable = true)\n",
      " |-- clicked: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- watch_percent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, regexp_replace\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"watch_percent\",\n",
    "    regexp_replace(\"watch_percent\", \"Infinity|-Infinity|NaN\", None)\n",
    ")\n",
    "\n",
    "# columnas numéricas a float\n",
    "df = df.withColumn(\"watch_time\", col(\"watch_time\").cast(\"float\")) \\\n",
    "    .withColumn(\"video_duration\", col(\"video_duration\").cast(\"float\")) \\\n",
    "    .withColumn(\"watch_percent\", col(\"watch_percent\").cast(\"float\"))\n",
    "\n",
    "df = df.fillna({\"watch_percent\": 0.0})\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"watch_time\",\n",
    "    when(col(\"watch_time\") < 0, 0).otherwise(col(\"watch_time\"))\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"video_duration\",\n",
    "    when(col(\"video_duration\") <= 0, 1).otherwise(col(\"video_duration\"))\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"watch_percent\",\n",
    "    when(col(\"watch_percent\") <= 0, 1).otherwise(col(\"watch_percent\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lower\n",
    "\n",
    "df = df.withColumn(\"subscribed_after\", col(\"subscribed_after\").cast(\"string\"))\n",
    "df = df.withColumn(\"recommended\", col(\"recommended\").cast(\"string\"))\n",
    "df = df.withColumn(\"commented\", col(\"commented\").cast(\"string\"))\n",
    "df = df.withColumn(\"clicked\", col(\"clicked\").cast(\"string\"))\n",
    "df = df.withColumn(\"liked\", col(\"liked\").cast(\"string\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"commented\",\n",
    "    when(lower(col(\"commented\")).isin(\"1\", \"yes\"), 1)\n",
    "    .when(lower(col(\"commented\")).isin(\"0\", \"no\"), 0)\n",
    "    .otherwise(None)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"subscribed_after\",\n",
    "    when(lower(col(\"subscribed_after\")).isin(\"1\", \"yes\"), 1)\n",
    "    .when(lower(col(\"subscribed_after\")).isin(\"0\", \"no\"), 0)\n",
    "    .otherwise(None)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"recommended\",\n",
    "    when(lower(col(\"recommended\")).isin(\"1\", \"yes\"), 1)\n",
    "    .when(lower(col(\"recommended\")).isin(\"0\", \"no\"), 0)\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"clicked\",\n",
    "    when(lower(col(\"clicked\")).isin(\"1\", \"yes\"), 1)\n",
    "    .when(lower(col(\"clicked\")).isin(\"0\", \"no\"), 0)\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"liked\",\n",
    "    when(lower(col(\"liked\")).isin(\"1\", \"yes\"), 1)\n",
    "    .when(lower(col(\"liked\")).isin(\"0\", \"no\"), 0)\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "df = df.filter(col(\"liked\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"device\", lower(col(\"device\")))\n",
    "df = df.withColumn(\n",
    "    \"device\",\n",
    "    when(col(\"device\").like(\"%mobil%\"), \"mobile\")\n",
    "    .when(col(\"device\").like(\"%desk%\"), \"desktop\")\n",
    "    .when(col(\"device\").like(\"%tabl%\"), \"tablet\")\n",
    "    .otherwise(col(\"device\"))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"category\", lower(col(\"category\")))\n",
    "df = df.withColumn(\n",
    "    \"category\",\n",
    "    when(col(\"category\").like(\"music%\"), \"music\")\n",
    "    .when(col(\"category\").like(\"educ%\"), \"education\")\n",
    "    .when(col(\"category\").like(\"gaming%\"), \"gaming\")\n",
    "    .otherwise(col(\"category\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código PySpark ver y corregir el balanceo del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|liked| count|\n",
      "+-----+------+\n",
      "|    0|697143|\n",
      "|    1|299802|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|liked|         percentage|\n",
      "+-----+-------------------+\n",
      "|    0| 0.6992792982561726|\n",
      "|    1|0.30072070174382737|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(\"liked\").count().show()\n",
    "\n",
    "df.groupBy(\"liked\") \\\n",
    "  .agg((F.count(\"*\") / df.count()).alias(\"percentage\")) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cantidad de positivos y negativos\n",
    "counts = df.groupBy(\"liked\").count().collect()\n",
    "count_0 = [row['count'] for row in counts if row['liked'] == 0][0]\n",
    "count_1 = [row['count'] for row in counts if row['liked'] == 1][0]\n",
    "\n",
    "# Clase minoritaria\n",
    "min_count = min(count_0, count_1)\n",
    "\n",
    "# Submuestrear ambos grupos al tamaño de la clase minoritaria\n",
    "df_0 = df.filter(\"liked = 0\").sample(False, min_count / count_0)\n",
    "df_1 = df.filter(\"liked = 1\").sample(False, min_count / count_1)\n",
    "\n",
    "# DataFrame balanceado\n",
    "df = df_0.union(df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|liked| count|\n",
      "+-----+------+\n",
      "|    0|299688|\n",
      "|    1|299802|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|liked|        percentage|\n",
      "+-----+------------------+\n",
      "|    0|0.4999049191813041|\n",
      "|    1|0.5000950808186959|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"liked\").count().show()\n",
    "\n",
    "df.groupBy(\"liked\") \\\n",
    "  .agg((F.count(\"*\") / df.count()).alias(\"percentage\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columnas finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- watch_time: double (nullable = true)\n",
      " |-- video_duration: double (nullable = true)\n",
      " |-- watch_percent: double (nullable = false)\n",
      " |-- recommended: integer (nullable = true)\n",
      " |-- clicked: integer (nullable = true)\n",
      " |-- subscribed_after: integer (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- watch_time_of_day: string (nullable = true)\n",
      " |-- commented: integer (nullable = true)\n",
      " |-- liked: integer (nullable = true)\n",
      "\n",
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+\n",
      "|watch_time|video_duration|watch_percent|recommended|clicked|subscribed_after| device| category|watch_time_of_day|commented|liked|\n",
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+\n",
      "|     389.0|         389.0|          1.0|          0|      0|               0|desktop|   sports|        Afternoon|        0|    0|\n",
      "|     962.0|         962.0|          1.0|          0|      0|               0|desktop|     news|            Night|        0|    0|\n",
      "|    1710.0|        1710.0|          1.0|          1|      0|               0|desktop|   gaming|        Afternoon|        0|    0|\n",
      "|     986.0|        1396.0|          1.0|          1|      0|               0|     tv|lifestyle|        Afternoon|        0|    0|\n",
      "|    1504.0|        2793.0|          1.0|          1|      0|               0| mobile|education|          Morning|        1|    0|\n",
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "cols_keep = [\"watch_time\",\"video_duration\",\"watch_percent\",\n",
    "             \"recommended\",\"clicked\",\"subscribed_after\",\n",
    "             \"device\",\"category\",\"watch_time_of_day\",\n",
    "             \"commented\",\"liked\"]\n",
    "df = df.select(*[c for c in cols_keep if c in df.columns])\n",
    "df.printSchema()\n",
    "df.show(5, truncate=120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODING de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "categorical_cols = [\"device\", \"category\", \"watch_time_of_day\"]\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[c + \"_idx\" for c in categorical_cols],\n",
    "    outputCols=[c + \"_ohe\" for c in categorical_cols]\n",
    ")\n",
    "\n",
    "numeric_features = [\n",
    "    \"watch_time\", \"video_duration\", \"watch_percent\",\n",
    "    \"recommended\", \"clicked\", \"subscribed_after\",\"commented\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the features into a single vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "assembler_inputs = numeric_features + [c + \"_ohe\" for c in categorical_cols]\n",
    "\n",
    "assembler = VectorAssembler( inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\" )\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder] + [assembler])\n",
    "\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df_transformed = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "#### 80% training data and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df_transformed.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show dataset (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+\n",
      "|watch_time|video_duration|watch_percent|recommended|clicked|subscribed_after| device| category|watch_time_of_day|commented|liked|\n",
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+\n",
      "|     389.0|         389.0|          1.0|          0|      0|               0|desktop|   sports|        Afternoon|        0|    0|\n",
      "|     962.0|         962.0|          1.0|          0|      0|               0|desktop|     news|            Night|        0|    0|\n",
      "|    1710.0|        1710.0|          1.0|          1|      0|               0|desktop|   gaming|        Afternoon|        0|    0|\n",
      "|     986.0|        1396.0|          1.0|          1|      0|               0|     tv|lifestyle|        Afternoon|        0|    0|\n",
      "|    1504.0|        2793.0|          1.0|          1|      0|               0| mobile|education|          Morning|        1|    0|\n",
      "|    1449.0|        1449.0|          1.0|          0|      0|               0| mobile|   sports|          Evening|        0|    0|\n",
      "|    1023.0|        1023.0|          1.0|          0|      1|               0|desktop|   gaming|          Morning|        0|    0|\n",
      "|    2590.0|        3299.0|          1.0|          0|      0|               0| mobile|    music|        Afternoon|        0|    0|\n",
      "|    1329.0|        1329.0|          1.0|          0|      0|               0| tablet|lifestyle|          Evening|        0|    0|\n",
      "|    2264.0|        2264.0|          1.0|          0|      1|               0|desktop|   gaming|          Morning|        0|    0|\n",
      "|    2354.0|        2354.0|          1.0|          0|      1|               1|     tv|lifestyle|        Afternoon|        0|    0|\n",
      "|    1816.0|        1816.0|          1.0|          1|      1|               0|desktop|   comedy|          Morning|        0|    0|\n",
      "|    1005.0|        1005.0|          1.0|          1|      0|               0| mobile|   sports|          Morning|        0|    0|\n",
      "|     837.0|         837.0|          1.0|          1|      1|               0|desktop|     tech|        Afternoon|        0|    0|\n",
      "|    1440.0|        1440.0|          1.0|          0|      0|               0|desktop|     tech|        Afternoon|        0|    0|\n",
      "|     258.0|         778.0|          1.0|          1|      0|               0| mobile|     tech|          Evening|        0|    0|\n",
      "|    1110.0|        1110.0|          1.0|          1|      1|               0| mobile|    music|          Evening|        1|    0|\n",
      "|     865.0|         865.0|          1.0|          0|      1|               0| mobile|   sports|          Morning|        0|    0|\n",
      "|    1161.0|        2182.0|          1.0|          0|      0|               0|desktop|   comedy|            Night|        0|    0|\n",
      "|    3136.0|        3136.0|          1.0|          1|      0|               1|desktop|   sports|        Afternoon|        0|    0|\n",
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+----------+------------+---------------------+-------------+--------------+---------------------+--------------------+\n",
      "|watch_time|video_duration|watch_percent|recommended|clicked|subscribed_after| device| category|watch_time_of_day|commented|liked|device_idx|category_idx|watch_time_of_day_idx|   device_ohe|  category_ohe|watch_time_of_day_ohe|            features|\n",
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+----------+------------+---------------------+-------------+--------------+---------------------+--------------------+\n",
      "|       0.0|      100000.0|          1.0|          0|      0|               0|desktop|   comedy|          Evening|        0|    0|       2.0|         1.0|                  0.0|(4,[2],[1.0])|(10,[1],[1.0])|        (4,[0],[1.0])|(25,[1,2,9,12,21]...|\n",
      "|       0.0|      100000.0|          1.0|          0|      0|               0|desktop|       ed|          Morning|        0|    0|       2.0|         9.0|                  1.0|(4,[2],[1.0])|(10,[9],[1.0])|        (4,[1],[1.0])|(25,[1,2,9,20,22]...|\n",
      "|       0.0|      100000.0|          1.0|          0|      0|               0| mobile|   comedy|          Evening|        0|    0|       1.0|         1.0|                  0.0|(4,[1],[1.0])|(10,[1],[1.0])|        (4,[0],[1.0])|(25,[1,2,8,12,21]...|\n",
      "|       0.0|      100000.0|          1.0|          0|      0|               0| mobile|       ed|          Evening|        0|    0|       1.0|         9.0|                  0.0|(4,[1],[1.0])|(10,[9],[1.0])|        (4,[0],[1.0])|(25,[1,2,8,20,21]...|\n",
      "|       0.0|      100000.0|          1.0|          0|      0|               0| mobile|   gaming|          Evening|        0|    0|       1.0|         2.0|                  0.0|(4,[1],[1.0])|(10,[2],[1.0])|        (4,[0],[1.0])|(25,[1,2,8,13,21]...|\n",
      "|       0.0|      100000.0|          1.0|          0|      0|               0| mobile|    music|          Evening|        0|    0|       1.0|         0.0|                  0.0|(4,[1],[1.0])|(10,[0],[1.0])|        (4,[0],[1.0])|(25,[1,2,8,11,21]...|\n",
      "|       0.0|      100000.0|          1.0|          0|      0|               0| tablet|    music|          Morning|        0|    0|       3.0|         0.0|                  1.0|(4,[3],[1.0])|(10,[0],[1.0])|        (4,[1],[1.0])|(25,[1,2,10,11,22...|\n",
      "|       0.0|      100000.0|          1.0|          0|      0|               1|desktop|       ed|            Night|        1|    0|       2.0|         9.0|                  3.0|(4,[2],[1.0])|(10,[9],[1.0])|        (4,[3],[1.0])|(25,[1,2,5,6,9,20...|\n",
      "|       0.0|      100000.0|          1.0|          0|      1|               0|desktop|       ed|            Night|        0|    0|       2.0|         9.0|                  3.0|(4,[2],[1.0])|(10,[9],[1.0])|        (4,[3],[1.0])|(25,[1,2,4,9,20,2...|\n",
      "|       0.0|      100000.0|          1.0|          0|      1|               0| mobile|   comedy|        Afternoon|        0|    0|       1.0|         1.0|                  2.0|(4,[1],[1.0])|(10,[1],[1.0])|        (4,[2],[1.0])|(25,[1,2,4,8,12,2...|\n",
      "|       0.0|      100000.0|          1.0|          1|      0|               0|desktop|       ed|          Evening|        0|    0|       2.0|         9.0|                  0.0|(4,[2],[1.0])|(10,[9],[1.0])|        (4,[0],[1.0])|(25,[1,2,3,9,20,2...|\n",
      "|       0.0|      100000.0|          1.0|          1|      0|               0|desktop|    music|            Night|        0|    0|       2.0|         0.0|                  3.0|(4,[2],[1.0])|(10,[0],[1.0])|        (4,[3],[1.0])|(25,[1,2,3,9,11,2...|\n",
      "|       0.0|      100000.0|          1.0|          1|      0|               0|desktop|    music|            Night|        0|    0|       2.0|         0.0|                  3.0|(4,[2],[1.0])|(10,[0],[1.0])|        (4,[3],[1.0])|(25,[1,2,3,9,11,2...|\n",
      "|       0.0|      100000.0|          1.0|          1|      0|               0| mobile|    tech |          Morning|        1|    0|       1.0|         8.0|                  1.0|(4,[1],[1.0])|(10,[8],[1.0])|        (4,[1],[1.0])|(25,[1,2,3,6,8,19...|\n",
      "|       0.0|      100000.0|          1.0|          1|      0|               0| tablet|   gaming|          Evening|        0|    0|       3.0|         2.0|                  0.0|(4,[3],[1.0])|(10,[2],[1.0])|        (4,[0],[1.0])|(25,[1,2,3,10,13,...|\n",
      "|       0.0|      100000.0|          1.0|          1|      0|               0|     tv|       ed|          Morning|        0|    0|       0.0|         9.0|                  1.0|(4,[0],[1.0])|(10,[9],[1.0])|        (4,[1],[1.0])|(25,[1,2,3,7,20,2...|\n",
      "|       0.0|      100000.0|          1.0|          1|      0|               0|     tv|    music|          Evening|        0|    0|       0.0|         0.0|                  0.0|(4,[0],[1.0])|(10,[0],[1.0])|        (4,[0],[1.0])|(25,[1,2,3,7,11,2...|\n",
      "|       0.0|      100000.0|          1.0|          1|      0|               0|     tv|    music|          Morning|        0|    0|       0.0|         0.0|                  1.0|(4,[0],[1.0])|(10,[0],[1.0])|        (4,[1],[1.0])|(25,[1,2,3,7,11,2...|\n",
      "|       5.0|          52.0|          1.0|          0|      0|               0|desktop|education|        Afternoon|        0|    0|       2.0|         3.0|                  2.0|(4,[2],[1.0])|(10,[3],[1.0])|        (4,[2],[1.0])|(25,[0,1,2,9,14,2...|\n",
      "|       5.0|          78.0|          1.0|          1|      0|               0| mobile|education|          Morning|        0|    0|       1.0|         3.0|                  1.0|(4,[1],[1.0])|(10,[3],[1.0])|        (4,[1],[1.0])|(25,[0,1,2,3,8,14...|\n",
      "+----------+--------------+-------------+-----------+-------+----------------+-------+---------+-----------------+---------+-----+----------+------------+---------------------+-------------+--------------+---------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset\")\n",
    "df.show()\n",
    "\n",
    "# Print train dataset\n",
    "print(\"train set\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(train_df)\n",
    "train_scaled = scaler_model.transform(train_df)\n",
    "test_scaled  = scaler_model.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"liked\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/22 22:52:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/22 22:52:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0028860835682247357,0.012991825620190436,0.0,0.00023686316036718375,0.0015476308741016198,0.001631676094180211,0.006303280443736069,-0.0006228164725007162,0.0004996278817889857,0.0002669912377043324,-0.00014317732859853995,0.004268190606815065,-0.0003553710641351675,-0.003276011244396921,0.0013998257022772495,-0.0012413634005454832,0.0008263627955438581,-0.0025798110100449067,-0.00046159394956371287,0.004583213987423227,0.008973123972715812,0.00042249219894984765,-0.0005521291595423634,0.001047551221454139,-0.00091672174002509]\n",
      "Intercept: 0.0004574609931941647\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_scaled)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients:\", lr_model.coefficients)\n",
    "print(\"Intercept:\", lr_model.intercept)\n",
    "\n",
    "# Display model summary\n",
    "training_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|liked|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|0    |1.0       |[0.36900792709623526,0.6309920729037648]|\n",
      "|0    |1.0       |[0.36782690569589405,0.632173094304106] |\n",
      "|0    |1.0       |[0.31225468022867225,0.6877453197713277]|\n",
      "|0    |1.0       |[0.3635568432748836,0.6364431567251164] |\n",
      "|0    |1.0       |[0.3683961170493194,0.6316038829506806] |\n",
      "|0    |0.0       |[0.5011703944523913,0.4988296055476087] |\n",
      "|0    |0.0       |[0.5004033953517848,0.4995966046482152] |\n",
      "|0    |1.0       |[0.49901046358189133,0.5009895364181087]|\n",
      "|0    |1.0       |[0.49810736996315447,0.5018926300368456]|\n",
      "|0    |0.0       |[0.5025113328646585,0.49748866713534146]|\n",
      "|0    |0.0       |[0.5005813286836277,0.4994186713163723] |\n",
      "|0    |1.0       |[0.49764395140480033,0.5023560485951997]|\n",
      "|0    |0.0       |[0.501798528756852,0.49820147124314795] |\n",
      "|0    |0.0       |[0.5031235644078774,0.4968764355921226] |\n",
      "|0    |0.0       |[0.5008978099161518,0.49910219008384815]|\n",
      "|0    |1.0       |[0.49553385299104635,0.5044661470089536]|\n",
      "|0    |0.0       |[0.5004971894500898,0.4995028105499102] |\n",
      "|0    |0.0       |[0.5040207941807427,0.49597920581925725]|\n",
      "|0    |1.0       |[0.49683517908709957,0.5031648209129005]|\n",
      "|0    |0.0       |[0.5010397410072892,0.4989602589927108] |\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_scaled)\n",
    "\n",
    "predictions.select(\"liked\", \"prediction\", \"probability\").show(20, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|65016|\n",
      "|       1.0|54896|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "predictions.groupBy(\"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|liked| count|\n",
      "+-----+------+\n",
      "|    0|239736|\n",
      "|    1|239842|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "train_scaled.groupBy(\"liked\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- watch_time: double (nullable = true)\n",
      " |-- video_duration: double (nullable = true)\n",
      " |-- watch_percent: double (nullable = false)\n",
      " |-- recommended: integer (nullable = true)\n",
      " |-- clicked: integer (nullable = true)\n",
      " |-- subscribed_after: integer (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- watch_time_of_day: string (nullable = true)\n",
      " |-- commented: integer (nullable = true)\n",
      " |-- liked: integer (nullable = true)\n",
      " |-- device_idx: double (nullable = false)\n",
      " |-- category_idx: double (nullable = false)\n",
      " |-- watch_time_of_day_idx: double (nullable = false)\n",
      " |-- device_ohe: vector (nullable = true)\n",
      " |-- category_ohe: vector (nullable = true)\n",
      " |-- watch_time_of_day_ohe: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaled_features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5002668623657349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5002716159314518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5002668623657348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.4993754340927429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"liked\",\n",
    "                            predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, \n",
    "                  {evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "precision = evaluator.evaluate(predictions,\n",
    "                  {evaluator.metricName: \"weightedPrecision\"})\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = evaluator.evaluate(predictions,\n",
    "                  {evaluator.metricName: \"weightedRecall\"})\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = evaluator.evaluate(predictions,\n",
    "                {evaluator.metricName: \"f1\"})\n",
    "print(f\"F1 Score: {f1}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 1.00000000e+00,  6.92453642e-01,             nan,\n",
      "               7.41644505e-04, -1.23716731e-03,  5.94209803e-04],\n",
      "             [ 6.92453642e-01,  1.00000000e+00,             nan,\n",
      "               1.03302358e-03, -1.29672085e-03,  4.67204024e-05],\n",
      "             [            nan,             nan,  1.00000000e+00,\n",
      "                          nan,             nan,             nan],\n",
      "             [ 7.41644505e-04,  1.03302358e-03,             nan,\n",
      "               1.00000000e+00,  1.31173328e-04,  8.30501903e-04],\n",
      "             [-1.23716731e-03, -1.29672085e-03,             nan,\n",
      "               1.31173328e-04,  1.00000000e+00,  4.27227898e-04],\n",
      "             [ 5.94209803e-04,  4.67204024e-05,             nan,\n",
      "               8.30501903e-04,  4.27227898e-04,  1.00000000e+00]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/22 22:54:02 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler_corr = VectorAssembler(\n",
    "    inputCols=[\"watch_time\",\"video_duration\",\"watch_percent\",\"recommended\",\"clicked\",\"subscribed_after\"],\n",
    "    outputCol=\"corr_features\"\n",
    ")\n",
    "\n",
    "df_corr = assembler_corr.transform(df)\n",
    "\n",
    "corr_matrix = Correlation.corr(df_corr, \"corr_features\").head()[0]\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
