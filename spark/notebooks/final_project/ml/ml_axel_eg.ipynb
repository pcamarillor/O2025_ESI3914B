{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7265e4ec-b68e-40b8-b803-d68684cb89fb",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Machine Learning** </center>\n",
    "---\n",
    "\n",
    "**Date**: November, 2025\n",
    "\n",
    "**Student Name**: Axel Escoto García\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca25e8-9224-4b4e-9b6e-fe3ecda7d2ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d3c9d6-e47c-4cc2-8542-764d6e382596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/25 05:08:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final Project: Machine Learning\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d401e5-663a-47d0-92cb-a69e9237fae8",
   "metadata": {},
   "source": [
    "# Justification\n",
    "El dataset que seleccione para la actividad es el siguiente:\n",
    "https://www.kaggle.com/datasets/rupakroy/online-payments-fraud-detection-dataset\n",
    "\n",
    "Me pareció interesante el tema de la detección de fraude, el cómo podemos detectar movimientos extraños y actuar de forma rápida, ya se apara mitigarlo o avisar sobre la sospecha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e02603-b60b-4d98-9347-d9a4b919efcc",
   "metadata": {},
   "source": [
    "# Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50e29bc-e9b4-47e8-a459-208e0539b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from axel2293.spark_utils import SparkUtils\n",
    "\n",
    "payments_schema = [\n",
    "    (\"step\", \"int\"),\n",
    "    (\"type\", \"string\"),\n",
    "    (\"amount\", \"float\"),\n",
    "    (\"nameOrig\", \"string\"),\n",
    "    (\"oldbalanceOrg\", \"float\"),\n",
    "    (\"newbalanceOrig\", \"float\"),\n",
    "    (\"nameDest\", \"string\"),\n",
    "    (\"oldbalanceDest\", \"float\"),\n",
    "    (\"newbalanceDest\", \"float\"),\n",
    "    (\"isFraud\", \"int\"),\n",
    "    (\"isFlaggedFraud\", \"int\"),\n",
    "]\n",
    "\n",
    "payments_schema = SparkUtils.generate_schema(payments_schema)\n",
    "df_fraud = spark.read \\\n",
    "    .schema(payments_schema) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"/opt/spark/work-dir/data/online_fraud/\")\n",
    "\n",
    "df_fraud.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fe17a8-3d5e-4b76-8173-aa1fb9245b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transactions: 6362620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|isFraud|  count|\n",
      "+-------+-------+\n",
      "|      0|6354407|\n",
      "|      1|   8213|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+\n",
      "|    type|isFraud|  count|\n",
      "+--------+-------+-------+\n",
      "| CASH_IN|      0|1399284|\n",
      "|CASH_OUT|      0|2233384|\n",
      "|CASH_OUT|      1|   4116|\n",
      "|   DEBIT|      0|  41432|\n",
      "| PAYMENT|      0|2151495|\n",
      "|TRANSFER|      0| 528812|\n",
      "|TRANSFER|      1|   4097|\n",
      "+--------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    type|  count|\n",
      "+--------+-------+\n",
      "| PAYMENT|2151495|\n",
      "|TRANSFER| 532909|\n",
      "| CASH_IN|1399284|\n",
      "|   DEBIT|  41432|\n",
      "|CASH_OUT|2237500|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(f\"Total transactions: {df_fraud.count()}\")\n",
    "\n",
    "# Distribución de isFraud\n",
    "df_fraud.groupBy(\"isFraud\").count().show()\n",
    "\n",
    "# Cantidad de fraudes por tipo de transacción\n",
    "df_fraud.groupBy(\"type\", \"isFraud\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"type\", \"isFraud\").show()\n",
    "\n",
    "# Ver tipos de transacciones\n",
    "df_fraud.groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e78c9c-f52d-48c4-adb0-a87f09ec65bb",
   "metadata": {},
   "source": [
    "El dataset está bastante desbalanceado, tenemos 6354407 (~99.87%) filas que no son fraude y solo 8213 (~0.12%) que sí son. Esto deja bastante claro que nuestra mejor opción es Random Forest, ya que es una buena opción para datasets desbalanceados.\n",
    "\n",
    "Solo Transfer y Cash out tienen filas con marca de ser fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad57a6-bb6f-4b3a-b571-e1c8e52736e9",
   "metadata": {},
   "source": [
    "# Vector assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f546e63f-4518-4077-bbb4-36e3088cc4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------+\n",
      "|label|features                                               |\n",
      "+-----+-------------------------------------------------------+\n",
      "|0    |[1.0,9839.6396484375,170136.0,160296.359375,0.0,0.0]   |\n",
      "|0    |[1.0,1864.280029296875,21249.0,19384.720703125,0.0,0.0]|\n",
      "|1    |[3.0,181.0,181.0,0.0,0.0,0.0]                          |\n",
      "|1    |[0.0,181.0,181.0,0.0,21182.0,0.0]                      |\n",
      "|0    |[1.0,11668.1396484375,41554.0,29885.859375,0.0,0.0]    |\n",
      "+-----+-------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "type_indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_indexed\")\n",
    "df_fraud = type_indexer.fit(df_fraud).transform(df_fraud)\n",
    "\n",
    "# Columnas/features para el vector\n",
    "feature_columns = [\n",
    "    \"type_indexed\", # El tipo de transacción puede estar muy relacionado con los fraudes. Por ejemplo, Transferencia o Cash_out son los más usados en los marcados por fraudes.\n",
    "    \"amount\", # Grandes cantidades pueden hacer sonar las alertas de fraude.\n",
    "    \"oldbalanceOrg\", # Balances bajos y transacciones altas pueden indicar fraude\n",
    "    \"newbalanceOrig\", # Ayuda a identificar si la cuenta fue drenada totalmente o parcialmente.\n",
    "    \"oldbalanceDest\", # Cuentas que tengan poco balance podrian ser sospechosas.\n",
    "    \"newbalanceDest\" # Si recibe grandes cantidades podria ser sospechoso.\n",
    "]\n",
    "\n",
    "# Construir vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_features = assembler.transform(df_fraud)\n",
    "\n",
    "# Renombrar isFraud a label\n",
    "df_final = df_features.withColumnRenamed(\"isFraud\", \"label\")\n",
    "\n",
    "df_final.select(\"label\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb74130-09f5-463e-a197-406abf079554",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "Como el Dataset está bastante desbalanceado, investigue formas para poder crear el train y test set, llegando al **\"Stratified Split\"**\n",
    "## Undersample\n",
    "El dataset es muy grande (6 millones de filas) y el training tarda demasiado, así que un poco de undersample es necesario para efectos prácticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e18f535-03c1-45b9-a67a-3e0ccfc5b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Separar filas de fraude y no fraude\n",
    "fraud_df = df_final.filter(col(\"label\") == 1)\n",
    "non_fraud_df = df_final.filter(col(\"label\") == 0)\n",
    "\n",
    "fraud_count = df_final.filter(col(\"label\") == 1).count()\n",
    "non_fraud_count = df_final.filter(col(\"label\") == 0).count()\n",
    "total_count = df_final.count()\n",
    "\n",
    "# Dividir 5:1\n",
    "desired_ratio = 5  # 5 non-fraud por 1 fraud\n",
    "non_fraud_sample_count = fraud_count * desired_ratio\n",
    "\n",
    "fraction = non_fraud_sample_count / non_fraud_count\n",
    "non_fraud_df = non_fraud_df.sample(\n",
    "    withReplacement=False,\n",
    "    fraction=fraction,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Separar 80/20\n",
    "fraud_train, fraud_test = fraud_df.randomSplit([0.8, 0.2], seed=42)\n",
    "non_fraud_train, non_fraud_test = non_fraud_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Combinar ambos df\n",
    "train_df = fraud_train.union(non_fraud_train)\n",
    "test_df = fraud_test.union(non_fraud_test)\n",
    "\n",
    "print(f\"{train_df.count()}\")\n",
    "\n",
    "print(f\"{test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc5c92-0e55-451f-bd43-344a2892df1f",
   "metadata": {},
   "source": [
    "# Train (Random Forest Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd620403-396c-42da-b89f-f0b7dd9c34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6275e1ac-4464-4b30-9b16-93cd15af86a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 05:10:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/25 05:12:14 WARN DAGScheduler: Broadcasting large task binary with size 1001.4 KiB\n",
      "25/11/25 05:12:15 WARN DAGScheduler: Broadcasting large task binary with size 1375.3 KiB\n",
      "25/11/25 05:12:17 WARN DAGScheduler: Broadcasting large task binary with size 1832.3 KiB\n",
      "25/11/25 05:12:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/11/25 05:12:20 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/11/25 05:12:21 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/11/25 05:12:24 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/11/25 05:12:26 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/11/25 05:12:28 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100,\n",
    "    maxDepth=15,\n",
    "    maxBins=32,\n",
    "    seed=42,\n",
    "    featureSubsetStrategy=\"sqrt\"\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf_model = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c3dea-aa62-4246-82f7-81a7d8013383",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd51157-bfb1-4c67-9d1e-66aecee059e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 05:12:33 WARN TaskSetManager: Stage 76 contains a task of very large size (1603 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en /opt/spark/work-dir/data/mlmodels/rf/fraud_rf_pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Save pipeline model\n",
    "rf_model_path = \"/opt/spark/work-dir/data/mlmodels/rf/fraud_rf_pipeline\"\n",
    "rf_model.write().overwrite().save(rf_model_path)\n",
    "print(f\"Modelo guardado en {rf_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac0e1b-ab5e-4fb1-976a-b81c655cfc0a",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b475acfd-05ac-4299-8252-d1a5c1213b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 05:15:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+----------+------------------------------------------+\n",
      "|type    |amount   |label|prediction|probability                               |\n",
      "+--------+---------+-----+----------+------------------------------------------+\n",
      "|CASH_OUT|20128.0  |1    |1.0       |[0.26274515837241996,0.73725484162758]    |\n",
      "|CASH_OUT|235238.66|1    |1.0       |[0.09149302030157122,0.9085069796984289]  |\n",
      "|CASH_OUT|1277212.8|1    |1.0       |[2.451920693872261E-4,0.9997548079306128] |\n",
      "|TRANSFER|35063.63 |1    |1.0       |[0.0016148411255892795,0.9983851588744108]|\n",
      "|CASH_OUT|1096187.2|1    |1.0       |[2.743010102608729E-4,0.9997256989897392] |\n",
      "+--------+---------+-----+----------+------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 05:16:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/11/25 05:16:39 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/11/25 05:17:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9938509640437728\n",
      "  Precision: 0.99393853226463\n",
      "  F1 Score:  0.9938758991961807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 05:17:36 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "[Stage 93:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0| 8021|\n",
      "|    0|       1.0|   45|\n",
      "|    1|       0.0|   14|\n",
      "|    1|       1.0| 1515|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 05:18:01 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "rf_predictions.select(\"type\", \"amount\", \"label\", \"prediction\", \"probability\").show(5, truncate=False)\n",
    "\n",
    "# Accuracy\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "rf_accuracy = evaluator_acc.evaluate(rf_predictions)\n",
    "\n",
    "# Precision\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "rf_precision = evaluator_precision.evaluate(rf_predictions)\n",
    "\n",
    "# F1 Score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "rf_f1 = evaluator_f1.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"  Accuracy:  {rf_accuracy}\")\n",
    "print(f\"  Precision: {rf_precision}\")\n",
    "print(f\"  F1 Score:  {rf_f1}\")\n",
    "\n",
    "confusion_matrix = rf_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94434b1c-f3dd-477c-9da1-99a478c0b876",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "El modelo obtuvo scores bastante altos, también se enfrentaron varios problemas como lo fue el desbalance de los datos y que el dataset era muy grande. Me pareció muy interesante analizar como con este tipo de datos podemos detectar fraudes entre transacciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
