{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b7ef28-7a6d-4e2e-8733-5187ecd89cc2",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final project: Machine Learning** </center>\n",
    "---\n",
    "\n",
    "**Date**: 23 de Noviembre, 2025\n",
    "\n",
    "**Student Name**: Aura Melina Gutierrez Jimenez\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96e8de-4675-4b3a-9e58-ba71a64906ba",
   "metadata": {},
   "source": [
    "## Obejtivo\n",
    "Agrupar puntos de conteo de tráfico en la ciudad según similaridad en volumen, ubicación y patrón horario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f595e29-47f2-4e96-aa9c-8798da45c770",
   "metadata": {},
   "source": [
    "## Justificación del algoritmo ML\n",
    "\n",
    "Elegí K-emans porque el dataset no tiene una columna \"label\" / objetivo que se pudiera predecir. Pero si puedo hacer una busqueda para segmentar ubicaciones con comportamientos afines (zonas de alto/médio/bajo tráfico, picos horarios, etc.). Tambien, K-Means es escalable, está implementado en SparkML y permite interpretar clusters geoespaciales y de volumen de manera sencilla para visualización y decisiones de movilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678b741-1795-4916-9b6a-f68762ce9afe",
   "metadata": {},
   "source": [
    "## Descipcion del Dataset\n",
    "\n",
    "**Origen del Dataset**\n",
    "Seleccione un conjunto de datos público que refleja mediciones reales de infraestructura de tráfico urbano.\n",
    "\n",
    "Nombre del Dataset: Chicago Average Daily Traffic Counts\n",
    "\n",
    "Fuente: Kaggle Public Repository\n",
    "\n",
    "URL: https://www.kaggle.com/datasets/chicago/chicago-average-daily-traffic-counts\n",
    "\n",
    "Archivo Principal: average-daily-traffic-counts.csv\n",
    "\n",
    "**Tamaño del Dataset**\n",
    "\n",
    "Número de filas: 1,280 registros\n",
    "\n",
    "Número de columnas: 15 columnas\n",
    "\n",
    "Peso del archivo: ~246 KB\n",
    "\n",
    "**Número de dimensiones**\n",
    "\n",
    "1. LATITUDE (double)\n",
    "2. LONGITUDE (double)\n",
    "3. AVG_TRAFFIC_VOLUME (double)(codificada)\n",
    "\n",
    "- Todas son variables numéricas continuas\n",
    "- No requieren encoding (ya son numéricas)\n",
    "- Serán normalizadas usando StandardScaler para que todas contribuyan equitativamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fad8eb-c4cf-4e53-8e2f-10aeff30c2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/24 17:44:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML: K-means\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "194be9f0-e949-457e-ae03-d0fbc9658d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auragutierrez.spark_utils import SparkUtils\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "traffic_schema_columns = [\n",
    " (\"ID\", \"int\"),\n",
    " (\"LOCATION_ADDRESS\", \"string\"),\n",
    " (\"STREET_NAME\", \"string\"),\n",
    " (\"DATE_OF_COUNT\", \"string\"),\n",
    " (\"TOTAL_VEHICLE_VOLUME\", \"int\"),\n",
    " (\"DIRECTIONAL_VOLUME\", \"string\"),\n",
    " (\"LATITUDE\", \"double\"),\n",
    " (\"LONGITUDE\", \"double\"),\n",
    " (\"LOCATION_RAW\", \"string\"),\n",
    " (\"BOUNDARIES_ZIP_CODES\", \"int\"),\n",
    " (\"COMMUNITY_AREAS\", \"int\"),\n",
    " (\"ZIP_CODES\", \"int\"),\n",
    " (\"CENSUS_TRACTS\", \"int\"),\n",
    " (\"WARDS\", \"int\"),\n",
    " (\"HISTORICAL_WARDS_2003_2015\", \"int\")\n",
    "]\n",
    "\n",
    "traffic_schema = SparkUtils.generate_schema(traffic_schema_columns)\n",
    "base_path = \"/opt/spark/work-dir/data/\"\n",
    "df = spark.read \\\n",
    " .option(\"header\", \"true\") \\\n",
    " .schema(traffic_schema) \\\n",
    " .csv(base_path + \"/Traffic/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dad3e28f-6ac1-4c50-8b0b-334ce6f17c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit, upper, trim, hour, dayofweek, when, avg, count\n",
    "\n",
    "df = df.select(\n",
    " \"ID\",\n",
    " \"STREET_NAME\",\n",
    " \"DATE_OF_COUNT\",\n",
    " \"TOTAL_VEHICLE_VOLUME\",\n",
    " \"LATITUDE\",\n",
    " \"LONGITUDE\")\n",
    "\n",
    "df_transformed = df.withColumn(\n",
    " \"COUNT_TIMESTAMP\",\n",
    "     to_timestamp(col(\"DATE_OF_COUNT\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ").drop(\"DATE_OF_COUNT\")\n",
    "\n",
    "df_clean = df_transformed.filter(\n",
    " (col(\"TOTAL_VEHICLE_VOLUME\").isNotNull()) &\n",
    " (col(\"TOTAL_VEHICLE_VOLUME\") > 0) &\n",
    " (col(\"LATITUDE\").isNotNull()) &\n",
    " (col(\"COUNT_TIMESTAMP\").isNotNull()))\n",
    "\n",
    "df_features = df_clean.withColumn(\n",
    " \"HOUR_OF_DAY\",\n",
    " hour(col(\"COUNT_TIMESTAMP\"))\n",
    ").withColumn(\n",
    " \"DAY_TYPE\",\n",
    " when(dayofweek(col(\"COUNT_TIMESTAMP\")).isin(1, 7), lit(\"Weekend\"))\n",
    " .otherwise(lit(\"WeekDay\")))\n",
    "\n",
    "df_aggregated = df_features.groupBy(\"LATITUDE\", \"LONGITUDE\", \"STREET_NAME\", \"HOUR_OF_DAY\").agg(\n",
    "    avg(\"TOTAL_VEHICLE_VOLUME\").alias(\"AVG_TRAFFIC_VOLUME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f603a0-49a1-44fb-8535-bb93a38e4975",
   "metadata": {},
   "source": [
    "## ML Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8932c117-2dca-4e6d-b783-d3ea0ca607a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "#Assemble the features into a single vector column\n",
    "feature_columns = [\"LATITUDE\", \"LONGITUDE\", \"AVG_TRAFFIC_VOLUME\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_raw\" )\n",
    "\n",
    "assembled_df = assembler.transform(df_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f828578-352e-4e0c-8520-7a4a21591800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Scale feature\n",
    "scaler = StandardScaler(inputCol=\"features_raw\",outputCol=\"features\",\n",
    "    withMean=True,  # Center the data\n",
    "    withStd=True    # Scale to unit variance\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "df_scaled = scaler_model.transform(assembled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9128dd48-5427-46eb-977c-28cfe9922441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Entrenando con K=2... Silhouette=0.4784\n",
      "   Entrenando con K=3... Silhouette=0.4058\n",
      "   Entrenando con K=4... Silhouette=0.4612\n",
      "   Entrenando con K=5... Silhouette=0.4846\n",
      "   Entrenando con K=6... Silhouette=0.5001\n",
      "   Entrenando con K=7... Silhouette=0.4602\n",
      "   Entrenando con K=8... Silhouette=0.4467\n",
      "   Entrenando con K=9... Silhouette=0.4522\n",
      "   Entrenando con K=10... Silhouette=0.4005\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Test different values of K\n",
    "k_values = range(2, 11)  # Test K from 2 to 10\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"   Entrenando con K={k}...\", end=\" \")\n",
    "    \n",
    "    # Train K-Means\n",
    "    kmeans = KMeans().setK(k).setSeed(13)\n",
    "    model = kmeans.fit(df_scaled)\n",
    "    predictions = model.transform(df_scaled)\n",
    "    \n",
    "    # Calculate Silhouette Score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    \n",
    "    print(f\"Silhouette={silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f7d79c0-ce8b-489b-a0c0-f4b8f1b4d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k_idx = silhouette_scores.index(max(silhouette_scores))\n",
    "optimal_k = list(k_values)[optimal_k_idx] \n",
    "\n",
    "#print(optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "252ea481-2f11-47d7-9e40-9d1ef704325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means model trained successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.ml.clustering.KMeansModel"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Final model with optmimal k\n",
    "kmeans = KMeans().setK(optimal_k).setSeed(13)\n",
    "\n",
    "model = kmeans.fit(df_scaled)\n",
    "print(\"K-means model trained successfully\")\n",
    "\n",
    "#Save model\n",
    "kmeans_model_path = \"/opt/spark/work-dir/data/mlmodels/kmeans/traffic\"\n",
    "model.write().overwrite().save(kmeans_model_path)\n",
    "model.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57dcab95-2ec2-4c41-98e5-28f012310e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar modelo\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "kmeans_model_path = \"/opt/spark/work-dir/data/mlmodels/kmeans/traffic\"\n",
    "\n",
    "k_model = KMeansModel.load(kmeans_model_path)\n",
    "df_predictions = k_model.transform(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e50a10-bcf6-4073-99e6-8c3d5faecfeb",
   "metadata": {},
   "source": [
    "## ML Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bff02506-7068-4175-988d-5485487da703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Silhouette Score final: 0.5001202059880351\n",
      "Cluster Centers: \n",
      "[-1.21350763  1.01948177 -0.49385911]\n",
      "[ 1.0251249  -1.50842142  0.14296192]\n",
      "[-0.71071943  0.31861707  3.95264883]\n",
      "[-0.96098644 -0.30259012  0.58631197]\n",
      "[ 0.40776896  0.20764467 -0.52359891]\n",
      "[ 0.81829538 -0.13535195  1.15266818]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Calculate Silhouette Score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(df_predictions)\n",
    "\n",
    "print(f\"   - Silhouette Score final: {silhouette}\")\n",
    "\n",
    "# Show the result\n",
    "print(\"Cluster Centers: \")\n",
    "for center in k_model.clusterCenters():\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997ebc7-8c95-4fcc-aebf-c445e6746c75",
   "metadata": {},
   "source": [
    "**Interpretación**\n",
    "- Clúster Crítico (Clúster 2): Requiere una atención inmediata para la optimización de semáforos, implementación de carriles reversibles o señalización de rutas alternas.\n",
    "\n",
    "- Clústeres 3 y 5 (Corredores Principales): Necesitan monitoreo constante e inversiones a mediano plazo, ya que representan la mayoría de los flujos de tráfico pesado.\n",
    "\n",
    "- Clústeres 0, 1 y 4 (Tráfico Promedio/Bajo): Pueden ser utilizados como rutas de desvío o zonas donde las intervenciones de tráfico pueden ser más ligeras (e.g., gestión de estacionamiento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "579959a0-d069-4292-8c6b-83d3a59c81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf397d10-dd12-4a70-b7f8-49046626558a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
