{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f917fa67",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Structured Streaming** </center>\n",
    "---\n",
    "\n",
    "**Date**: November, 2025\n",
    "\n",
    "**Student Name**: Luis Adrian Bravo Ramirez\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123fd7e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Objective\n",
    "To build a data pipeline in Python using Apache Spark for data consumption in continuous mode, transformation, and persistence, with the objective of addressing a practical problem. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6671803a",
   "metadata": {},
   "source": [
    "# Producer\n",
    "\n",
    "The producer is a Python script that will generate information in continuous mode, specifically information related to the _twitch gamers_ dataset. This script will generate random information, like the following:\n",
    "\n",
    "```bash\n",
    "\"numeric_id\": 83382684,\n",
    "\"views\": 303579,\n",
    "\"mature\": true,\n",
    "\"life_time\": 137,\n",
    "\"created_at\": \"2010-05-17\",\n",
    "\"updated_at\": \"2010-10-01\",\n",
    "\"dead_account\": true,\n",
    "\"language\": \"DE\",\n",
    "\"affiliate\": true,\n",
    "\"activity_status\": \"Inactive\",\n",
    "\"account_category\": \"Mega\"\n",
    "```\n",
    "\n",
    "\n",
    "## Command Line Arguments\n",
    "The producer requires two arguments:\n",
    "```bash\n",
    "python3 producer_script_luis_bravo.py  \n",
    "```\n",
    "- `<broker>`: Kafka broker address (`kafka:9093`)\n",
    "\n",
    "- `<topic>`: Kafka topic name to publish messages to\n",
    "\n",
    "## Creating topic\n",
    "\n",
    "As we said, the producer requires a topic. To create it, execute the following command that will create a topic called _*streaming_processing_luisbravo*_.\n",
    "\n",
    "```bash\n",
    "docker exec -it <kafka_container_id> \\\n",
    "/opt/kafka/bin/kafka-topics.sh \\\n",
    "--create --zookeeper zookeeper:2181 \\\n",
    "--replication-factor 1 --partitions 1 \\\n",
    "--topic streaming_processing_luisbravo\n",
    "```\n",
    "\n",
    "NOTE: You must use your Kafka Container ID on <kafka_container_id>.\n",
    "\n",
    "## Testing from Jupyter Notebook\n",
    "To test the producer, you must open the Jupyter Notebook on your Web Browser and do the following:\n",
    "- Select _File > New > Terminal_\n",
    "\n",
    "- Access to my folder which is _/opt/spark/work-dir/lib/luisbravor00_ using a `cd /opt/spark/work-dir/lib/luisbravor00`\n",
    "\n",
    "- Execute `python3 producer_script_luis_bravo.py kafka:9093 streaming_processing_luisbravo` to start the producer\n",
    "\n",
    "This producer uses Kafka as the message broker to simulate real-time streaming data ingestion for analytics and processing pipelines.\n",
    "\n",
    "*NOTE*: I configured my Producer to iterate 100 times, so I can have a 100 records in total (so I can have more information to work with). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5780a0",
   "metadata": {},
   "source": [
    "# Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271c204",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d35fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-11685153-d168-4ec8-9f4f-1c93f61ec48f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      ":: resolution report :: resolve 498ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-11685153-d168-4ec8-9f4f-1c93f61ec48f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/14ms)\n",
      "25/11/11 00:46:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Structured Streaming: Twitch Gamers\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef308c9e",
   "metadata": {},
   "source": [
    "## Dataset and Stream creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd131149",
   "metadata": {},
   "source": [
    "### Create the remote connection to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcd3c9",
   "metadata": {},
   "source": [
    "*REMINDER*: Create the topic before running the following cell. \\\n",
    "\n",
    "`docker exec -it 776688eed872951b48536248b0cdc3182f813837eb7ebf30deb8162ab5c8cb3c /opt/kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic streaming_processing_luisbravo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f3f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the remote connection\n",
    "kafka_df = spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "            .option(\"subscribe\", \"streaming_processing_luisbravo\") \\\n",
    "            .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddeb65e",
   "metadata": {},
   "source": [
    "### Create the schema for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdf5742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value_str: string (nullable = true)\n",
      " |-- telemetry: struct (nullable = true)\n",
      " |    |-- views: integer (nullable = true)\n",
      " |    |-- mature: integer (nullable = true)\n",
      " |    |-- life_time: integer (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- updated_at: string (nullable = true)\n",
      " |    |-- numeric_id: integer (nullable = true)\n",
      " |    |-- dead_account: integer (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- affiliate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from luisbravor00.spark_utils import SparkUtils\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "ts_telemetry_df = kafka_df.select(kafka_df.value.cast(\"string\").alias(\"value_str\"))\n",
    "\n",
    "# Extract the columns from the input JSON\n",
    "schema_columns = [(\"views\", \"int\"),\n",
    "                  (\"mature\", \"int\"),\n",
    "                  (\"life_time\", \"int\"),\n",
    "                  (\"created_at\", \"string\"),\n",
    "                  (\"updated_at\", \"string\"),\n",
    "                  (\"numeric_id\", \"int\"),\n",
    "                  (\"dead_account\", \"int\"),\n",
    "                  (\"language\", \"string\"),\n",
    "                  (\"affiliate\", \"int\")]\n",
    "pkg_schema = SparkUtils.generate_schema(schema_columns)\n",
    "ts_extracted_df = ts_telemetry_df.withColumn(\"telemetry\", from_json(ts_telemetry_df.value_str, pkg_schema))\n",
    "\n",
    "ts_extracted_df.printSchema()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5732b7b",
   "metadata": {},
   "source": [
    "### Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3590ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 00:46:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Send the stream to a files sink\n",
    "query_files = ts_extracted_df.writeStream \\\n",
    "                .trigger(processingTime=\"4 seconds\") \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"path\", \"/opt/spark/work-dir/data/ts_output/\") \\\n",
    "                .option(\"checkpointLocation\", \"/opt/spark/work-dir/data/ts_checkpoint\") \\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d98a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /opt/spark/work-dir/data/ts_output/\n",
    "!rm -rf /opt/spark/work-dir/ts_checkpoint/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0843acb",
   "metadata": {},
   "source": [
    "## Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c9861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- views: integer (nullable = true)\n",
      " |-- mature: integer (nullable = true)\n",
      " |-- life_time: integer (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- updated_at: string (nullable = true)\n",
      " |-- numeric_id: integer (nullable = true)\n",
      " |-- dead_account: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- affiliate: integer (nullable = true)\n",
      "\n",
      "+------+------+---------+----------+----------+----------+------------+--------+---------+\n",
      "|views |mature|life_time|created_at|updated_at|numeric_id|dead_account|language|affiliate|\n",
      "+------+------+---------+----------+----------+----------+------------+--------+---------+\n",
      "|795628|1     |2612     |2009-10-03|2016-11-27|659589    |1           |Other   |1        |\n",
      "|693539|0     |1097     |2013-03-06|2016-03-07|931752    |0           |EN      |1        |\n",
      "|575685|1     |162      |2016-12-05|2017-05-16|550204    |1           |Other   |0        |\n",
      "|866955|0     |866      |2012-06-16|2014-10-30|732318    |0           |DE      |1        |\n",
      "|430380|0     |547      |2016-01-19|2017-07-19|279966    |1           |DE      |0        |\n",
      "|88003 |1     |1412     |2007-10-12|2011-08-24|267542    |1           |DE      |0        |\n",
      "|406689|0     |2587     |2010-11-25|2017-12-25|886829    |0           |EN      |1        |\n",
      "|313766|0     |2248     |2007-11-12|2014-01-07|977692    |0           |Other   |0        |\n",
      "|564344|0     |358      |2014-01-13|2015-01-06|994138    |1           |Other   |1        |\n",
      "|142262|1     |411      |2017-01-05|2018-02-20|380794    |0           |DE      |1        |\n",
      "+------+------+---------+----------+----------+----------+------------+--------+---------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "path = \"/opt/spark/work-dir/data/ts_output/\"\n",
    "\n",
    "# Read all parquet part files in that folder\n",
    "df = spark.read.parquet(path)\n",
    "\n",
    "# Parse the dataframe\n",
    "df = df.select(\"telemetry.*\")\n",
    "\n",
    "# Inspect the schema and show some rows\n",
    "df.printSchema()\n",
    "df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf4d758a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8e220",
   "metadata": {},
   "source": [
    "Here, the transformations (create new columns, aggreate, run joins, etc.) will take place. For this case, we'll execute the following:\n",
    "\n",
    "1. Remove duplicates using `numeric_id` entries and clean null values (for this quantity of records and due to the randomness of the producer, no repeated records should be found).\n",
    "\n",
    "2. Convert `mature`, `dead_account`and `affiliate` from integer (0 or 1) to a boolean type (true or false).\n",
    "\n",
    "3. Parse `created_at`and `updated_at` string into date objects.\n",
    "\n",
    "4. Make sure that `numeric_id`, `views`, and `life_time` are integers.\n",
    "\n",
    "5. Add a column called `activity_status` that will categorize the account activity with these values:\n",
    "    - \"Very Active\" (updated within 30 days)\n",
    "    - \"Active\" (within 90 days)\n",
    "    - \"Less Active\" (within 180 days)\n",
    "    - \"Inactive\" (no activity)\n",
    "\n",
    "6. Add a column called `account_category` that will classify the account based on views:\n",
    "    - \"Nano\": views < 1,000 (New or very small streamers)\n",
    "    - \"Micro\": 1,000 ≤ views < 10,000 (Small but growing audience)\n",
    "    - \"Mid-Tier\": 10,000 ≤ views < 50,000 (Established presence)\n",
    "    - \"Macro\": 50,000 ≤ views < 100,000 (Significant reach)\n",
    "    - \"Mega\": 100,000 ≤ views < 500,000 (High influence)\n",
    "    - \"Elite\": views ≥ 500,000 (Top-tier streamers)\n",
    "\n",
    "7. Trim whitespace from all string fields to avoid any future issues with these fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5220dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, when, isnull, count, to_date, max, datediff, lit, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b01eb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for the dataframe before cleaning: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----------+----------+----------+------------+--------+---------+\n",
      "|views|mature|life_time|created_at|updated_at|numeric_id|dead_account|language|affiliate|\n",
      "+-----+------+---------+----------+----------+----------+------------+--------+---------+\n",
      "|    0|     0|        0|         0|         0|         0|           0|       0|        0|\n",
      "+-----+------+---------+----------+----------+----------+------------+--------+---------+\n",
      "\n",
      "\n",
      "Number of records for the dataframe after cleaning with trim & dropna: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates using numeric_id and clean null values\n",
    "print(f\"Number of records for the dataframe before cleaning: {df.count()}\")\n",
    "df.select([count(when(isnull(c[0]) | col(c[0]).isNull(), c[0])).alias(c[0]) for c in schema_columns]).show()\n",
    "\n",
    "# Perform data cleaning with trim (column by column)\n",
    "df_clean = df \\\n",
    "        .dropDuplicates([\"numeric_id\"]) \\\n",
    "        .filter(col(\"numeric_id\").isNotNull()) \\\n",
    "        .filter(col(\"views\").isNotNull()) \\\n",
    "        .filter(col(\"mature\").isNotNull()) \\\n",
    "        .filter(col(\"life_time\").isNotNull()) \\\n",
    "        .filter(col(\"dead_account\").isNotNull()) \\\n",
    "        .filter(col(\"affiliate\").isNotNull()) \\\n",
    "        .withColumn(\"created_at\", trim(\"created_at\")) \\\n",
    "        .withColumn(\"updated_at\", trim(\"updated_at\")) \\\n",
    "        .withColumn(\"language\", trim(\"language\")) \\\n",
    "\n",
    "# Perform data cleaning with dropna (make sure that it erases null values)\n",
    "df_clean = df_clean.dropna()\n",
    "print(f\"\\nNumber of records for the dataframe after cleaning with trim & dropna: {df_clean.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert mature, dead_account and affiliate from integer to boolean\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"mature\", col(\"mature\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"dead_account\", col(\"dead_account\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"affiliate\", col(\"affiliate\").cast(\"boolean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd17f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse created_at and updated_at string values into date objects\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"created_at\", to_date(\"created_at\")) \\\n",
    "    .withColumn(\"updated_at\", to_date(\"updated_at\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202cb568",
   "metadata": {},
   "source": [
    "* Already checked that `numeric_id`, `views` and `life_time` fields are **integers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13ee484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a columns called activity status that will categorize the account activity using the updated_at field\n",
    "## Get the latest date from the updated_at column so we can use it as current date\n",
    "max_date = df_clean.select(max(col(\"updated_at\")).alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "\n",
    "## Add the new column activity_status and compute it's values\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"activity_status\",\n",
    "    when(datediff(lit(max_date), col(\"updated_at\")) <= 30, \"Very Active\")\n",
    "    .when(datediff(lit(max_date), col(\"updated_at\")) <= 90, \"Active\")\n",
    "    .when(datediff(lit(max_date), col(\"updated_at\")) <= 180, \"Less Active\")\n",
    "    .otherwise(\"Inactive\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33afea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column called account_category that will classify the account based on views column\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"account_category\",\n",
    "    when(col(\"views\") < 1000, \"Starter\")\n",
    "    .when((col(\"views\") >= 1000) & (col(\"views\") < 10000), \"Emerging\")\n",
    "    .when((col(\"views\") >= 10000) & (col(\"views\") < 50000), \"Established\")\n",
    "    .when((col(\"views\") >= 50000) & (col(\"views\") < 100000), \"Popular\")\n",
    "    .when((col(\"views\") >= 100000) & (col(\"views\") < 500000), \"Star\")\n",
    "    .otherwise(\"Legend\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b19cf6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+----------+----------+----------+------------+--------+---------+---------------+----------------+\n",
      "| views|mature|life_time|created_at|updated_at|numeric_id|dead_account|language|affiliate|activity_status|account_category|\n",
      "+------+------+---------+----------+----------+----------+------------+--------+---------+---------------+----------------+\n",
      "|231510| false|     1410|2014-03-24|2018-02-01|      8260|       false|      DE|    false|       Inactive|            Star|\n",
      "|163777| false|      164|2017-11-18|2018-05-01|     15016|        true|      DE|     true|    Less Active|            Star|\n",
      "|844309|  true|        8|2014-03-02|2014-03-10|     15871|        true|   Other|    false|       Inactive|          Legend|\n",
      "|760260| false|       57|2018-07-09|2018-09-04|     52516|        true|   Other|     true|         Active|          Legend|\n",
      "|137025|  true|     2999|2009-07-16|2017-10-01|     64539|       false|      EN|     true|       Inactive|            Star|\n",
      "|491641|  true|     2091|2008-12-08|2014-08-30|     74434|        true|      EN|     true|       Inactive|            Star|\n",
      "|198970| false|      262|2009-09-17|2010-06-06|     87168|       false|      EN|    false|       Inactive|            Star|\n",
      "|835152| false|     1031|2014-03-29|2017-01-23|     94653|        true|      DE|     true|       Inactive|          Legend|\n",
      "|238458| false|     2218|2010-09-29|2016-10-25|    101456|       false|      EN|     true|       Inactive|            Star|\n",
      "|117893| false|      373|2011-08-31|2012-09-07|    103186|       false|   Other|     true|       Inactive|            Star|\n",
      "|490149| false|     2061|2012-10-29|2018-06-21|    108373|        true|      DE|     true|    Less Active|            Star|\n",
      "|905186|  true|      325|2017-01-05|2017-11-26|    117816|       false|   Other|    false|       Inactive|          Legend|\n",
      "|256172| false|      797|2016-05-11|2018-07-17|    126923|        true|      EN|     true|         Active|            Star|\n",
      "| 43212|  true|     1838|2012-03-25|2017-04-06|    140906|       false|      DE|     true|       Inactive|     Established|\n",
      "|288710| false|      257|2015-02-26|2015-11-10|    141206|       false|      DE|    false|       Inactive|            Star|\n",
      "|122602|  true|       91|2018-05-18|2018-08-17|    157446|       false|   Other|    false|         Active|            Star|\n",
      "|848247| false|      232|2016-08-09|2017-03-29|    177273|       false|      EN|    false|       Inactive|          Legend|\n",
      "|854877|  true|      574|2015-01-27|2016-08-23|    194272|        true|      EN|    false|       Inactive|          Legend|\n",
      "|619839| false|      394|2014-01-12|2015-02-10|    228182|       false|      DE|    false|       Inactive|          Legend|\n",
      "|400081| false|      418|2014-01-29|2015-03-23|    249341|        true|      DE|     true|       Inactive|            Star|\n",
      "+------+------+---------+----------+----------+----------+------------+--------+---------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "root\n",
      " |-- views: integer (nullable = true)\n",
      " |-- mature: boolean (nullable = true)\n",
      " |-- life_time: integer (nullable = true)\n",
      " |-- created_at: date (nullable = true)\n",
      " |-- updated_at: date (nullable = true)\n",
      " |-- numeric_id: integer (nullable = true)\n",
      " |-- dead_account: boolean (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- affiliate: boolean (nullable = true)\n",
      " |-- activity_status: string (nullable = false)\n",
      " |-- account_category: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show()\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d6e56",
   "metadata": {},
   "source": [
    "## Persistence Data\n",
    "\n",
    "We'll persist this dataframe using two formats:\n",
    "\n",
    "1. Parquet file with a vertical partition on activity_status column\n",
    "2. On a CSV file, so we can upload it to PowerBI and create a Dashboard with relevant information about this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "223530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/opt/spark/work-dir/data/\"\n",
    "\n",
    "# Save it into a Parquet file\n",
    "df_clean.write \\\n",
    "    .partitionBy(\"activity_status\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(base_path + \"streaming_processing/output/parquet\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a46df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it into a CSV file\n",
    "df_clean.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(base_path + \"streaming_processing/output/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89882ca",
   "metadata": {},
   "source": [
    "## PowerBI Dashboard\n",
    "\n",
    "Now, here is an image for the PowerBI Dashboard created with the imported CSV file on the step before.\n",
    "The dashbaord shows\n",
    "\n",
    "\n",
    "*NOTE*: The information shown on the dashboard can change because of the randomness of the producer, but the base path for all this process is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371784ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
