{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f917fa67",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Structured Streaming** </center>\n",
    "---\n",
    "\n",
    "**Date**: November, 2025\n",
    "\n",
    "**Student Name**: Luis Adrian Bravo Ramirez\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123fd7e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Objective\n",
    "To build a data pipeline in Python using Apache Spark for data consumption in continuous mode, transformation, and persistence, with the objective of addressing a practical problem. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6671803a",
   "metadata": {},
   "source": [
    "# Producer\n",
    "\n",
    "The producer is a Python script that will generate information in continuous mode, specifically information related to the _twitch gamers_ dataset. This script will generate random information, like the following:\n",
    "\n",
    "```bash\n",
    "\"numeric_id\": 83382684,\n",
    "\"views\": 303579,\n",
    "\"mature\": true,\n",
    "\"life_time\": 137,\n",
    "\"created_at\": \"2010-05-17\",\n",
    "\"updated_at\": \"2010-10-01\",\n",
    "\"dead_account\": true,\n",
    "\"language\": \"DE\",\n",
    "\"affiliate\": true,\n",
    "\"activity_status\": \"Inactive\",\n",
    "\"account_category\": \"Mega\"\n",
    "```\n",
    "\n",
    "\n",
    "## Command Line Arguments\n",
    "The producer requires two arguments:\n",
    "```bash\n",
    "python3 producer_script_luis_bravo.py  \n",
    "```\n",
    "- `<broker>`: Kafka broker address (`kafka:9093`)\n",
    "\n",
    "- `<topic>`: Kafka topic name to publish messages to\n",
    "\n",
    "## Creating topic\n",
    "\n",
    "As we said, the producer requires a topic. To create it, execute the following command that will create a topic called _*streaming_processing_luisbravo*_.\n",
    "\n",
    "```bash\n",
    "docker exec -it <kafka_container_id> \\\n",
    "/opt/kafka/bin/kafka-topics.sh \\\n",
    "--create --zookeeper zookeeper:2181 \\\n",
    "--replication-factor 1 --partitions 1 \\\n",
    "--topic streaming_processing_luisbravo\n",
    "```\n",
    "\n",
    "NOTE: You must use your Kafka Container ID on <kafka_container_id>.\n",
    "\n",
    "## Testing from Jupyter Notebook\n",
    "To test the producer, you must open the Jupyter Notebook on your Web Browser and do the following:\n",
    "- Select _File > New > Terminal_\n",
    "\n",
    "- Access to my folder which is _/opt/spark/work-dir/lib/luisbravor00_ using a `cd /opt/spark/work-dir/lib/luisbravor00`\n",
    "\n",
    "- Execute `python3 producer_script_luis_bravo.py kafka:9093 streaming_processing_luisbravo` to start the producer\n",
    "\n",
    "This producer uses Kafka as the message broker to simulate real-time streaming data ingestion for analytics and processing pipelines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5780a0",
   "metadata": {},
   "source": [
    "# Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271c204",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d35fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-628de3ec-95c3-4d0c-87b6-07ba0f0220c0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      ":: resolution report :: resolve 549ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-628de3ec-95c3-4d0c-87b6-07ba0f0220c0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/14ms)\n",
      "25/11/07 22:10:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Structured Streaming: Twitch Gamers\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef308c9e",
   "metadata": {},
   "source": [
    "## Dataset and Stream creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd131149",
   "metadata": {},
   "source": [
    "### Create the remote connection to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcd3c9",
   "metadata": {},
   "source": [
    "*REMINDER*: Create the topic before running the following cell. \\\n",
    "\n",
    "`docker exec -it 776688eed872951b48536248b0cdc3182f813837eb7ebf30deb8162ab5c8cb3c /opt/kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic streaming_processing_luisbravo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f3f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the remote connection\n",
    "kafka_df = spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "            .option(\"subscribe\", \"streaming_processing_luisbravo\") \\\n",
    "            .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddeb65e",
   "metadata": {},
   "source": [
    "### Create the schema for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdf5742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value_str: string (nullable = true)\n",
      " |-- telemetry: struct (nullable = true)\n",
      " |    |-- views: integer (nullable = true)\n",
      " |    |-- mature: integer (nullable = true)\n",
      " |    |-- life_time: integer (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- updated_at: string (nullable = true)\n",
      " |    |-- numeric_id: integer (nullable = true)\n",
      " |    |-- dead_account: integer (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- affiliate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from luisbravor00.spark_utils import SparkUtils\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "ts_telemetry_df = kafka_df.select(kafka_df.value.cast(\"string\").alias(\"value_str\"))\n",
    "\n",
    "# Extract the columns from the input JSON\n",
    "schema_columns = [(\"views\", \"int\"),\n",
    "                  (\"mature\", \"int\"),\n",
    "                  (\"life_time\", \"int\"),\n",
    "                  (\"created_at\", \"string\"),\n",
    "                  (\"updated_at\", \"string\"),\n",
    "                  (\"numeric_id\", \"int\"),\n",
    "                  (\"dead_account\", \"int\"),\n",
    "                  (\"language\", \"string\"),\n",
    "                  (\"affiliate\", \"int\")]\n",
    "pkg_schema = SparkUtils.generate_schema(schema_columns)\n",
    "ts_extracted_df = ts_telemetry_df.withColumn(\"telemetry\", from_json(ts_telemetry_df.value_str, pkg_schema))\n",
    "\n",
    "ts_extracted_df.printSchema()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5732b7b",
   "metadata": {},
   "source": [
    "### Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3590ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/07 22:10:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Send the stream to a files sink\n",
    "query_files = ts_extracted_df.writeStream \\\n",
    "                .trigger(processingTime=\"4 seconds\") \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"path\", \"/opt/spark/work-dir/data/ts_output/\") \\\n",
    "                .option(\"checkpointLocation\", \"/opt/spark/work-dir/ts_checkpoint\") \\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d98a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /opt/spark/work-dir/data/ts_output/\n",
    "!rm -rf /opt/spark/work-dir/ts_checkpoint/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0843acb",
   "metadata": {},
   "source": [
    "## Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c9861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value_str: string (nullable = true)\n",
      " |-- telemetry: struct (nullable = true)\n",
      " |    |-- views: integer (nullable = true)\n",
      " |    |-- mature: integer (nullable = true)\n",
      " |    |-- life_time: integer (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- updated_at: string (nullable = true)\n",
      " |    |-- numeric_id: integer (nullable = true)\n",
      " |    |-- dead_account: integer (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- affiliate: integer (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|value_str                                                                                                                                                                              |telemetry                                                     |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|{\"views\": 954512, \"mature\": 0, \"life_time\": 96, \"created_at\": \"2013-04-14\", \"updated_at\": \"2013-07-19\", \"numeric_id\": 270402, \"dead_account\": 1, \"language\": \"Other\", \"affiliate\": 1}  |{954512, 0, 96, 2013-04-14, 2013-07-19, 270402, 1, Other, 1}  |\n",
      "|{\"views\": 722138, \"mature\": 1, \"life_time\": 198, \"created_at\": \"2016-01-18\", \"updated_at\": \"2016-08-03\", \"numeric_id\": 659253, \"dead_account\": 1, \"language\": \"EN\", \"affiliate\": 1}    |{722138, 1, 198, 2016-01-18, 2016-08-03, 659253, 1, EN, 1}    |\n",
      "|{\"views\": 779062, \"mature\": 1, \"life_time\": 3593, \"created_at\": \"2008-04-13\", \"updated_at\": \"2018-02-13\", \"numeric_id\": 312963, \"dead_account\": 0, \"language\": \"Other\", \"affiliate\": 0}|{779062, 1, 3593, 2008-04-13, 2018-02-13, 312963, 0, Other, 0}|\n",
      "|{\"views\": 456015, \"mature\": 1, \"life_time\": 798, \"created_at\": \"2014-10-28\", \"updated_at\": \"2017-01-03\", \"numeric_id\": 251196, \"dead_account\": 1, \"language\": \"EN\", \"affiliate\": 0}    |{456015, 1, 798, 2014-10-28, 2017-01-03, 251196, 1, EN, 0}    |\n",
      "|{\"views\": 427268, \"mature\": 1, \"life_time\": 934, \"created_at\": \"2013-12-06\", \"updated_at\": \"2016-06-27\", \"numeric_id\": 873134, \"dead_account\": 1, \"language\": \"DE\", \"affiliate\": 1}    |{427268, 1, 934, 2013-12-06, 2016-06-27, 873134, 1, DE, 1}    |\n",
      "|{\"views\": 596774, \"mature\": 1, \"life_time\": 321, \"created_at\": \"2007-11-17\", \"updated_at\": \"2008-10-03\", \"numeric_id\": 824508, \"dead_account\": 0, \"language\": \"Other\", \"affiliate\": 1} |{596774, 1, 321, 2007-11-17, 2008-10-03, 824508, 0, Other, 1} |\n",
      "|{\"views\": 188410, \"mature\": 0, \"life_time\": 925, \"created_at\": \"2013-08-17\", \"updated_at\": \"2016-02-28\", \"numeric_id\": 303498, \"dead_account\": 0, \"language\": \"DE\", \"affiliate\": 1}    |{188410, 0, 925, 2013-08-17, 2016-02-28, 303498, 0, DE, 1}    |\n",
      "|{\"views\": 926662, \"mature\": 1, \"life_time\": 331, \"created_at\": \"2013-07-06\", \"updated_at\": \"2014-06-02\", \"numeric_id\": 566546, \"dead_account\": 0, \"language\": \"DE\", \"affiliate\": 1}    |{926662, 1, 331, 2013-07-06, 2014-06-02, 566546, 0, DE, 1}    |\n",
      "|{\"views\": 315076, \"mature\": 1, \"life_time\": 3, \"created_at\": \"2018-07-22\", \"updated_at\": \"2018-07-25\", \"numeric_id\": 293739, \"dead_account\": 0, \"language\": \"EN\", \"affiliate\": 0}      |{315076, 1, 3, 2018-07-22, 2018-07-25, 293739, 0, EN, 0}      |\n",
      "|{\"views\": 382515, \"mature\": 1, \"life_time\": 1509, \"created_at\": \"2008-08-27\", \"updated_at\": \"2012-10-14\", \"numeric_id\": 446873, \"dead_account\": 0, \"language\": \"EN\", \"affiliate\": 0}   |{382515, 1, 1509, 2008-08-27, 2012-10-14, 446873, 0, EN, 0}   |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "path = \"/opt/spark/work-dir/data/ts_output/\"\n",
    "\n",
    "# Read all parquet part files in that folder\n",
    "df = spark.read.parquet(path)\n",
    "\n",
    "# Inspect the schema and show some rows\n",
    "df.printSchema()\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f424b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d6e56",
   "metadata": {},
   "source": [
    "## Persistence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223530fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89882ca",
   "metadata": {},
   "source": [
    "## PowerBI Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371784ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
