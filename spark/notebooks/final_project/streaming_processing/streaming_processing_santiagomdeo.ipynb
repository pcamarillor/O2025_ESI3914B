{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee90666",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Autumn 2025** </center>\n",
    "---\n",
    "### <center> **Streaming processing final project** </center>\n",
    "---\n",
    "**Profesor**: Pablo Camarillo Ramirez\n",
    "**Estudiante**: Santiago Montes de Oca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc145d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dd6607e4-8436-46b8-a819-a47ca20d972b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/4.0.0/spark-sql-kafka-0-10_2.13-4.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0!spark-sql-kafka-0-10_2.13.jar (252ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/4.0.0/spark-token-provider-kafka-0-10_2.13-4.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0!spark-token-provider-kafka-0-10_2.13.jar (176ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.2.0/scala-parallel-collections_2.13-1.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0!scala-parallel-collections_2.13.jar (167ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.9.0/kafka-clients-3.9.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.9.0!kafka-clients.jar (715ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (188ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.12.0!commons-pool2.jar (90ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.4.1/hadoop-client-runtime-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.4.1!hadoop-client-runtime.jar (1718ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (110ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.7/snappy-java-1.1.10.7.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.7!snappy-java.jar(bundle) (293ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.16/slf4j-api-2.0.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.16!slf4j-api.jar (123ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.4.1/hadoop-client-api-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.4.1!hadoop-client-api.jar (1126ms)\n",
      ":: resolution report :: resolve 7951ms :: artifacts dl 4964ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dd6607e4-8436-46b8-a819-a47ca20d972b\n",
      "\tconfs: [default]\n",
      "\t11 artifacts copied, 0 already retrieved (62673kB/74ms)\n",
      "25/11/23 19:24:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Examples on Structured Streaming (Kafka)\") \\\n",
    "    .master(\"spark://b064a789aeff:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6832a4d9",
   "metadata": {},
   "source": [
    "cd kafka<br/>\n",
    "docker compose --file docker-compose.yml up -d<br/>\n",
    "\n",
    "create a topic:<br/>\n",
    "\n",
    "docker exec -it 31da5decad48 /opt/kafka/bin/kafka-topics.sh --create --topic raw-crawl-telemetry --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
    "\n",
    "show that it exists:<br/>\n",
    "\n",
    "docker exec -it 31da5decad48 /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08860553",
   "metadata": {},
   "source": [
    "## Dataset and Stream creation (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b22281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = \"raw-crawl-telemetry\"\n",
    "\n",
    "kafka_df = spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "            .option(\"subscribe\", topic) \\\n",
    "            .load()\n",
    "\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1853e6",
   "metadata": {},
   "source": [
    "## Transformations and Actions (15 points)\n",
    "In this section, you need to provide the code to clean and process data (create new columns, aggregate, run joins, etc.). In this section is NOT expected to have any action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949bcf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n",
      "string\n",
      "int\n",
      "root\n",
      " |-- value_str: string (nullable = true)\n",
      " |-- telemetry: struct (nullable = true)\n",
      " |    |-- from: string (nullable = true)\n",
      " |    |-- to: string (nullable = true)\n",
      " |    |-- depth: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructField, StringType\n",
    "from santiagomdeo.spark_utils import SparkUtils\n",
    "\n",
    "#process the received data:\n",
    "#this is a single column string probably\n",
    "crawler_telemetry_df = kafka_df.select(kafka_df.value.cast(\"string\").alias(\"value_str\"))\n",
    "\n",
    "\n",
    "#format it with my library \n",
    "crawler_schema = SparkUtils.generate_schema([(\"from\", \"string\"), (\"to\", \"string\"),(\"depth\", \"int\")])\n",
    "\n",
    "#god\n",
    "crawler_extracted_df = crawler_telemetry_df.withColumn(\"telemetry\", from_json(crawler_telemetry_df.value_str, crawler_schema))\n",
    "\n",
    "crawler_extracted_df.printSchema()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033d5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, count              #i made this \"udf\", so i might be importing something else\n",
    "from santiagomdeo.utils3 import normalize_url\n",
    "\n",
    "normalize_url_udf = udf(normalize_url, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8088b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = crawler_extracted_df.select(\n",
    "    col(\"telemetry.from\").alias(\"from\"),\n",
    "    col(\"telemetry.to\").alias(\"to\"),\n",
    "    col(\"telemetry.depth\").alias(\"depth\")\n",
    ")\n",
    "\n",
    "cleaned = (\n",
    "    parsed\n",
    "    .withColumn(\"from_url\", normalize_url_udf(col(\"from\")))\n",
    "    .withColumn(\"to_url\", normalize_url_udf(col(\"to\")))\n",
    "    .drop(\"from\", \"to\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f352428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = (\n",
    "    cleaned\n",
    "    .groupBy(\"from_url\", \"to_url\", \"depth\")\n",
    "    .agg(count(\"*\").alias(\"appearances\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7508f",
   "metadata": {},
   "source": [
    "cd to my lib\n",
    "cd .\\spark\\notebooks\\lib\\santiagomdeo\\producerStreaming\\\n",
    "\n",
    "\n",
    "### like the great gatsby get it? XD\n",
    "docker build -t the-great-crawler .\n",
    "\n",
    "### the actual container run\n",
    "ctrl + lshift + v (allows to paste as one line)\n",
    "\n",
    "also remeber the second you start this thing it fires a few messages so you have to have the outpur running\n",
    "\n",
    "docker run -it\n",
    " --network=host\n",
    " -v \"$(pwd)/verified_domains.txt:/app/verified_domains.txt\"\n",
    " the-great-crawler\n",
    " https://www.iteso.mx\n",
    " 2\n",
    " localhost:9092\n",
    " raw-crawl-telemetry\n",
    "\n",
    "docker run -it --network=host -v $(pwd)/verified_domains.txt:/app/verified_domains.txt the-great-crawler https://www.iteso.mx 2 localhost:9092 raw-crawl-telemetry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ef5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 19:37:38 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d2536e61-4470-47eb-99d6-73c0839f35c5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/23 19:37:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+------+-----+-----------+\n",
      "|from_url|to_url|depth|appearances|\n",
      "+--------+------+-----+-----------+\n",
      "+--------+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = (\n",
    "    aggregated.writeStream\n",
    "              .format(\"console\")\n",
    "              .outputMode(\"update\")     # <-- required for streaming aggregations\n",
    "              .option(\"truncate\", \"false\")\n",
    "              .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257e51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+-----+-----------+\n",
      "|from_url            |to_url              |depth|appearances|\n",
      "+--------------------+--------------------+-----+-----------+\n",
      "|https://www.iteso.mx|https://www.iteso.mx|0    |1          |\n",
      "+--------------------+--------------------+-----+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+----------------------------------------------------------------------------------------+-----+-----------+\n",
      "|from_url            |to_url                                                                                  |depth|appearances|\n",
      "+--------------------+----------------------------------------------------------------------------------------+-----+-----------+\n",
      "|https://www.iteso.mx|https://vinculacion.iteso.mx                                                            |0    |3          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/web/internacional                                                  |0    |3          |\n",
      "|https://www.iteso.mx|https://prepa.iteso.mx                                                                  |0    |1          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/web/general/detalle?group_id=19965582                              |0    |2          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/web/general/detalle?group_id=17276924                              |0    |3          |\n",
      "|https://www.iteso.mx|https://deporteysalud.iteso.mx                                                          |0    |1          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/c/portal/login?p_l_id=26561                                        |0    |2          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/aspirantes                                                         |0    |4          |\n",
      "|https://www.iteso.mx|https://egresados.iteso.mx                                                              |0    |4          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/contacto                                                           |0    |4          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/web/radio                                                          |0    |1          |\n",
      "|https://www.iteso.mx|https://diplomados.iteso.mx                                                             |0    |1          |\n",
      "|https://www.iteso.mx|https://carreras.iteso.mx                                                               |0    |1          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/web/general/detalle?group_id=19946144                              |0    |2          |\n",
      "|https://www.iteso.mx|https://radio.iteso.mx                                                                  |0    |2          |\n",
      "|https://www.iteso.mx|https://biblio.iteso.mx                                                                 |0    |1          |\n",
      "|https://www.iteso.mx|https://formacionsocial.iteso.mx                                                        |0    |2          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/c/portal/update_language?languageId=en_US&p_l_id=26561&redirect=%2F|0    |2          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/c/portal/update_language?languageId=es_ES&p_l_id=26561&redirect=%2F|0    |2          |\n",
      "|https://www.iteso.mx|https://www.iteso.mx/conocenos                                                          |0    |4          |\n",
      "+--------------------+----------------------------------------------------------------------------------------+-----+-----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+-----------------------------+-----+-----------+\n",
      "|from_url            |to_url                       |depth|appearances|\n",
      "+--------------------+-----------------------------+-----+-----------+\n",
      "|https://www.iteso.mx|https://ite.so/magis508portal|0    |1          |\n",
      "+--------------------+-----------------------------+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+----------------------------------------------------+-----+-----------+\n",
      "|from_url            |to_url                                              |depth|appearances|\n",
      "+--------------------+----------------------------------------------------+-----+-----------+\n",
      "|https://www.iteso.mx|https://informedemedios.iteso.mx                    |0    |1          |\n",
      "|https://www.iteso.mx|https://encartes.mx                                 |0    |1          |\n",
      "|https://www.iteso.mx|https://analisisplural.iteso.mx/index.php/ap        |0    |1          |\n",
      "|https://www.iteso.mx|https://sinectica.iteso.mx/index.php/SINECTICA/index|0    |1          |\n",
      "|https://www.iteso.mx|https://publicaciones.iteso.mx                      |0    |1          |\n",
      "|https://www.iteso.mx|https://complexus.iteso.mx                          |0    |1          |\n",
      "+--------------------+----------------------------------------------------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e80cbb",
   "metadata": {},
   "source": [
    "# Persistence Data (15 points)\n",
    "\n",
    "Once information is transformed, you need to document the process of persistence data in files using at least one vertical partition (partitionBy).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_files = aggregated.writeStream \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/opt/spark/work-dir/data/crawler_clean_output/\") \\\n",
    "    .option(\"checkpointLocation\", \"/opt/spark/work-dir/crawler_checkpoint\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339917f",
   "metadata": {},
   "source": [
    "# Power BI Dashboard (10 points)\n",
    "\n",
    "With the files generated in the persistence section, you should create a Dashboard using Power BI to summarize and visualize the information you processed in the Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c3584",
   "metadata": {},
   "source": [
    "teach this is a note, the files generated are not exactly dashboard friendly and im not going to get any useful information from them, so the plan is to send to the dashboard health data.\n",
    "\n",
    "power bi doesnt have a native kafka connector, so im going to receive it here, and send it in parquet files so that the dash board can see them, it will update every ten seconds or so, but i find this experiment much more interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368fd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63bafa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
