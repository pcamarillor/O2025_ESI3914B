{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a93ee6a-e77f-4fc8-a1d1-30338c2a9860",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Structured Streaming** </center>\n",
    "---\n",
    "\n",
    "**Date**: 23 November, 2025\n",
    "\n",
    "**Student Name**: Antonia Horburger\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f53ad5-6f1d-41ad-a646-9b077fa7c1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/24 05:31:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream_in : /opt/spark/work-dir/data/stream/btc/incoming\n",
      "stream_out: /opt/spark/work-dir/data/stream/btc/persisted\n",
      "chkpt_path: /opt/spark/work-dir/data/stream/btc/checkpoints\n"
     ]
    }
   ],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Final Project: Streaming Processing\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "base_path = \"/opt/spark/work-dir\"\n",
    "stream_in  = f\"{base_path}/data/stream/btc/incoming\"\n",
    "stream_out = f\"{base_path}/data/stream/btc/persisted\"\n",
    "chkpt_path = f\"{base_path}/data/stream/btc/checkpoints\"\n",
    "\n",
    "print(\"stream_in :\", stream_in)\n",
    "print(\"stream_out:\", stream_out)\n",
    "print(\"chkpt_path:\", chkpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ffcc7-563d-4a2a-b76a-402a5fac9982",
   "metadata": {},
   "source": [
    "## Dataset & Stream Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef770c7-71bc-44db-8462-36d43b57a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from antoniahorburger.spark_utils import SparkUtils\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "btc_schema_columns = [\n",
    "    (\"timestamp\", \"string\"),\n",
    "    (\"price\",     \"double\"),\n",
    "    (\"quantity\",  \"double\"),\n",
    "    (\"side\",      \"string\"),\n",
    "    (\"level\",     \"int\"),\n",
    "    (\"update_id\", \"string\"),\n",
    "]\n",
    "\n",
    "btc_schema = SparkUtils.generate_schema(btc_schema_columns)\n",
    "\n",
    "import os\n",
    "os.makedirs(stream_in, exist_ok=True)\n",
    "\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "         .format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .schema(btc_schema)\n",
    "         .load(stream_in)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98ef6fe-a5ec-4ed9-aec7-85c15baf6987",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc150564-1a13-4e0f-b2ea-9ba3e695c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, from_unixtime, to_timestamp, window,\n",
    "    avg, sum as _sum, min as _min, max as _max, to_date\n",
    ")\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "ds = (\n",
    "    raw_stream\n",
    "      .withColumn(\"timestamp_ms\", col(\"timestamp\").cast(LongType()))\n",
    "      .withColumn(\"update_id_long\", col(\"update_id\").cast(LongType()))\n",
    "      .withColumn(\n",
    "          \"event_time\",\n",
    "          to_timestamp(from_unixtime((col(\"timestamp_ms\") / 1000).cast(\"long\")))\n",
    "      )\n",
    "      .withColumn(\"price\",    col(\"price\").cast(\"double\"))\n",
    "      .withColumn(\"quantity\", col(\"quantity\").cast(\"double\"))\n",
    "      .withColumn(\"level\",    col(\"level\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "feat_per_min = (\n",
    "    ds\n",
    "      .withWatermark(\"event_time\", \"2 minutes\")\n",
    "      .groupBy(\n",
    "          window(col(\"event_time\"), \"1 minute\").alias(\"w\"),\n",
    "          col(\"side\"),\n",
    "      )\n",
    "      .agg(\n",
    "          avg(\"price\").alias(\"avg_price\"),\n",
    "          _sum(\"quantity\").alias(\"total_qty\"),\n",
    "          _min(\"price\").alias(\"min_price\"),\n",
    "          _max(\"price\").alias(\"max_price\"),\n",
    "      )\n",
    ")\n",
    "\n",
    "feat_per_min_partitioned = feat_per_min.withColumn(\n",
    "    \"date\",\n",
    "    to_date(col(\"w.start\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237778e-b932-4586-a50e-59cea8e7267f",
   "metadata": {},
   "source": [
    "## Persistence with Vertical Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a40845f-0b7e-4102-8372-5aa92125ead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x71eecd9378b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "for q in spark.streams.active:\n",
    "    q.stop()\n",
    "\n",
    "sink_query = (\n",
    "    feat_per_min_partitioned\n",
    "        .writeStream\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", stream_out)\n",
    "        .option(\"checkpointLocation\", chkpt_path)\n",
    "        .partitionBy(\"date\")\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "sink_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b570fbb0-6e65-4788-8aad-7e3f4a685b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"id\" : \"497649b5-6a86-4495-bf1f-90bc60b0d497\",\n",
       "  \"runId\" : \"5e0aacba-108a-4b1d-b314-49c18c8fce0a\",\n",
       "  \"name\" : null,\n",
       "  \"timestamp\" : \"2025-11-24T05:35:13.362Z\",\n",
       "  \"batchId\" : 3,\n",
       "  \"batchDuration\" : 32,\n",
       "  \"numInputRows\" : 0,\n",
       "  \"inputRowsPerSecond\" : 0.0,\n",
       "  \"processedRowsPerSecond\" : 0.0,\n",
       "  \"durationMs\" : {\n",
       "    \"latestOffset\" : 32,\n",
       "    \"triggerExecution\" : 32\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"watermark\" : \"2025-08-11T13:58:34.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"operatorName\" : \"stateStoreSave\",\n",
       "    \"numRowsTotal\" : 4,\n",
       "    \"numRowsUpdated\" : 0,\n",
       "    \"allUpdatesTimeMs\" : 15,\n",
       "    \"numRowsRemoved\" : 0,\n",
       "    \"allRemovalsTimeMs\" : 90,\n",
       "    \"commitTimeMs\" : 35730,\n",
       "    \"memoryUsedBytes\" : 90720,\n",
       "    \"numRowsDroppedByWatermark\" : 0,\n",
       "    \"numShufflePartitions\" : 200,\n",
       "    \"numStateStoreInstances\" : 200,\n",
       "    \"customMetrics\" : {\n",
       "      \"loadedMapCacheHitCount\" : 800,\n",
       "      \"loadedMapCacheMissCount\" : 0,\n",
       "      \"stateOnCurrentVersionSizeBytes\" : 24992\n",
       "    }\n",
       "  } ],\n",
       "  \"sources\" : [ {\n",
       "    \"description\" : \"FileStreamSource[file:/opt/spark/work-dir/data/stream/btc/incoming]\",\n",
       "    \"startOffset\" : {\n",
       "      \"logOffset\" : 1\n",
       "    },\n",
       "    \"endOffset\" : {\n",
       "      \"logOffset\" : 1\n",
       "    },\n",
       "    \"latestOffset\" : null,\n",
       "    \"numInputRows\" : 0,\n",
       "    \"inputRowsPerSecond\" : 0.0,\n",
       "    \"processedRowsPerSecond\" : 0.0\n",
       "  } ],\n",
       "  \"sink\" : {\n",
       "    \"description\" : \"FileSink[file:/opt/spark/work-dir/data/stream/btc/persisted]\",\n",
       "    \"numOutputRows\" : -1\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink_query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2f5ac1-96c0-44be-9a54-e0a90abfe8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir/data/stream/btc/persisted -> []\n",
      "/opt/spark/work-dir/data/stream/btc/persisted/_spark_metadata -> ['.0.crc', '.1.crc', '.2.crc', '0', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(stream_out):\n",
    "    print(root, \"->\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10928d-6421-47d2-89fa-c2ec886638ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in spark.streams.active:\n",
    "    q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a19133-d041-448c-9a7b-f9632093a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_persisted = spark.read.parquet(stream_out)\n",
    "\n",
    "df_persisted.printSchema()\n",
    "df_persisted.show(10, truncate=False)\n",
    "\n",
    "# 2) Build an aggregated table for Power BI\n",
    "df_powerbi = (\n",
    "    df_persisted\n",
    "      .groupBy(\"date\", \"side\")\n",
    "      .agg(\n",
    "          count(\"*\").alias(\"rows\"),\n",
    "          avg(\"avg_price\").alias(\"mean_avg_price\"),\n",
    "          avg(\"total_qty\").alias(\"mean_total_qty\")\n",
    "      )\n",
    ")\n",
    "\n",
    "df_powerbi.show(10, truncate=False)\n",
    "\n",
    "powerbi_path = f\"{base_path}/data/stream/btc/powerbi\"\n",
    "\n",
    "(df_powerbi\n",
    "    .coalesce(1)                  \n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(powerbi_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b677f0a8-bffc-4aea-accf-65f15c09991e",
   "metadata": {},
   "source": [
    "**Aviso:** Implementé todo el código para la persistencia y la exportación a Power BI pero en mi entorno local Spark no logró escribir los archivos Parquet (solo generó _spark_metadata). Probé reiniciar el checkpoint, las carpetas y el producer, pero el error seguía apareciendo. Por el tiempo de entrega lo dejo así, pero antes de la presentación voy a intentar de corregirlo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
