{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67dee45-a1bc-4d82-b46e-d27bfe8e261b",
   "metadata": {},
   "source": [
    "# Producer (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789bc43-4d20-46fe-8052-4529308db976",
   "metadata": {},
   "source": [
    "In a separate file, in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb22b243-2ad7-4286-9f88-08e989ebbf5d",
   "metadata": {},
   "source": [
    "# Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e681639-8ba6-4c7f-af33-27a2a3e09f18",
   "metadata": {},
   "source": [
    "## Dataset and Stream creation (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7542821-bac1-457f-bac6-f9170d7340b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0d6cd9f0-b500-4087-9940-d9bc9352073d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      ":: resolution report :: resolve 351ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0d6cd9f0-b500-4087-9940-d9bc9352073d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/9ms)\n",
      "25/11/24 04:58:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final project: Structured Streaming\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc730b4-c99f-407c-964d-be86c0fe2533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string, row_index: bigint, source_timestamp: timestamp]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the remote connection\n",
    "kafka_df = spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "            .option(\"subscribe\", \"telemetry-project\") \\\n",
    "            .load() \n",
    "kafka_df.selectExpr(\n",
    "    \"CAST(key AS STRING)\", \n",
    "    \"CAST(value AS STRING)\",\n",
    "    \"offset as row_index\",\n",
    "    \"timestamp as source_timestamp\", \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af5b73d-f56c-4726-82f9-b6d7c2d90970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24568d6c-8b78-4c63-8f9d-10aeb6f6124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regalado_floriano.spark_utils import SparkUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61fd830e-c000-4a89-986d-de845dfee0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8770ee84-8786-42d2-a2ef-489682d89100",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_keys = ((\"property_id\",\"int\"),\n",
    "(\"country\",\"string\"),\n",
    "(\"city\",\"string\"),\n",
    "(\"property_type\",\"string\"),\n",
    "(\"furnishing_status\",\"string\"),\n",
    "(\"property_size_sqft\",\"int\"),\n",
    "(\"price\",\"int\"),\n",
    "(\"constructed_year\",\"int\"),\n",
    "(\"previous_owners\",\"int\"),\n",
    "(\"rooms\",\"int\"),\n",
    "(\"bathrooms\",\"int\"),\n",
    "(\"garage\",\"bool\"),\n",
    "(\"garden\",\"bool\"),\n",
    "(\"crime_cases_reported\",\"int\"),\n",
    "(\"legal_cases_on_property\",\"bool\"),\n",
    "(\"customer_salary\",\"int\"),\n",
    "(\"loan_amount\",\"int\"),\n",
    "(\"loan_tenure_years\",\"int\"),\n",
    "(\"monthly_expenses\",\"int\"),\n",
    "(\"down_payment\",\"int\"),\n",
    "(\"emi_to_income_ratio\",\"float\"),\n",
    "(\"satisfaction_score\",\"int\"),\n",
    "(\"neighbourhood_rating\",\"int\"),\n",
    "(\"connectivity_score\",\"int\"),\n",
    "(\"decision\",\"bool\")\n",
    ")\n",
    "houses_schema = SparkUtils.generate_schema(schema_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bf57c-9a69-487a-8814-69beca3290f4",
   "metadata": {},
   "source": [
    "## Transformations and Actions (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc77a8b6-50fa-464e-9aa1-f47b47f8d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440e3f01-15ab-4c49-a607-9922f7c63b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, day, from_json, col \n",
    "from pyspark.sql.types import StructField, StringType, ArrayType\n",
    "from regalado_floriano.spark_utils import SparkUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103018b7-095f-4c91-bd6f-31491c556ce1",
   "metadata": {},
   "source": [
    "## Transformation 1: Reading JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cac9681e-e72e-48df-93b4-f06721397fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, day, from_json, col \n",
    "from pyspark.sql.types import StructField, StringType\n",
    "from regalado_floriano.spark_utils import SparkUtils\n",
    "\n",
    "raw_string_column = kafka_df.value.cast(\"string\").alias(\"value_str\")\n",
    "raw_string_df = kafka_df.select(raw_string_column)\n",
    "raw_telemetry_df = kafka_df.select( raw_string_column   , \"offset\",\"timestamp\")\n",
    "vg_telemetry_df = raw_telemetry_df.withColumn(\"telemetry\", (from_json(\"value_str\", (houses_schema) ) ))\n",
    "# We need to extract the columns from the input JSON \n",
    " \n",
    "vg_extracted_df = vg_telemetry_df.withColumn(\"year\", year(vg_telemetry_df.timestamp)) \\\n",
    "                                      .withColumn(\"month\", month(vg_telemetry_df.timestamp)) \\\n",
    "                                      .withColumn(\"day\", day(vg_telemetry_df.timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cbfbc9f-b733-454e-956b-1a58ea422b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_select = []\n",
    "for key, type_ in schema_keys:\n",
    "    to_select.append(f\"telemetry.{key}\")\n",
    "to_select.extend([\"timestamp\",\"offset\",\"value_str\",\"year\",\"month\",\"day\"])\n",
    "house_df = vg_extracted_df.select(to_select)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4f761d0-fd74-42b1-9cdf-1a318daa4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, day, from_json, col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9379aae-21c8-4546-a0e9-fa1eab9b5600",
   "metadata": {},
   "source": [
    "## Transformation 2: Add sale info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d5b322f-d38d-41bb-af4c-482a44240f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_with_sales = house_df.withColumn(\"sale_total\", house_df.price * house_df.decision.cast(\"int\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c863a-4804-4b08-829f-c993d649ddfc",
   "metadata": {},
   "source": [
    "## Transformation 3: Window and Group By Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1adaef48-8fbe-4a95-97af-2896d50d659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_houses = (\n",
    "    houses_with_sales\n",
    "        .withWatermark(\"timestamp\", \"1 minute\")\n",
    "        .groupBy(\n",
    "            window(\"timestamp\", \"30 seconds\", \"15 seconds\"),\n",
    "            F.col(\"country\"),\n",
    "            F.col(\"property_type\")\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb237ea-1f0d-463e-8920-17ff17d0205a",
   "metadata": {},
   "source": [
    "## Transformation 4: Aggregate Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64494014-1e4f-47b8-987d-b5fe88f48e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_houses = windowed_houses.agg(\n",
    "    F.sum(houses_with_sales.sale_total).alias(\"sale_total\"),\n",
    "    F.max(houses_with_sales.sale_total).alias(\"biggest_sale\"),\n",
    "    F.avg(houses_with_sales.sale_total).alias(\"average_sale\"),\n",
    "    F.avg(houses_with_sales.satisfaction_score).alias(\"average_satisfaction\") \n",
    ")\\\n",
    "    .withColumn(\"window_start\", F.col(\"window.start\")) \\\n",
    "    .withColumn(\"window_end\",   F.col(\"window.end\")) \\\n",
    "    .withColumn(\"year\",  F.year(\"window.start\")) \\\n",
    "    .withColumn(\"month\", F.month(\"window.start\"))\\\n",
    "    .drop(\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb960f0e-6ec4-4cee-91eb-6f7110577d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8091a06c-9763-403a-b579-a1335f0dd7a4",
   "metadata": {},
   "source": [
    "# Persistence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c24ca56e-4a8e-4129-a67d-9722cabaac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/24 05:10:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query_files = aggregated_houses.writeStream \\\n",
    "                .trigger(processingTime=\"10 seconds\") \\\n",
    "                .partitionBy(\"year\", \"month\") \\\n",
    "                .format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"path\", \"/opt/spark/work-dir/data/final_streaming_csv/\") \\\n",
    "                .option(\"checkpointLocation\", \"/opt/spark/work-dir/data/final_streaming_csv_delta\") \\\n",
    "                .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2e6d4be-5864-4b11-8daa-d6c6511d355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_files.awaitTermination(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aca7fb6a-60a6-4220-b865-2e55b2dfc9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"id\" : \"6210ade4-fc16-4d11-ae14-621fedd3e657\",\n",
       "  \"runId\" : \"aff74035-903d-4d07-9bb9-f4dd368f0043\",\n",
       "  \"name\" : null,\n",
       "  \"timestamp\" : \"2025-11-24T05:16:20.001Z\",\n",
       "  \"batchId\" : 10,\n",
       "  \"batchDuration\" : 617,\n",
       "  \"numInputRows\" : 902,\n",
       "  \"inputRowsPerSecond\" : 90.2,\n",
       "  \"processedRowsPerSecond\" : 1461.9124797406807,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 558,\n",
       "    \"commitOffsets\" : 21,\n",
       "    \"getBatch\" : 0,\n",
       "    \"latestOffset\" : 3,\n",
       "    \"queryPlanning\" : 11,\n",
       "    \"triggerExecution\" : 617,\n",
       "    \"walCommit\" : 23\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2025-11-24T05:16:15.008Z\",\n",
       "    \"max\" : \"2025-11-24T05:16:19.996Z\",\n",
       "    \"min\" : \"2025-11-24T05:16:10.013Z\",\n",
       "    \"watermark\" : \"2025-11-24T05:15:10.002Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"operatorName\" : \"stateStoreSave\",\n",
       "    \"numRowsTotal\" : 232,\n",
       "    \"numRowsUpdated\" : 232,\n",
       "    \"allUpdatesTimeMs\" : 48,\n",
       "    \"numRowsRemoved\" : 468,\n",
       "    \"allRemovalsTimeMs\" : 9,\n",
       "    \"commitTimeMs\" : 88,\n",
       "    \"memoryUsedBytes\" : 215624,\n",
       "    \"numRowsDroppedByWatermark\" : 0,\n",
       "    \"numShufflePartitions\" : 5,\n",
       "    \"numStateStoreInstances\" : 5,\n",
       "    \"customMetrics\" : {\n",
       "      \"loadedMapCacheHitCount\" : 100,\n",
       "      \"loadedMapCacheMissCount\" : 0,\n",
       "      \"stateOnCurrentVersionSizeBytes\" : 66704\n",
       "    }\n",
       "  } ],\n",
       "  \"sources\" : [ {\n",
       "    \"description\" : \"KafkaV2[Subscribe[telemetry-project]]\",\n",
       "    \"startOffset\" : {\n",
       "      \"telemetry-project\" : {\n",
       "        \"0\" : 3735899\n",
       "      }\n",
       "    },\n",
       "    \"endOffset\" : {\n",
       "      \"telemetry-project\" : {\n",
       "        \"0\" : 3736801\n",
       "      }\n",
       "    },\n",
       "    \"latestOffset\" : {\n",
       "      \"telemetry-project\" : {\n",
       "        \"0\" : 3736801\n",
       "      }\n",
       "    },\n",
       "    \"numInputRows\" : 902,\n",
       "    \"inputRowsPerSecond\" : 90.2,\n",
       "    \"processedRowsPerSecond\" : 1461.9124797406807,\n",
       "    \"metrics\" : {\n",
       "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
       "      \"maxOffsetsBehindLatest\" : \"0\",\n",
       "      \"minOffsetsBehindLatest\" : \"0\"\n",
       "    }\n",
       "  } ],\n",
       "  \"sink\" : {\n",
       "    \"description\" : \"FileSink[file:/opt/spark/work-dir/data/final_streaming_csv]\",\n",
       "    \"numOutputRows\" : -1\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_files.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89486134-306d-4fa4-b5dd-e4311f57c8e5",
   "metadata": {},
   "source": [
    "# Power BI Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2997bef-1fdb-4b45-8c9c-64bba2df7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from powerbiclient import Report, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "323e9d82-d107-49e7-b4ea-24b921828e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from powerbiclient.authentication import DeviceCodeLoginAuthentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5bbfe39-aa01-46b2-8329-0479546d44c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing device flow authentication. Please follow the instructions below.\n",
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code F54DYK7XQ to authenticate.\n",
      "\n",
      "Device flow authentication successfully completed.\n",
      "You are now logged in .\n",
      "\n",
      "The result should be passed only to trusted code in your notebook.\n"
     ]
    }
   ],
   "source": [
    "device_auth = DeviceCodeLoginAuthentication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64662859-7124-48b7-8cd4-269033c4eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id=\"ab09a2e8-9e06-41f3-8b0c-333cd6d5d1d4\"\n",
    "report_id=\"31aeecd4-2363-4024-ba80-02d7da8ac74c/80b19268b93e0e02d088\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1a845c9-f4d7-43f0-8e15-797974818b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not get embed URL: Get embed URL failed with status code 404",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/powerbiclient/report.py:268\u001b[0m, in \u001b[0;36mReport.__init__\u001b[0;34m(self, group_id, report_id, auth, view_mode, permissions, dataset_id, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m         response_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedUrl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 268\u001b[0m     embed_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_embed_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/powerbiclient/report.py:309\u001b[0m, in \u001b[0;36mReport._get_embed_url\u001b[0;34m(self, request_url, token, response_key)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet embed URL failed with status code \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(response\u001b[38;5;241m.\u001b[39mstatus_code))\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[response_key]\n",
      "\u001b[0;31mException\u001b[0m: Get embed URL failed with status code 404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mReport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m report\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/powerbiclient/report.py:272\u001b[0m, in \u001b[0;36mReport.__init__\u001b[0;34m(self, group_id, report_id, auth, view_mode, permissions, dataset_id, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     embed_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_embed_url(\n\u001b[1;32m    269\u001b[0m         request_url\u001b[38;5;241m=\u001b[39mrequest_url, token\u001b[38;5;241m=\u001b[39maccess_token, response_key\u001b[38;5;241m=\u001b[39mresponse_key)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not get embed URL: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ex))\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Tells if Power BI events are being observed\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observing_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Could not get embed URL: Get embed URL failed with status code 404"
     ]
    }
   ],
   "source": [
    "report = Report(group_id=group_id, report_id=report_id, auth=device_auth)\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae557186-1fe3-4016-9d49-7ffbbf19c3e2",
   "metadata": {},
   "source": [
    "This did not work because I do not have admin permissions in Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e11995-72ee-46e9-bb3e-364a25ec9966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
