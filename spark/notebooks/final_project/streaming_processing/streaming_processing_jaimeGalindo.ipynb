{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final project: Structured Streaming** </center>\n",
    "---\n",
    "\n",
    "**Date**: November, 2025\n",
    "\n",
    "**Student Name**: Jaime Enrique Galindo Villegas\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630cf8a",
   "metadata": {},
   "source": [
    "## Producer\n",
    "Using the application and dataset used in the batch_processing Notebook, you need to write a producer script in python to generate the information in continuous mode. The producer uses a Kafka topic\n",
    "\n",
    "Producer to kafka: ml_producer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257d589",
   "metadata": {},
   "source": [
    "## Consumer\n",
    "### Dataset and Stream creation\n",
    "\n",
    "This section should contain the code to create a stream using Spark Structured Streaming API to consume the data in continuous mode from the selected stream channel (files or kafka).  The information you need to consume in this section should contain the schema generation (with the SparkUtils class) and the correct format and reading options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3fa5b",
   "metadata": {},
   "source": [
    "#### Iniciar sesión de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc1a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c0c0ef9d-706b-40a1-889f-0404f31f418c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      ":: resolution report :: resolve 619ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c0c0ef9d-706b-40a1-889f-0404f31f418c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/11ms)\n",
      "25/11/15 05:17:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "\t.appName(\"MercadoLibreTransformations\") \\\n",
    "\t.master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0\") \\\n",
    "\t.config(\"spark.ui.port\", \"4040\") \\\n",
    "\t.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92565218",
   "metadata": {},
   "source": [
    "#### Iniciar stream de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601c07b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df = spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "            .option(\"subscribe\", \" structured-streaming\") \\\n",
    "            .load()\n",
    "\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad82b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ml_orders = kafka_df.select(kafka_df.value.cast(\"string\").alias(\"value_str\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7eb13",
   "metadata": {},
   "source": [
    "#### Crear esquema y dataset con la información relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f33d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del esquema\n",
    "\n",
    "from pyspark.sql.types import ArrayType # type: ignore\n",
    "from jaime_galindo.spark_utils import SparkUtils\n",
    "\n",
    "# Esquemas básicos de cada entidad\n",
    "user_schema = SparkUtils.generate_schema([\n",
    "\t(\"user_id\", \"string\"),\n",
    "\t(\"region\", \"string\"),\n",
    "\t(\"payment_method\", \"string\")\n",
    "])\n",
    "\n",
    "item_schema = SparkUtils.generate_schema([\n",
    "\t(\"item_id\", \"string\"),\n",
    "\t(\"title\", \"string\"),\n",
    "\t(\"category\", \"string\"),\n",
    "\t(\"quantity\", \"integer\"),\n",
    "\t(\"unit_price\", \"double\"),\n",
    "\t(\"final_price\", \"double\"),\n",
    "\t(\"discount_applied\", \"boolean\")\n",
    "])\n",
    "\n",
    "shipping_schema = SparkUtils.generate_schema([\n",
    "\t(\"logistics_provider\", \"string\"),\n",
    "\t(\"warehouse_origin\", \"string\"),\n",
    "\t(\"estimated_delivery\", \"string\"),\n",
    "\t(\"tracking_id\", \"string\")\n",
    "])\n",
    "\n",
    "\n",
    "# Equema principal de la orden\n",
    "order_schema = SparkUtils.generate_schema([\n",
    "\t(\"order_id\", \"string\"),\n",
    "\t(\"timestamp\", \"string\"),\n",
    "\t(\"user\", user_schema),\n",
    "\t(\"items\", ArrayType(item_schema)),\n",
    "\t(\"total_amount\", \"double\"),\n",
    "\t(\"shipping\", shipping_schema)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16cd3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value_str: string (nullable = true)\n",
      " |-- orders: struct (nullable = true)\n",
      " |    |-- order_id: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- user_id: string (nullable = true)\n",
      " |    |    |-- region: string (nullable = true)\n",
      " |    |    |-- payment_method: string (nullable = true)\n",
      " |    |-- items: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |-- category: string (nullable = true)\n",
      " |    |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |    |-- unit_price: double (nullable = true)\n",
      " |    |    |    |-- final_price: double (nullable = true)\n",
      " |    |    |    |-- discount_applied: boolean (nullable = true)\n",
      " |    |-- total_amount: double (nullable = true)\n",
      " |    |-- shipping: struct (nullable = true)\n",
      " |    |    |-- logistics_provider: string (nullable = true)\n",
      " |    |    |-- warehouse_origin: string (nullable = true)\n",
      " |    |    |-- estimated_delivery: string (nullable = true)\n",
      " |    |    |-- tracking_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "ml_extracted_df = ml_orders.withColumn(\"orders\", from_json(ml_orders.value_str, order_schema))\n",
    "\n",
    "ml_extracted_df.printSchema()   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcb712",
   "metadata": {},
   "source": [
    "### Transformations and Actions\n",
    "In this section, you need to provide the code to clean and process data (create new columns, aggregate, run joins, etc.). In this section is NOT expected to have any action.\n",
    "\n",
    "#### limpieza de datos\n",
    "separar la información que se tiene en nuevas columnas y nuevas filas para tener la información \"plana\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f275fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones\n",
    "from pyspark.sql.functions import col, explode \n",
    "\n",
    "# hacer explode para obtener las columnas de cada elemento y filas para cada item\n",
    "df_with_items = ml_extracted_df.select(\n",
    "   col(\"orders.order_id\"),\n",
    "   col(\"orders.timestamp\"),\n",
    "   col(\"orders.user\"),\n",
    "   col(\"orders.total_amount\"),\n",
    "   col(\"orders.shipping\"),\n",
    "\n",
    "\texplode(\"orders.items\").alias(\"item\")\n",
    ")\n",
    "\n",
    "# \"Aplanar\" cada estructura compleja para tener solo los valores y un posterior analisis más eficiente\n",
    "df_flattened = df_with_items.select(\n",
    "   col(\"order_id\"),\n",
    "   col(\"timestamp\"),\n",
    "   col(\"total_amount\").alias(\"order_total_amount\"),\n",
    "   \n",
    "\t# Aplanar user\n",
    "   col(\"user.user_id\"),\n",
    "   col(\"user.region\"),\n",
    "   col(\"user.payment_method\"),\n",
    "   \n",
    "\t# Aplanar shipping,\n",
    "   col(\"shipping.logistics_provider\"),\n",
    "   col(\"shipping.warehouse_origin\"),\n",
    "   col(\"shipping.estimated_delivery\"),\n",
    "   col(\"shipping.tracking_id\"),\n",
    "   \n",
    "\t# Aplanar item,\n",
    "\tcol(\"item.item_id\"),\n",
    "\tcol(\"item.title\"),\n",
    "\tcol(\"item.category\"),\n",
    "\tcol(\"item.quantity\"),\n",
    "\tcol(\"item.unit_price\"),\n",
    "\tcol(\"item.final_price\"),\n",
    "\tcol(\"item.discount_applied\")\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23931ce7",
   "metadata": {},
   "source": [
    "#### Procesar los datos\n",
    "Crear nuevas columnas en base a la información existente para enriquecer la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0339a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, year, month, dayofmonth\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Enriquecimiento, crear nuevas columnas para agregar valor a los datos\n",
    "# Se convierte la columna timestamp de texto simple a tipo Timestamp de spark\n",
    "# Se crean nuevos datos calculados de cada item y se separa el tiempo en año, mes, dia para futuro analisis \n",
    "df_enriched = df_flattened \\\n",
    "\t.withColumn(\"order_date\", col(\"timestamp\").cast(TimestampType())) \\\n",
    "\t.withColumn(\"estimated_delivery_date\", col(\"estimated_delivery\").cast(TimestampType())) \\\n",
    "\t.withColumn(\"item_total_price\", col(\"final_price\") * col(\"quantity\")) \\\n",
    "\t.withColumn(\"item_total_discount\", (col(\"unit_price\") - col(\"final_price\")) * col(\"quantity\")) \\\n",
    "\t.withColumn(\"processing_timestamp\", current_timestamp()) \\\n",
    "\t.withColumn(\"year\", year(col(\"order_date\"))) \\\n",
    "\t.withColumn(\"month\", month(col(\"order_date\"))) \\\n",
    "\t.withColumn(\"day\", dayofmonth(col(\"order_date\"))) \\\n",
    "\t.drop(\"timestamp\") \\\n",
    "\t.drop(\"estimated_delivery\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fee25c",
   "metadata": {},
   "source": [
    "### Persistence Data\n",
    "Once information is transformed, you need to document the process of persistence data in files using at least one vertical partition (partitionBy).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e37250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/15 05:17:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Dejaré corriendo durante mas de ~1h para obtener bastante información historica para Power BI\n",
    "query_files = df_enriched.writeStream \\\n",
    "\t\t\t\t\t.trigger(processingTime=\"1 minute\") \\\n",
    "\t\t\t\t\t.partitionBy(\"year\", \"month\", \"day\") \\\n",
    "\t\t\t\t\t.format(\"parquet\") \\\n",
    "\t\t\t\t\t.option(\"header\", \"true\") \\\n",
    "\t\t\t\t\t.option(\"path\", \"/opt/spark/work-dir/data/mlibre_stream_output/\") \\\n",
    "\t\t\t\t\t.option(\"checkpointLocation\", \"/opt/spark/work-dir/data/mlibre_stream_checkpoint\") \\\n",
    "\t\t\t\t\t.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc96dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/15 06:14:48 WARN DAGScheduler: Failed to cancel job group bee63b33-f643-4936-b5af-3fe0d80df7da. Cannot find active jobs for it.\n",
      "25/11/15 06:14:48 WARN DAGScheduler: Failed to cancel job group bee63b33-f643-4936-b5af-3fe0d80df7da. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "query_files.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73bec6",
   "metadata": {},
   "source": [
    "### Power BI Dashboard \n",
    "With the files generated in the persistence section, you should create a Dashboard using Power BI to summarize and visualize the information you processed in the Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f64573",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
