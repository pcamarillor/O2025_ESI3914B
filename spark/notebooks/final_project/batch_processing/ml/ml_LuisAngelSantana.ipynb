{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c235a7d",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Machine Learning** </center>\n",
    "---\n",
    "\n",
    "**Date**: November, 2025\n",
    "\n",
    "**Student Name**: Luis Angel Santana Hernandez\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99d453",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this this projectwe are going predict traffic conditions using machine learning on data collected through a computer vision system. The dataset includes counts of different vehicle types, along with time related data such as time, date, and day of the week. the data is labeled with a traffic category: 1-Heavy, 2-High, 3-Normal, or 4-Low. Because the target variable consists classes representing traffic levels, the problem is classification related.\n",
    "\n",
    "Among the available machine learning algoriths, the classification category is the usefull, since it allows the model to learn from examples and give an use this characteristics to lable with traffic classes. Within the classification algorithms, this project wll use Random Forest Classifier, chosen for its performance, robustnes to noise and ability to handle both numerical and categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97db5cf",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eda2cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Proyect ML: Random Forest\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1535b976",
   "metadata": {},
   "source": [
    "# Model Characterstics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015924f5",
   "metadata": {},
   "source": [
    "Page :https://www.kaggle.com/datasets/hasibullahaman/traffic-prediction-dataset\n",
    "\n",
    "size: 393.11 kB\n",
    "\n",
    "files:2\n",
    "\n",
    "\n",
    "\n",
    "Data present:\n",
    "\n",
    "\n",
    "1. Date\n",
    "\n",
    "The calendar date of the observation.\n",
    "\n",
    "Format: 1-31\n",
    "\n",
    "Used to analyze trends across days of the month.\n",
    "\n",
    "2. Time\n",
    "\n",
    "The hour and minutes when the data was recorded.\n",
    "\n",
    "Data is logged every 15 minutes.\n",
    "\n",
    "Example: 14:30\n",
    "\n",
    "3. Day of the week\n",
    "\n",
    "Example values: \"Monday\", \"Tuesday\", etc.\n",
    "\n",
    "4. CarCount\n",
    "\n",
    "Number of cars detected by the computer in that 15-minute interval.\n",
    "\n",
    "5. BikeCount\n",
    "\n",
    "Number of bikes/motorcycles detected in that interval.\n",
    "\n",
    "6. BusCount\n",
    "\n",
    "Number of buses detected.\n",
    "\n",
    "7. TruckCount\n",
    "\n",
    "Number of trucks detected.\n",
    "\n",
    "8. Total\n",
    "\n",
    "The sum of all detected vehicle types (Car + Bike + Bus + Truck).\n",
    "\n",
    "Represents the overall traffic volume.\n",
    "\n",
    "9. Traffic Situation / Traffic\n",
    "\n",
    "label used for classification.\n",
    "\n",
    "Describes traffic level.\n",
    "\n",
    "Encoded as:\n",
    "\n",
    "1 = Heavy\n",
    "\n",
    "2 = High\n",
    "\n",
    "3 = Normal\n",
    "\n",
    "4 = Low\n",
    "\n",
    "This is the column we will predict using Random Forest Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "53543f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from luis_santana.spark_utils import SparkUtils\n",
    "\n",
    "#import \n",
    "trafic_schema = SparkUtils.generate_schema([\n",
    "    (\"Time\", \"string\"), \n",
    "    (\"Date\", \"int\"), \n",
    "    (\"Day of the week\", \"string\"), \n",
    "    (\"CarCount\", \"int\"), \n",
    "    (\"BikeCount\", \"int\"), \n",
    "    (\"BusCount\", \"int\"),\n",
    "    (\"TruckCount\", \"int\"), \n",
    "    (\"Total\", \"int\"), \n",
    "    (\"Trafic Situation\", \"string\")])\n",
    "\n",
    "trafic_df = spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .schema(trafic_schema) \\\n",
    "                .csv(\"/opt/spark/work-dir/data/Proyect/ml/\")\n",
    "\n",
    "#eliminate nulls\n",
    "trafic_df = trafic_df.na.drop()\n",
    "# trafic_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27549f",
   "metadata": {},
   "source": [
    "number of elements: 8928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f7a512b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance\n",
    "# counts = trafic_df.groupBy(\"Trafic Situation\").count()\n",
    "# counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d337cd7",
   "metadata": {},
   "source": [
    "```python\n",
    " +----------------+-----+\n",
    "|Trafic Situation|count|\n",
    "+----------------+-----+\n",
    "|            high|  692|\n",
    "|             low| 1138|\n",
    "|           heavy| 1819|\n",
    "|          normal| 5279|\n",
    "+----------------+-----\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932e415",
   "metadata": {},
   "source": [
    "This is an unbalanced model since there is more normal trafic than the other values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f401c",
   "metadata": {},
   "source": [
    "## Convert string labels to numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f3016c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trafic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1035bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 12 hour time to float time\n",
    "from pyspark.sql.functions import to_timestamp,hour,minute,col\n",
    "\n",
    "trafic_df = trafic_df.withColumn(\"HourFloat\", \n",
    "    hour(to_timestamp(col(\"Time\"), \"h:mm:ss a\"))\n",
    "    + minute(to_timestamp(col(\"Time\"), \"h:mm:ss a\"))/60.0\n",
    ")\n",
    "\n",
    "\n",
    "# trafic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fd53289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfor day of the week to numeric(normally)\n",
    "\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# day_indexer = StringIndexer(inputCol=\"Day of the week\", outputCol=\"DayIndex\")\n",
    "# trafic_df = day_indexer.fit(trafic_df).transform(trafic_df)\n",
    "# trafic_df.show()\n",
    "\n",
    "# I want monday to be 0 and sunday to be 6\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "trafic_df = trafic_df.withColumn(\n",
    "    \"DayIndex\",\n",
    "    when(col(\"Day of the week\") == \"Monday\",    1)\n",
    "    .when(col(\"Day of the week\") == \"Tuesday\",  2)\n",
    "    .when(col(\"Day of the week\") == \"Wednesday\",3)\n",
    "    .when(col(\"Day of the week\") == \"Thursday\", 4)\n",
    "    .when(col(\"Day of the week\") == \"Friday\",   5)\n",
    "    .when(col(\"Day of the week\") == \"Saturday\", 6)\n",
    "    .when(col(\"Day of the week\") == \"Sunday\",   7)\n",
    ")\n",
    "# trafic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "98b30bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform trafic situation to numeric (noramlly)\n",
    "## Convert string labels to numeric\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# label_indexer = StringIndexer(inputCol=\"Trafic Situation\", outputCol=\"label\")\n",
    "# trafic_df = label_indexer.fit(trafic_df).transform(trafic_df)\n",
    "# trafic_df.show()\n",
    "\n",
    "# i want high = 0, low = 1, heavy = 2, normal = 3\n",
    "trafic_df = trafic_df.withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"Trafic Situation\") == \"high\",   0)\n",
    "    .when(col(\"Trafic Situation\") == \"low\",    1)\n",
    "    .when(col(\"Trafic Situation\") == \"heavy\",  2)\n",
    "    .when(col(\"Trafic Situation\") == \"normal\", 3)\n",
    ")\n",
    "# trafic_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564ad72",
   "metadata": {},
   "source": [
    "## Asemble and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "923c55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"CarCount\", \"BikeCount\", \"BusCount\", \"TruckCount\", \"Total\", \"HourFloat\", \"DayIndex\",\"Date\"]\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(trafic_df)\n",
    "assembled_df = assembled_df.select(\"features\", \"label\")\n",
    "# assembled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "151f22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plit data\n",
    "train_df, test_df = assembled_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7593d",
   "metadata": {},
   "source": [
    "## Create and train Ml\n",
    "\n",
    "matadata numbers found by empirical methot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bdfc03b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/22 00:14:12 WARN DAGScheduler: Broadcasting large task binary with size 1007.8 KiB\n",
      "25/11/22 00:14:13 WARN DAGScheduler: Broadcasting large task binary with size 1621.7 KiB\n",
      "25/11/22 00:14:14 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/11/22 00:14:16 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/11/22 00:14:17 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/11/22 00:14:23 WARN TaskSetManager: Stage 31 contains a task of very large size (1627 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf_trafic = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=110,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Build a pipeline\n",
    "\n",
    "rf_trafic = rf_trafic.fit(train_df)\n",
    "\n",
    "# Save the model\n",
    "rf_path = \"/opt/spark/work-dir/data/Proyect/mlmodel_110/\"\n",
    "rf_trafic.write().overwrite().save(rf_path)\n",
    "# print(f\"Random forest model generated: {rf_trafic.toDebugString}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b47c48",
   "metadata": {},
   "source": [
    "## ML Evaluation\n",
    "\n",
    "using f1 score since this data is not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6e7ea0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the RF model\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "rf_path = \"/opt/spark/work-dir/data/Proyect/mlmodel_110/\"\n",
    "\n",
    "rf_model_saved = RandomForestClassificationModel.load(rf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7beb639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/22 00:14:34 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "[Stage 42:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9299846492501319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "predictions_rf = rf_model_saved.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                            predictionCol=\"prediction\")\n",
    "\n",
    "f1 = evaluator.evaluate(predictions_rf,\n",
    "                {evaluator.metricName: \"f1\"})\n",
    "print(f\"F1 Score: {f1}\")  \n",
    "\n",
    "# trees=50 F1 Score: 0.9247594289588481\n",
    "# trees=100 F1 Score: 0.9251138445634184\n",
    "# trees=110, Depht=5 F1 Score: 0.9291349287137586\n",
    "# trees=110, Depht=10 F1 Score: 0.9299846492501319\n",
    "# trees=110, Depht=20 F1 Score: 0.9095975809451398\n",
    "# trees=125 F1 Score: 0.9268303004137959\n",
    "# trees=150 F1 Score: 0.9245284793669771\n",
    "\n",
    "# presition = evaluator.evaluate(predictions_rf,\n",
    "#                 {evaluator.metricName: \"weightedPrecision\"})\n",
    "# print(f\"Presition: {presition}\")\n",
    "\n",
    "# recall_rf = evaluator.evaluate(predictions_rf,\n",
    "#                 {evaluator.metricName: \"weightedRecall\"})\n",
    "# print(f\"Recall of Random Forest: {recall_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eeb5febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
