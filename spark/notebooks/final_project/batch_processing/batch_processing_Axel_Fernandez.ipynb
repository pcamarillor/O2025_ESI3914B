{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Batch Processing** </center>\n",
    "---\n",
    "\n",
    "**Date**: October, 2025\n",
    "\n",
    "**Student Name**: Axel Leonardo Fernandez Albarran\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd2743",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Data Set**: https://www.kaggle.com/datasets/rishabhrajsharma/cityride-dataset-rides-data-drivers-data?select=Rides_Data.csv\n",
    "\n",
    "Urban ride-sharing platforms generate large volumes of trip and driver data every day. Analyzing this information helps improve pricing, demand prediction, and operational efficiency.\n",
    "\n",
    "This project uses the CityRide dataset, which contains details about rides and drivers, to build a batch data processing pipeline with Apache Spark. The goal is to clean, transform, and persist the data to uncover patterns in ride demand, driver performance, and fare distribution, supporting smarter urban mobility decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757b962",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fab7ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/24 03:38:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Structured Streaming (files)\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd0c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir\n",
      "'Cityride Drivers Data.csv'  'Cityride Rides Data.csv'\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls /opt/spark/work-dir/data/final_project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c9d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from axel_fernandez.spark_utils import SparkUtils\n",
    "\n",
    "# Esquema de ambos datasets \n",
    "drivers_schema_columns = [\n",
    "    (\"Driver_ID\", \"int\"),\n",
    "    (\"Name\", \"string\"),\n",
    "    (\"Age\", \"int\"),\n",
    "    (\"City\", \"string\"),\n",
    "    (\"Experience_Years\", \"int\"),\n",
    "    (\"Average_Rating\", \"double\"),\n",
    "    (\"Active_Status\", \"string\")\n",
    "]\n",
    "\n",
    "rides_schema_columns = [\n",
    "    (\"Ride_ID\", \"int\"),\n",
    "    (\"Driver_ID\", \"int\"),\n",
    "    (\"City\", \"string\"),\n",
    "    (\"Date\", \"string\"),      \n",
    "    (\"Distance_km\", \"double\"),\n",
    "    (\"Duration_min\", \"int\"),\n",
    "    (\"Fare\", \"double\"),\n",
    "    (\"Rating\", \"double\"),\n",
    "    (\"Promo_Code\", \"string\")\n",
    "]\n",
    "\n",
    "schema_drivers = SparkUtils.generate_schema(drivers_schema_columns)\n",
    "schema_rides = SparkUtils.generate_schema(rides_schema_columns)\n",
    "\n",
    "drivers_df = spark.read.schema(schema_drivers).option(\"header\", True)\\\n",
    "    .csv(\"/opt/spark/work-dir/data/final_project/Cityride Drivers Data.csv\")\n",
    "\n",
    "rides_df = spark.read.schema(schema_rides).option(\"header\", True)\\\n",
    "    .csv(\"/opt/spark/work-dir/data/final_project/Cityride Rides Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2a65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de ambos datasets (no grandes cambios, solo filtrado/normalización)\n",
    "from pyspark.sql.functions import col, trim, lower, when, to_date, regexp_replace\n",
    "\n",
    "# Clean drivers\n",
    "drivers_clean_df = (\n",
    "    drivers_df\n",
    "    .dropDuplicates([\"Driver_ID\"])\n",
    "    .withColumn(\"Name\", trim(col(\"Name\")))\n",
    "    .withColumn(\"City\", trim(col(\"City\")))\n",
    "    .withColumn(\"Active_Status\", when(lower(trim(col(\"Active_Status\"))).like(\"%active%\"), \"Active\")\n",
    "                                   .when(lower(trim(col(\"Active_Status\"))).like(\"%inactive%\"), \"Inactive\")\n",
    "                                   .otherwise(None))\n",
    "    # eliminar filas sin ID o sin nombre\n",
    "    .filter(col(\"Driver_ID\").isNotNull() & col(\"Name\").isNotNull())\n",
    "    # filtrar valores numéricos fuera de rango razonable\n",
    "    .filter(col(\"Age\").between(18, 100))\n",
    "    .filter((col(\"Experience_Years\") >= 0) & (col(\"Experience_Years\") <= 80))\n",
    "    .filter(col(\"Average_Rating\").between(0.0, 5.0))\n",
    ")\n",
    "\n",
    "# Clean rides\n",
    "rides_clean_df = (\n",
    "    rides_df\n",
    "    .dropDuplicates([\"Ride_ID\"])\n",
    "    .withColumn(\"City\", trim(col(\"City\")))\n",
    "    .withColumn(\"Date_raw\", trim(col(\"Date\")))\n",
    "    # parsear a tipo date (acepta días/meses de 1 o 2 dígitos)\n",
    "    .withColumn(\"Ride_Date\", to_date(col(\"Date_raw\"), \"M/d/yyyy\"))\n",
    "    # normalizar promo code: convertir cadenas vacías a null\n",
    "    .withColumn(\"Promo_Code\", when(trim(col(\"Promo_Code\")) == \"\", None).otherwise(trim(col(\"Promo_Code\"))))\n",
    "    # eliminar filas sin Ride_ID, Driver_ID o fecha inválida\n",
    "    .filter(col(\"Ride_ID\").isNotNull() & col(\"Driver_ID\").isNotNull() & col(\"Ride_Date\").isNotNull())\n",
    "    # filtrar valores numéricos inválidos\n",
    "    .filter((col(\"Distance_km\") >= 0) & (col(\"Duration_min\") >= 0) & (col(\"Fare\") >= 0))\n",
    "    .filter(col(\"Rating\").between(0.0, 5.0))\n",
    "    # opcional: eliminar la columna intermedia Date_raw\n",
    "    .drop(\"Date\", \"Date_raw\")\n",
    ")\n",
    "\n",
    "# Mantener solo rides cuyos drivers existen en drivers_clean_df (evita datos huérfanos)\n",
    "rides_clean_df = rides_clean_df.join(drivers_clean_df.select(\"Driver_ID\"), on=\"Driver_ID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a0e72",
   "metadata": {},
   "source": [
    "# Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61d130",
   "metadata": {},
   "source": [
    "# Persistence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ef5f1",
   "metadata": {},
   "source": [
    "# DAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
