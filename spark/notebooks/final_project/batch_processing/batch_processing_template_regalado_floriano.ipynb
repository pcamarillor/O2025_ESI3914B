{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Batch Processing** </center>\n",
    "---\n",
    "\n",
    "**Date**: October, 2025\n",
    "\n",
    "**Student Name**: Regalado Floriano Luis A.\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd2743",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925ab56-e9fa-44e6-bdec-2c7f904fb5f8",
   "metadata": {},
   "source": [
    "OrangeStone wishes to enter the House Market, and has access to a database that is updated monthly with information pertaining to global house purchases. They want to use the most of these information to make decisions on the real state they buy. As such, they decided to transform their raw data into a star model for ease of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368617a-bb1e-4a3c-86d7-6c25ad6b3d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1757b962",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea76579-7ace-4f08-bb92-cffafa0076de",
   "metadata": {},
   "source": [
    "The dataset they have access to is https://www.kaggle.com/datasets/mohankrishnathalla/global-house-purchase-decision-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093c280-0e65-48d8-a3f0-58dfabc242f1",
   "metadata": {},
   "source": [
    "It is a single table, and they want to convert it to the following Star Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f9b2c-08cf-4641-bb77-d12e1d68829b",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "---\n",
    "title: House Loan Star Model\n",
    "---\n",
    "erDiagram\n",
    "\n",
    "    Customer ||..|{ FactHouse : INTERESTED_IN\n",
    "\n",
    "    FactHouse ||--|| Location: AT \n",
    "    FactHouse ||--|{ LoanDetails: HAS_LOAN  \n",
    "    FactHouse ||--|| HouseDetails: ABOUT_HOUSE  \n",
    "    Location ||--|| CityDetails: IN_CITY\n",
    "    Location ||--|| CountryDetails: IN_COUNTRY\n",
    "\n",
    "    HouseDetails ||--|| FurnishingDetails: IS_FURNISHED\n",
    "    HouseDetails ||--|| PropertyTypeDetails: OF_PROPERTY_TYPE\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62871b-4975-4626-bfda-1bfd9669e6a3",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "---\n",
    "title: House Loan Star Model\n",
    "---\n",
    "erDiagram\n",
    "\n",
    "    Customer ||..|{ FactHouse : INTERESTED_IN\n",
    "\n",
    "    FactHouse ||--|| Location: AT \n",
    "    FactHouse ||--|{ LoanDetails: HAS_LOAN  \n",
    "    FactHouse ||--|| HouseDetails: ABOUT_HOUSE  \n",
    "    Location ||--|| CityDetails: IN_CITY\n",
    "    Location ||--|| CountryDetails: IN_COUNTRY\n",
    "\n",
    "    HouseDetails ||--|| FurnishingDetails: IS_FURNISHED\n",
    "    HouseDetails ||--|| PropertyTypeDetails: OF_PROPERTY_TYPE\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a0e72",
   "metadata": {},
   "source": [
    "# Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da11abc-91d5-47ac-9a77-7e97dad1328e",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1ad0bb-bc66-4c0f-99ca-8b79aa2689cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/10/27 02:47:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML: Logistic Regression\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/work-dir/jars/postgresql-42.7.8.jar\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d7fd64-cccd-4157-88c9-84ba1705e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regalado_floriano.spark_utils import SparkUtils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7477f2-5f47-470e-af84-e9a3a69e355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_schema = SparkUtils.generate_schema(\n",
    " \n",
    "    ((\"property_id\",\"int\"),\n",
    "(\"country\",\"string\"),\n",
    "(\"city\",\"string\"),\n",
    "(\"property_type\",\"string\"),\n",
    "(\"furnishing_status\",\"string\"),\n",
    "(\"property_size_sqft\",\"int\"),\n",
    "(\"price\",\"int\"),\n",
    "(\"constructed_year\",\"int\"),\n",
    "(\"previous_owners\",\"int\"),\n",
    "(\"rooms\",\"int\"),\n",
    "(\"bathrooms\",\"int\"),\n",
    "(\"garage\",\"bool\"),\n",
    "(\"garden\",\"bool\"),\n",
    "(\"crime_cases_reported\",\"int\"),\n",
    "(\"legal_cases_on_property\",\"bool\"),\n",
    "(\"customer_salary\",\"int\"),\n",
    "(\"loan_amount\",\"int\"),\n",
    "(\"loan_tenure_years\",\"int\"),\n",
    "(\"monthly_expenses\",\"int\"),\n",
    "(\"down_payment\",\"int\"),\n",
    "(\"emi_to_income_ratio\",\"float\"),\n",
    "(\"satisfaction_score\",\"int\"),\n",
    "(\"neighbourhood_rating\",\"int\"),\n",
    "(\"connectivity_score\",\"int\"),\n",
    "(\"decision\",\"bool\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74428443-4d9c-45e3-b308-f70d912087ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df = spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .schema(houses_schema) \\\n",
    "                .csv(\"/opt/spark/work-dir/data/house_purchases\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1caa6b-44e7-424e-9b29-ae79590289a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "house_df= house_df.na.fill(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08c145-ec0c-4a1e-951e-660f1e33603e",
   "metadata": {},
   "source": [
    "## Transformation 1: Extracting Strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28ce741-2280-47cb-b298-e91789c56c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to prepare for ease of analysis, all categorical data will be put in it's own frame. That is to say, each country and city will get \n",
    "# their own id\n",
    "categories = \"country city property_type furnishing_status\".split()\n",
    "_localMap = SparkUtils.generate_keyed_distinct_column(house_df)\n",
    "\n",
    "categoricalTables =   {\n",
    "     key:     _localMap(key) for key in categories\n",
    " }\n",
    "\n",
    "id_house = house_df\n",
    "for cat in categories:\n",
    "    cur_df = categoricalTables[cat]\n",
    "    id_house = SparkUtils.replace_column_for_key(id_house)(cur_df)(cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0281ed-7de2-4197-b876-767b1cebd01b",
   "metadata": {},
   "source": [
    "## Transformation 2: Creating unique location id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c22b80d5-32ce-42d0-8af6-ae7b0845d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/27 02:47:17 WARN Column: Constructing trivially true equals predicate, 'm.country_id == m.country_id'. Perhaps you need to use aliases.\n",
      "25/10/27 02:47:17 WARN Column: Constructing trivially true equals predicate, 'm.city_id == m.city_id'. Perhaps you need to use aliases.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "locations = id_house.select( \"country_id\", \"city_id\").distinct().withColumn(\"id\",monotonically_increasing_id())\n",
    "id_house = id_house.join(\n",
    "    locations ,\n",
    "    on=[id_house[\"country_id\"] == locations[\"country_id\"],\n",
    "        id_house[\"city_id\"] == locations[\"city_id\"]],\n",
    "    how=\"left\"\n",
    ").drop(\"country_id\", \"city_id\").withColumnRenamed(\"id\", \"location_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2c211-ff84-4087-9c7d-3534b1d45532",
   "metadata": {},
   "source": [
    "## Transformation 3: Creating House Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bf39bc7-ba2c-4e1a-9a0f-96e533725a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_details = id_house.select(\"property_id\", \n",
    "                                \"previous_owners\",\n",
    "                                \"rooms\",\"bathrooms\", \"garage\",\"garden\",\"crime_cases_reported\",\"legal_cases_on_property\",\n",
    "                               \"neighbourhood_rating\", \"satisfaction_score\", \"property_size_sqft\", \"price\"\n",
    "                               ,\"constructed_year\", \"furnishing_status_id\", \"property_type_id\"\n",
    "                               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee0c88-7806-48da-974c-ed71b025a005",
   "metadata": {},
   "source": [
    "## Transformation 4: Creating Loan Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4237da7f-fb67-434b-915a-4d0c3cfed8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_details = house_df.select( \n",
    "            \"property_id\",\n",
    "            \"loan_amount\",\"loan_tenure_years\",\"down_payment\"\n",
    "                              ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bb679-0302-4356-bf98-f8eaae4d3696",
   "metadata": {},
   "source": [
    "### Transformation 5: Creating Buyer Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ee732f4-2160-4ccb-bf6d-4ed56d3a7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyer_details = id_house.select(\n",
    "    \"property_id\", \"customer_salary\",\"emi_to_income_ratio\", \"monthly_expenses\", \"connectivity_score\"\n",
    "                               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecdbee9c-33c9-4f0d-a147-25262fd38263",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_details = [house_details , loan_details, buyer_details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d4c45a-6419-4363-adad-a9968c284dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "factHouses =  id_house.select(\"property_id\", \"decision\", \"location_id\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61d130",
   "metadata": {},
   "source": [
    "# Persistence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb416e-ea30-400a-8ea7-38c67725bc44",
   "metadata": {},
   "source": [
    "Since we are going to write a Star Model, a relational database is the obvious choice. Since we would be dealing with big data, a distributed environment would be much preferred, which is why we chose to use Postgres as our Database Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084f1ad4-e89a-4c3a-94f7-c6a7edb541fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres-iteso:5432/postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db708f6e-fc5e-48b3-ae71-70c0a325f109",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view 'FactHouses' already exists. SaveMode: ErrorIfExists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSparkUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteToPostGres\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactHouses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFactHouses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/work-dir/lib/regalado_floriano/spark_utils.py:63\u001b[0m, in \u001b[0;36mSparkUtils.writeToPostGres.<locals>.get_endpoint.<locals>.get_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_name\u001b[39m(name):\n\u001b[1;32m     56\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdmin@1234\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1743\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view 'FactHouses' already exists. SaveMode: ErrorIfExists."
     ]
    }
   ],
   "source": [
    "SparkUtils.writeToPostGres(factHouses)(jdbc_url)(\"FactHouses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e42d95-b5c2-41d5-b3d2-0456db438b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "SparkUtils.writeToPostGres(locations)(jdbc_url)(\"LocationDetails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "009d2a9e-9f22-4c88-bcdb-aa7d00be9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "for tableName in categoricalTables:\n",
    "    table = categoricalTables[tableName] \n",
    "    \n",
    "    SparkUtils.writeToPostGres(table)(jdbc_url)(f\"{tableName}Details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48cdc72d-2741-43f9-b502-344f72780f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/27 03:13:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "all_details = { \"HouseDetails\": house_details , \"LoanDetails\": loan_details, \"BuyerDetails\": buyer_details}\n",
    "for tableName in all_details:\n",
    "    table = all_details[tableName] \n",
    "    \n",
    "    SparkUtils.writeToPostGres(table)(jdbc_url)(f\"{tableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0be916-0cf5-4b4a-b58b-3c3597448301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86c6cad9-fc0c-40b3-8e56-eade36431159",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/batch_regalado_floriano_post.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ef5f1",
   "metadata": {},
   "source": [
    "# DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c76909-e515-4574-8fa7-7f3b379b5c05",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/batch_regalado_floriano_dag.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aade594-ca26-4cfd-bac4-4c1165a5cd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
