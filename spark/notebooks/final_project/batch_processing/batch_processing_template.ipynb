{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Batch Processing** </center>\n",
    "---\n",
    "\n",
    "**Date**: October, 2025\n",
    "\n",
    "**Student Name**: Luis Roberto Chávez Mancilla\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd2743",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "En la actulidad el crecimiento sin medida del contenido audiovisual ha hecho cada vez mas dificil para las plataformas de streaming y las audiencias comprender las tendecias y las preferencias del publico. \n",
    "Ante esto el analis de los datos se vuelve una herramienta fundamental para que la industria del entretenimiento pueda tomar decisiones sobre produccion, localizacion y estrategias de recomendaciones a los usuarios.\n",
    "\n",
    "En este proyecto tenemos que desarrollar nuestro data pipeline que analice informacion global de programas de television con el objetivo de identificar tendencias relacionadas con la popularidad, calificaciones, el idioma, genero y el pais de origen. El data set utilizado es *10k Popular TV Shows Dataset 1944-2025 (TMDB)* contiene 10 mil registros con metadatos detallados como el titulo, idioma original, paises de origen, generos, metricas de popularidad, promedios de votos y fechas de estreno.\n",
    "\n",
    "En este proyecto busco mediante el..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757b962",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Obtuve este dataset de Kaggle:\n",
    "- https://www.kaggle.com/datasets/riteshswami08/10000-popular-tv-shows-dataset-tmdb\n",
    "\n",
    "Para este proyecto el modelo seleccionado para nuestro pipeline es un *modelo relacional* usando un container de PostgreSQL como nuestro sistema de gestión.\n",
    "\n",
    "Creo que en este caso un modelo relacional es lo mas adecuado debido a las relaciones naturales entre las entidades de nuestro data set (series, generos y paises), lo cual facilita la integridad referencial, normalizacion y nos permite podeer realizar analisis complejos mediante operaciones como joins y aggregations.\n",
    "\n",
    "El esquema propuesto estaría compuesto por 5 tablas principales:\n",
    "1. Shows (tabla con a información principal)\n",
    "\n",
    "`id` - int\n",
    "\n",
    "`name` - str\n",
    "\n",
    "`original_name` - str\n",
    "\n",
    "`original_language` - str\n",
    "\n",
    "`first_air_date` - date(str)\n",
    "\n",
    "`overview` - str\n",
    "\n",
    "`popularity` - float\n",
    "\n",
    "`vote_average` - float\n",
    "\n",
    "`vote_count` - int\n",
    "\n",
    "`adult` - int\n",
    "\n",
    "`poster_path` - str\n",
    "\n",
    "`backdrop_path` - str\n",
    "\n",
    "2. Genres (lista de géneros disponibles en el data set)\n",
    "\n",
    "`genre_id` - int\n",
    "\n",
    "`genre_name` - str\n",
    "\n",
    "3. Show genres (tabla de relacion entre shows y generos)\n",
    "\n",
    "`show_id` - int (fk)\n",
    "\n",
    "`genre_id` - int (fk)\n",
    "\n",
    "4. Countries (catálogo de países)\n",
    "\n",
    "`country_code` - str\n",
    "\n",
    "`country_name` - str\n",
    "\n",
    "5. Show countries (relación entre shows y país de origen)\n",
    "\n",
    "`show_id` - int\n",
    "\n",
    "`country_code` - str\n",
    "\n",
    "**Notas:** `genre_ids` y `origin_country` en el CSV vienen como strings tipo lista (ej. `\"[18, 35]\"` o `\"['US']\"`). En la etapa de ingestión se parsearán (con `ast.literal_eval` / `json.loads`) y se poblarán las tablas `genres` / `show_genres` y `countries` / `show_countries`. También se crearán columnas derivadas en Transformations (p. ej. `release_year`, `genre_count`, `weighted_popularity`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a0e72",
   "metadata": {},
   "source": [
    "# Transformations and Actions\n",
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ef0142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/10/24 02:12:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project batch processing - Part 1.\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/work-dir/jars/postgresql-42.7.8.jar\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b7d19",
   "metadata": {},
   "source": [
    "### Define schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff586f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('adult', BooleanType(), True), StructField('backdrop_path', StringType(), True), StructField('genre_ids', StringType(), True), StructField('id', IntegerType(), True), StructField('origin_country', StringType(), True), StructField('original_language', StringType(), True), StructField('original_name', StringType(), True), StructField('overview', StringType(), True), StructField('popularity', FloatType(), True), StructField('poster_path', StringType(), True), StructField('first_air_date', StringType(), True), StructField('name', StringType(), True), StructField('vote_average', FloatType(), True), StructField('vote_count', IntegerType(), True)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from robertoman.spark_utils import SparkUtils\n",
    "\n",
    "# Definición de columnas para la tabla shows (orden exacto del CSV)\n",
    "shows_schema_columns = [\n",
    "    (\"adult\", \"boolean\"),           # True/False en CSV\n",
    "    (\"backdrop_path\", \"string\"),\n",
    "    (\"genre_ids\", \"string\"),        # parsear después con ast.literal_eval\n",
    "    (\"id\", \"int\"),                 # numeric id\n",
    "    (\"origin_country\", \"string\"),   # parsear después con ast.literal_eval\n",
    "    (\"original_language\", \"string\"),\n",
    "    (\"original_name\", \"string\"),\n",
    "    (\"overview\", \"string\"),\n",
    "    (\"popularity\", \"float\"),\n",
    "    (\"poster_path\", \"string\"),\n",
    "    (\"first_air_date\", \"string\"),   # convertir a date en Transformations\n",
    "    (\"name\", \"string\"),\n",
    "    (\"vote_average\", \"float\"),\n",
    "    (\"vote_count\", \"int\")\n",
    "]\n",
    "\n",
    "# Generamos el esquema de PySpark usando SparkUtils\n",
    "shows_schema = SparkUtils.generate_schema(shows_schema_columns)\n",
    "shows_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3474722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ad9c8",
   "metadata": {},
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a42102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adult: boolean (nullable = true)\n",
      " |-- backdrop_path: string (nullable = true)\n",
      " |-- genre_ids: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- origin_country: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_name: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      " |-- first_air_date: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- vote_average: float (nullable = true)\n",
      " |-- vote_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/opt/spark/work-dir/data/\"\n",
    "\n",
    "df_shows = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(shows_schema) \\\n",
    "    .csv(base_path + \"shows/\")  \n",
    "\n",
    "# df_shows.show(5, truncate=False)\n",
    "df_shows.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ae0ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de registros: 10989\n",
      "\n",
      "Columna                   NULLs      Porcentaje\n",
      "--------------------------------------------------\n",
      "adult                     989          9.00%\n",
      "adult                     989          9.00%\n",
      "backdrop_path             733          6.67%\n",
      "backdrop_path             733          6.67%\n",
      "genre_ids                 105          0.96%\n",
      "genre_ids                 105          0.96%\n",
      "id                        989          9.00%\n",
      "id                        989          9.00%\n",
      "origin_country            190          1.73%\n",
      "origin_country            190          1.73%\n",
      "original_language         213          1.94%\n",
      "original_language         213          1.94%\n",
      "original_name             234          2.13%\n",
      "original_name             234          2.13%\n",
      "overview                  1295        11.78%\n",
      "overview                  1295        11.78%\n",
      "popularity                2055        18.70%\n",
      "popularity                2055        18.70%\n",
      "poster_path               1510        13.74%\n",
      "poster_path               1510        13.74%\n",
      "first_air_date            1461        13.30%\n",
      "first_air_date            1461        13.30%\n",
      "name                      1524        13.87%\n",
      "name                      1524        13.87%\n",
      "vote_average              2166        19.71%\n",
      "vote_average              2166        19.71%\n",
      "vote_count                2288        20.82%\n",
      "vote_count                2288        20.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 13:25:43 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_records = df_shows.count()\n",
    "print(f\"\\nTotal de registros: {total_records}\\n\")\n",
    "\n",
    "# Contar NULLs por columna usando PySpark\n",
    "print(f\"{'Columna':<25} {'NULLs':<10} {'Porcentaje':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for column_name in df_shows.columns:\n",
    "    null_count = df_shows.filter(col(column_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        percentage = (null_count / total_records * 100)\n",
    "        print(f\"{column_name:<25} {null_count:<10} {percentage:>6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f33354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros en el dataset: 10989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros en el dataset sin NULLs: 7407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Definir columnas críticas (no pueden ser NULL)\n",
    "critical_columns = ['id', 'name', 'first_air_date', 'genre_ids', 'origin_country']\n",
    "\n",
    "# Paso 2.1: Eliminar filas donde columnas críticas sean NULL\n",
    "df_clean = df_shows.dropna(subset=critical_columns)\n",
    "\n",
    "print(f\"\\nRegistros eliminados por NULLs en columnas críticas: {df_shows.count() - df_clean.count()}\")\n",
    "print(f\"Registros restantes: {df_clean.count()}\")\n",
    "\n",
    "# Paso 2.2: Rellenar NULLs en columnas opcionales con valores por defecto\n",
    "df_clean = df_clean.fillna({\n",
    "    'overview': 'No description available',\n",
    "    'backdrop_path': '',\n",
    "    'poster_path': '',\n",
    "    'vote_count': 0,\n",
    "    'vote_average': 0.0,\n",
    "    'popularity': 0.0,\n",
    "    'adult': False,\n",
    "    'original_language': 'unknown',\n",
    "    'original_name': ''\n",
    "})\n",
    "\n",
    "print(\"\\nNULLs en columnas opcionales rellenados con valores por defecto\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paso 2.3: Verificar que no quedan NULLs (100% PySpark)\n",
    "print(\"\\n✓ Verificando NULLs restantes...\")\n",
    "total_nulls = 0\n",
    "for column_name in df_clean.columns:\n",
    "    null_count = df_clean.filter(col(column_name).isNull()).count()\n",
    "    total_nulls += null_count\n",
    "\n",
    "print(f\"\\n✓ Total de NULLs después de limpieza: {total_nulls}\")\n",
    "print(f\"✓ Dataset final: {df_clean.count()} registros\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61d130",
   "metadata": {},
   "source": [
    "# Persistence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ef5f1",
   "metadata": {},
   "source": [
    "# DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc2ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
