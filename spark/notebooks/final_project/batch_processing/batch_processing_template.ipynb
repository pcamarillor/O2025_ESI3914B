{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Batch Processing** </center>\n",
    "---\n",
    "\n",
    "**Date**: October, 2025\n",
    "\n",
    "**Student Name**: Juan Bernardo Orozco Quirarte\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c542e7f",
   "metadata": {},
   "source": [
    "# Objective \n",
    "To build a data pipeline in Python using Apache Spark for data consumption, transformation, and persistence, with the objective of addressing a practical problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd2743",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "### Breve descripción del problema: optimizar ocupación de gimnasios para reducir congestión en horarios punta, mejorar experiencia de usuario y planear recursos.\n",
    "\n",
    "### Fuentes de decisión: usar checkins de usuarios y uso de equipo para:\n",
    "- detectar horas pico\n",
    "- calcular ocupación por zona/equipo\n",
    "- proponer ventanas con baja ocupación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98cc0ac",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "### Modelo de datos: relacional (tablas: gyms, users, checkins)\n",
    "- gyms(gym_id, name, city)\n",
    "- users(user_id, name, age, membership_type)\n",
    "- checkins(checkin_id, gym_id, user_id, timestamp, equipment, duration_min)\n",
    "\n",
    "### CSVs Generados con Faker y disponibles en la carpeta lib/bernardoorozco junto con el .py que genera datos con faker \"faker_project_generator.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final Project Batch Processing\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bernardoorozco.spark_utils import SparkUtils\n",
    "base_path = \"/opt/spark/work-dir/lin/bernardoorozco\"\n",
    "\n",
    "gyms_schema, _ = SparkUtils.generate_schema([\n",
    "    (\"gym_id\", \"string\"),\n",
    "    (\"name\", \"string\"),\n",
    "    (\"city\", \"string\")\n",
    "])\n",
    "\n",
    "users_schema, _ = SparkUtils.generate_schema([\n",
    "    (\"user_id\", \"string\"),\n",
    "    (\"name\", \"string\"),\n",
    "    (\"age\", \"int\"),\n",
    "    (\"membership_type\", \"string\")\n",
    "])\n",
    "\n",
    "checkins_schema, _ = SparkUtils.generate_schema([\n",
    "    (\"checkin_id\", \"string\"),\n",
    "    (\"gym_id\", \"string\"),\n",
    "    (\"user_id\", \"string\"),\n",
    "    (\"timestamp\", \"string\"),   # se parseará después\n",
    "    (\"equipment\", \"string\"),\n",
    "    (\"duration_min\", \"int\")\n",
    "])\n",
    "\n",
    "gyms_df = spark.read.schema(gyms_schema).option(\"header\", True).csv(f\"{base_path}/gyms.csv\")\n",
    "users_df = spark.read.schema(users_schema).option(\"header\", True).csv(f\"{base_path}/users.csv\")\n",
    "checkins_df = spark.read.schema(checkins_schema).option(\"header\", True).csv(f\"{base_path}/checkins.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a0e72",
   "metadata": {},
   "source": [
    "# Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, hour, dayofweek, when, col,count, avg\n",
    "\n",
    "checkins_clean = (\n",
    "    checkins_df\n",
    "    .withColumn(\"timestamp_ts\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .drop(\"timestamp\")\n",
    "    .filter(col(\"timestamp_ts\").isNotNull())               # eliminamos si no tienen timestamp\n",
    "    .filter(col(\"duration_min\") > 0)                       # eliminar duraciones no valida en caso de que existan (cosa que no pasa porque nuestro generador solo genera duraciones de entre 15 y 120 mins)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f25921",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_timedata = (\n",
    "    checkins_clean\n",
    "    .withColumn(\"hour\", hour(col(\"timestamp_ts\")))\n",
    "    .withColumn(\"dayofweek\", dayofweek(col(\"timestamp_ts\")))\n",
    ")\n",
    "\n",
    "checkins_timedata = checkins_timedata.dropDuplicates([\"checkin_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_full=checkins_timedata.join(gyms_df, on=\"gym_id\", how=\"left\").join(users_df, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4707da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_by_hour = (\n",
    "    checkins_full\n",
    "    .groupBy(\"gym_id\", \"name\", \"city\", \"hour\", \"equipment\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total chekins\"),\n",
    "        avg(\"duration_min\").alias(\"avg_duration_min\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5854cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_equipment = occupancy_by_hour.orderBy(col(\"checkin_count\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61d130",
   "metadata": {},
   "source": [
    "# Persistence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_to_write = occupancy_by_hour.select(\n",
    "    \"gym_id\", \"name\", \"city\", \"hour\", \"equipment\", \"checkin_count\", \"avg_duration\"\n",
    ")\n",
    "occupancy_to_write = occupancy_to_write.withColumn(\"hour\", col(\"hour\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ef5f1",
   "metadata": {},
   "source": [
    "# DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ff37a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
