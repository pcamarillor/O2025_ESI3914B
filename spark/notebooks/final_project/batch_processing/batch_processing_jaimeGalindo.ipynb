{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Batch Processing** </center>\n",
    "---\n",
    "\n",
    "**Date**: October, 2025\n",
    "\n",
    "**Student Name**: Jaime Enrique Galindo Villegas\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd2743",
   "metadata": {},
   "source": [
    "# Introducción: Análisis de Transacciones en Mercado Libre\n",
    "La aplicación web y móvil de Mercado libre procesa millones de transacciones diarias que pueden ser pedidos de artículos, suscripciones, pagos, etc. En el 2023 la plataforma logró ventas de 14,500 millones de dólares, con picos de demanda en momentos significativos del año como en el \"Buen Fin\" o el \"Hot Sale\" en los que sus ventas se disparan, en el 2024 llegaron a picos de más de 70% más ventas en el buen fin respecto a años anteriores.  \n",
    "\n",
    "Este volumen masivo de datos, generado en multiples formatos presenta un desafio. Para abordarlo implementaré un pipeline de datos que procesa la información de órdenes de compra, con el objetivo de transformar los datos crudos en un formato limpio y estructurado que facilite posteriores analisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757b962",
   "metadata": {},
   "source": [
    "# Dataset: Órdenes de Compra\n",
    "\n",
    "## Modelo de Datos\n",
    "Para este problema, el **modelo de datos** más adecuado es el de **documento**.\n",
    "\n",
    "Esto debido a que, según el diseño de pipeline que cree anteriormente, una orden de compra de ML es una entidad (JSON) que contiene toda su información de manera autocontenida, el usuario, monto, detalles de envio y articulos comprados. Por esto el modelo de documento es ideal al permitir presentar esta estructura jerárquica.\n",
    "\n",
    "Esto es diferente de un modelo **relacional** ya que tendria que normalizar y dividir la información en multiples tablas como `Ordenes`, `Usuarios`, `Productos`, etc. Requeriria operaciones más complejas y, debido a la necesidad de procesar millones de transacciones, es mejor un modelo de **documento** que es más naturalmente compatible\n",
    "\n",
    "\n",
    "## Generación del Dataset y Esquema\n",
    "Para simular el flujo de datos de ML, se generará un archivo json (`orders.json`) usando la libreria Faker, en el que cada linea representará un objeto JSON de una orden de compra. Así simularé un sistema de ingesta en el que se recibirian los datos en tiempo real.\n",
    "\n",
    "El **esquema** de cada objeto JSON generado será el siguiente:\n",
    "\n",
    "- **order_id**: `String` - Identificador único de la orden.\n",
    "- **timestamp**: `String` - Fecha y hora de la transacción en formato ISO.\n",
    "- **user**: `Struct` - Objeto usuario.\n",
    "  - **user_id**: `String` - Identificador del usuario.\n",
    "  - **region**: `String` - Región de la compra (ej. \"GDL\").\n",
    "  - **payment_method**: `String` - Método de pago utilizado (ej. \"MP\").\n",
    "- **items**: `Array` - Lista de artículos comprados.\n",
    "  - **item_id**: `String` - ID del producto.\n",
    "  - **title**: `String` - Nombre del producto.\n",
    "  - **category**: `String` - Categoría del producto.\n",
    "  - **quantity**: `Integer` - Cantidad de unidades.\n",
    "  - **unit_price**: `Double` - Precio original por unidad.\n",
    "  - **final_price**: `Double` - Precio final por unidad con descuento.\n",
    "  - **discount_applied**: `Boolean` - Indica si se aplicó descuento.\n",
    "- **total_amount**: `Double` - Monto total de la orden.\n",
    "- **shipping**: `Struct` - Información de envío.\n",
    "  - **logistics_provider**: `String` - Empresa de paquetería.\n",
    "  - **warehouse_origin**: `String` - Almacén de origen.\n",
    "  - **estimated_delivery**: `String` - Fecha estimada de entrega.\n",
    "  - **tracking_id**: `String` - Número de seguimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8238249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo orders.json generado con 1000 órdenes.\n"
     ]
    }
   ],
   "source": [
    "from jaime_galindo.order_generator import OrderGenerator\n",
    "import json\n",
    "\n",
    "# Generar 1000 órdenes y guardarlas en un archivo JSON\n",
    "# cada JSON esté en una nueva línea para Spark\n",
    "with open(\"./data/batch_processing/orders.json\", \"w\") as f:\n",
    "\tfor i in range(1000):\n",
    "\t\tf.write(json.dumps(OrderGenerator.create_random_order(i)) + '\\n')\n",
    "\n",
    "print(\"Archivo orders.json generado con 1000 órdenes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a0e72",
   "metadata": {},
   "source": [
    "# Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3fa5b",
   "metadata": {},
   "source": [
    "### Iniciar sesión de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc1a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/25 05:25:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MercadoLibreTransformations\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b4914",
   "metadata": {},
   "source": [
    "### Leer los datos con esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82338a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType # type: ignore\n",
    "from jaime_galindo.spark_utils import SparkUtils\n",
    "\n",
    "# Esquemas básicos de cada entidad\n",
    "user_schema = SparkUtils.generate_schema([\n",
    "\t(\"user_id\", \"string\"),\n",
    "\t(\"region\", \"string\"),\n",
    "\t(\"payment_method\", \"string\")\n",
    "])\n",
    "\n",
    "item_schema = SparkUtils.generate_schema([\n",
    "\t(\"item_id\", \"string\"),\n",
    "\t(\"title\", \"string\"),\n",
    "\t(\"category\", \"string\"),\n",
    "\t(\"quantity\", \"integer\"),\n",
    "\t(\"unit_price\", \"double\"),\n",
    "\t(\"final_price\", \"double\"),\n",
    "\t(\"discount_applied\", \"boolean\")\n",
    "])\n",
    "\n",
    "shipping_schema = SparkUtils.generate_schema([\n",
    "\t(\"logistics_provider\", \"string\"),\n",
    "\t(\"warehouse_origin\", \"string\"),\n",
    "\t(\"estimated_delivery\", \"string\"),\n",
    "\t(\"tracking_id\", \"string\")\n",
    "])\n",
    "\n",
    "\n",
    "# Equema principal de la orden\n",
    "order_schema = SparkUtils.generate_schema([\n",
    "\t(\"order_id\", \"string\"),\n",
    "\t(\"timestamp\", \"string\"),\n",
    "\t(\"user\", user_schema),\n",
    "\t(\"items\", ArrayType(item_schema)),\n",
    "\t(\"total_amount\", \"double\"),\n",
    "\t(\"shipping\", shipping_schema)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0bbfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos\n",
    "df_raw_orders = spark.read \\\n",
    "    .schema(order_schema) \\\n",
    "    .json(\"./data/batch_processing/orders.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc129c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones\n",
    "from pyspark.sql.functions import col, explode \n",
    "# , year, month, dayofmonth, current_timestamp, split, lit\n",
    "\n",
    "# hacer explode para obtener las columnas de cada elemento y filas para cada item\n",
    "df_with_items = df_raw_orders.select(\n",
    "   col(\"order_id\"),\n",
    "   col(\"timestamp\"),\n",
    "   col(\"user\"),\n",
    "   col(\"total_amount\"),\n",
    "   col(\"shipping\"),\n",
    "\n",
    "\texplode(\"items\").alias(\"item\")\n",
    ")\n",
    "\n",
    "# \"Aplanar\" cada estructura compleja para tener solo los valores\n",
    "df_flattened = df_with_items.select(\n",
    "   col(\"order_id\"),\n",
    "   col(\"timestamp\"),\n",
    "   col(\"total_amount\").alias(\"order_total_amount\"),\n",
    "   \n",
    "\t# Aplanar user\n",
    "   col(\"user.user_id\"),\n",
    "   col(\"user.region\"),\n",
    "   col(\"user.payment_method\"),\n",
    "   \n",
    "\t# Aplanar shipping,\n",
    "   col(\"shipping.logistics_provider\"),\n",
    "   col(\"shipping.warehouse_origin\"),\n",
    "   col(\"shipping.estimated_delivery\"),\n",
    "   col(\"shipping.tracking_id\"),\n",
    "   \n",
    "\t# Aplanar item,\n",
    "\tcol(\"item.item_id\"),\n",
    "\tcol(\"item.title\"),\n",
    "\tcol(\"item.category\"),\n",
    "\tcol(\"item.quantity\"),\n",
    "\tcol(\"item.unit_price\"),\n",
    "\tcol(\"item.final_price\"),\n",
    "\tcol(\"item.discount_applied\")\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0961d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, year, month, dayofmonth\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Enriquecimiento, crear nuevas columnas para agregar valor a los datos\n",
    "df_enriched = df_flattened \\\n",
    "    .withColumn(\"order_timestamp\", col(\"timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"item_total_price\", col(\"final_price\") * col(\"quantity\")) \\\n",
    "    .withColumn(\"item_total_discount\", (col(\"unit_price\") - col(\"final_price\")) * col(\"quantity\")) \\\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp()) \n",
    "\n",
    "# Particionado por fecha para optimizar la insersión más adelante\n",
    "final_df = df_enriched.withColumn(\"year\", year(col(\"order_timestamp\"))) \\\n",
    "                      .withColumn(\"month\", month(col(\"order_timestamp\"))) \\\n",
    "                      .withColumn(\"day\", dayofmonth(col(\"order_timestamp\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61d130",
   "metadata": {},
   "source": [
    "# Persistence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ef5f1",
   "metadata": {},
   "source": [
    "# DAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
